\documentclass[12pt]{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{xparse}
\usepackage{textcomp}

\captionsetup[table]{skip=10pt} % table skip

%macros
\newcommand{\defeq}[2]{\stackrel{\mathclap{\normalfont\mbox{#1}}}{#2}}
\def\D{\displaystyle}
\def\att{                    % mark at the margin
        \marginpar[ \hspace*{\fill} \raisebox{-0.2em}{\rule{2mm}{1.2em}} ]
        {\raisebox{-0.2em}{\rule{2mm}{1.2em}} }
        }
\def\at#1{[*** \att #1 ***]}  % text needing attention
\def\spc{\hspace*{0.5cm}}

\NewDocumentCommand{\codeword}{v}{%
	\texttt{\textcolor{blue}{#1}}%
}
\lstset{language=C,keywordstyle={\bfseries \color{blue}}}


\title{RoSeMI: Robust Shepard model for interpolation}
%\author{Beryl Ramadhian Aribowo}

\begin{document}
\maketitle


\section{Feature extraction}
\label{sec:feature}
The very first piece which needs to be computed is the collection of molecular features. The feature matrix $F \in \mathbb{R}^{N_\text{QM9} \times n_f}$ where $N_\text{QM9}$ is the number of molecules in the QM9 database and $n_f$ is the number of features, needs to be precomputed and stored in the non-volatile storage in order to avoid re-computation.
\subsection{Atomic feature selection}
\begin{itemize}
	\item Compute the atomic features using ACSF (\at{cite Behler}). This gives a row vector $f^l_{i:}$ with length $n^\text{atom}_f = 51$ for each atom $i$ in a molecule $l$. Apply this to all molecules in the dataset, hence there are a total of $N_\text{QM9}$ ACSF matrices $f^l$.
	\item In order to select the most relevant features, we do the \textbf{principal component analysis} (PCA), first by computing the mean vectors and the matrix $S$ by
	\begin{equation}
		\begin{split}
			s &= \frac{1}{N_\text{QM9}}\sum_{l} \frac{1}{n^l_\text{atom}}\sum_{i} f^l_{i:}, \\ 
			S &= \frac{1}{N_\text{QM9}}\sum_{l} \frac{1}{n^l_\text{atom}} \sum_{i} f^l_{i:} (f^l_{i:})^\top, \\ 
			\text{for } l &= 1,...,N_\text{QM9}, ~~ i = 1,2,...,n^l_\text{atom},
		\end{split}
	\end{equation}
	where $n^l_\text{atom}$ is the number of atom in molecule $l$; then compute the covariance matrix
	\begin{equation}
		\label{eq:pca_atom_start}
		C = S - ss^\top. \\			
	\end{equation}
	The correlation matrix can be formed by
	\begin{equation}
		C' := DCD, ~~ D = \text{Diag}(1/\sqrt{C_{ii}});
	\end{equation}
	alternatively, the sensitivity of the atomic features is viable as the scaling factor
	\begin{equation}
		D = \text{Diag}(1/\sigma_j),
	\end{equation}
	where the sensitivity vector's entries are
	\begin{equation}
		\sigma_j = \frac{1}{\sum_l n^l_\text{atom}}\sum_{li} d^l_{ij},
	\end{equation}
	where
	\begin{equation}
		d^l_{ij} := \max_k |f^l_{ij} - (\hat{f}_k)^l_{ij}|~~\text{ for }k = 1,2,...,
	\end{equation}
	where $\hat{f_k}$ is the set of perturbed atomic features, obtained by perturbing the atomic coordinates randomly
	\begin{equation}
		x \leftarrow x \pm 0.01,
	\end{equation}
	and using the perturbed coordinates to extract the atomic features $\hat{f}_k$.

	Select the eigenvectors such that they correspond to the $n_{af}$ largest eigenvalues. They are obtained by first doing the spectral decomposition of the correlation matrix
	\begin{equation}
		C' = Q\Lambda Q^\top,
	\end{equation}
	where $\Lambda$ is the diagonal matrix containing the $k$th eigenvalue $\Lambda_{kk}$ and $Q$ is the matrix containing the $k$th eigenvector $Q_{:k}$, then we permute the columns of $Q$ and the diagonal entries $\lambda_i$ of $\Lambda$ such that
	\begin{equation}
		\lambda_{1} \geq \lambda_{2} \geq  ... \geq \lambda_{n^\text{atom}_f},
	\end{equation}
	and select
	\begin{equation}
		\hat{Q} = Q_{:, 1:n_{af}}.
	\end{equation}
	Finally the transformed feature can be obtained by
	\begin{equation}
		\label{eq:pca_atom_end}
		f^l_{i:} \leftarrow \hat{Q}^\top(f^l_{i:} - s), \text{ for }l = 1,2,...,N_\text{QM9}.
	\end{equation}
	\item Scale
	\begin{equation}
		\label{eq:scale_1}
		f^l_{ij} \in [0, 1]
	\end{equation}
	by
	\begin{equation}
		\underbar{$f$}_j := \min_{li} f^l_{ij},
	\end{equation}
	\begin{equation}
		\overline{f}_j := \max_{li} f^l_{ij},
	\end{equation}
	\begin{equation}
		f^l_{ij} \leftarrow (f^l_{ij} - \underbar{$f$}_j)/ (\overline{f}_j - \underbar{$f$}_j), \text{ for } j = 1,..., n_f^\text{select}.
	\end{equation}
	\begin{figure}[h]
		\label{fig:PCA_atom_rat}
		\centering
		\includegraphics[scale=0.55]{plot/eigenvalue_atom_exp_all_1_4_18_false_false_ratio.png}
		\caption{The ratio of each eigenvalue against the largest eigenvalue, obtained from the PCA of ACSF on atomic level.}
	\end{figure}
	\begin{figure}[H]
		\label{fig:PCA_atom_dist}
		\centering
		\includegraphics[scale=0.55]{plot/eigenvalue_atom_exp_all_1_4_18_false_false_distribution.png}
		\caption{$\mu_i := \sum_{i\geq k} \lambda_i \text{ for } k = 1,2,...,n_f$, obtained from the PCA of ACSF on atomic level. This shows the distribution of the eigenvalues when $i$ amount of the (sorted) eigenvalues are dropped.}
	\end{figure}
\end{itemize}

\subsubsection{Timing and storage}
Features based on atomic symmetry functions in general has $\mathcal{O}(n_\text{atom}^2c)$ time complexity, where $c$ is a factor referring to the feature extractor's method of computation (e.g., ACSF requires $c$ amount of radial symmetry function computations), as it is required to sweep through the atom and its neighbours each time an atomic symmetry quantity is queried. The resulting features of each atom is a vector with length $n_{af}$, therefore a molecule has an order of $\mathcal{O}(n_\text{atom}n_{af})$ storage complexity. In total for the QM9 challenge, each time an atomic feature extraction is queried, it has $\mathcal{O}(N_\text{QM9}n_\text{atom}^2c)$ time complexity and $\mathcal{O}(N_\text{QM9}n_\text{atom}n_{af})$ storage complexity.

In general, each query of PCA needs to sweep all entries of the feature matrix multiple times. Specific for atomic features, it can be broken down into:
\begin{itemize}
	\item Computing mean vector $s$ requires $\mathcal{O}(N_\text{QM9}n_\text{atom}n_{af})$ time complexity and $\mathcal{O}(n_{af})$ storage complexity.
	\item Symmetric matrix $S$ requires $\mathcal{O}(N_\text{QM9}n_\text{atom}n^2_{af})$ time complexity and $\mathcal{O}(n^2_{af})$ storage requirement.
	\item The covariance matrix can be computed by $\mathcal{O}(n^2_{af})$ order of operations given the mean vector $s$ and the symmetric matrix $S$, with also $\mathcal{O}(n^2_{af})$ storage complexity.
	\item The correlation matrix can be computed by $\mathcal{O}(n^2_{af})$ order of operations given the covariance matrix $C$ and the scaling factors $D$, requires $\mathcal{O}(n^2_{af})$ storage.
	\item Spectral decomposition of the symmetric matrix requires $\mathcal{O}(n^3_{af})$ time complexity and $\mathcal{O}(n^2_{af})$ storage.
	\item Projecting the atomic features into lower dimensional space requires $\mathcal{O}(n^2_{af})$ time for each atom, hence for all of the QM9 molecules, it requires $\mathcal{O}(N_\text{QM9}n_\text{atom}n^2_{af})$; the storage complexity is $\mathcal{O}(N_\text{QM9}n_\text{atom}n_{af})$.
\end{itemize}
Overall, in terms of the timing and storage, the raw atomic feature extraction itself should be the upper bound of the whole atomic feature processing block, since the factor $c$ itself could also be a composition of many factors depending on the method of the feature extractor.


\subsection{Molecular feature selection}
\begin{itemize}
    \item $l$th molecule contains $n^l_\text{atom}$ number of atoms and a set of atom types $P = \{\text{H}, \text{C}, \text{O}, ...\}$ with cardinality $n^l_\text{type}$,
	the molecular features are computed by
        \begin{equation}
			\label{eq:acsf}
            \begin{split}
                s^l_{j} &= \sum_{m \in M_j} f^l_{m:}, \\
                %S^l &= \sum_i f^l_{i:}(f^l_{i:})^\top,
            \end{split}
        \end{equation}
		where $M_j$ is the set of index which contains the row positions of atom type $j \in P$ in the $f^l$ matrix. 
		Furthermore we add
		\begin{equation}
			N_j = |M_j|,
		\end{equation}
		combined into
		\begin{equation}
			\hat{N} = [N_\text{H}, N_\text{C}, N_\text{O},..., 1/n^l_\text{atom}].
		\end{equation}
		The feature vector of the $l$th molecule becomes
		\begin{equation}
			F_l := [s^l_{j}, \hat{N} ] \in \mathbb{R}^{1 \times n_f^\text{mol}}, \forall j \in P,
        \end{equation}
		where
		\begin{equation}
    		n_f^\text{mol} := 5n_f + 6,
    	\end{equation}
		in most cases this is a sufficiently good feature vector for fitting.
		
		Additional features can be introduced, such as the sum of squares
		\begin{equation}
			\hat{s}^l_j = \sum_{m \in M_j} (f^l_{m:})^2;
		\end{equation}
		the binomial feature
		\begin{equation}
			S^l_j = \sum_{m \in M_j} f^l_{m:}(f^l_{m:})^\top;
		\end{equation}
		since $S^l_j$ is a symmetric matrix, only the upper triangular and the diagonal entries are needed. 
		Hence if we include both the sum of squares and the binomial features
		\begin{equation}
			F_l := [s^l_{j}, \hat{s}^l_{j}, S^l_j, \hat{N} ] \in \mathbb{R}^{1 \times n_f^\text{mol}}, \forall j \in P,
		\end{equation}
        where
		\begin{equation}
			n_f^\text{mol} := \frac{5}{2}(n_f^2+n_f)+6,
		\end{equation}
    	this defines the fingerprint matrix $F \in \mathbb{R}^{N \times n_f}$.
    \item Do feature selection by using PCA in a similar manner as (\refeq{eq:pca_atom_start}) -- (\refeq{eq:pca_atom_end}) to take the subset of the columns of $W$, this can be done by the following:
	    \begin{equation}
			\label{eq:pca_mol_start}
			\begin{split}
				s &= \frac{1}{N} \sum_l F_{l:}, \\
				S &= \frac{1}{N} \sum_l F_{l:}(F_{l:})^\top, \\
				\text{ for } l &= 1,2,...,N_\text{QM9}, \\
			\end{split}
	    \end{equation}
		compute the covariance matrix
		\begin{equation}
			C = S - ss^\top, \\			
		\end{equation}
		compute the correlation matrix
		\begin{equation}
			C' := DCD, ~~ D = \text{Diag}(1\sqrt{C_{ii}}),
		\end{equation}
		and its spectral decomposition
		\begin{equation}
			C' = Q\Lambda Q^\top,
		\end{equation}
		permute the columns of $Q$ and the entries of $\Lambda$ such that
		\begin{equation}
			\lambda_{1} \geq \lambda_{2} \geq  ... \geq \lambda_{n^\text{mol}_f},
		\end{equation}
		and select
		\begin{equation}
			\hat{Q} = Q_{:, 1:n_{mf}}.
		\end{equation}
		Obtain the transformed feature by
		\begin{equation}
			\label{eq:pca_mol_end}
			F_{l:} \leftarrow \hat{Q}^\top(F_{l:} - s), \text{ for }l = 1,2,...,N_\text{QM9}.
		\end{equation}
	\item Scale $F$ such that 
	\begin{equation}
		\label{eq:scale_mol_1}
		F_{lj} \in [0, 1]
	\end{equation}
	by
	\begin{equation}
		\label{eq:scale_mol_2}
		\underbar{$F$}_{j} := \min_{l} F_{lj}
	\end{equation}
	\begin{equation}
		\overline{F}_{j} := \max_{l} F_{lj}
	\end{equation}
	\begin{equation}
		F_{:j} \leftarrow (F_{:j} - \underbar{$F$}_{j})/ (\overline{F}_{j} - \underbar{$F$}_{j}), \text{ for } j = 1,2,..., n_{mf}.
	\end{equation}
	\begin{figure}[h]
		\label{fig:PCA_mol_rat}
		\centering
		\includegraphics[scale=0.55]{plot/eigenvalue_mol_exp_all_1_4_18_false_false_ratio.png}
		\caption{The ratio of each eigenvalue against the largest eigenvalue with hyperparameter $n_{af} = 4$, obtained from the PCA of ACSF on molecular level.}
	\end{figure}
	\begin{figure}[H]
		\label{fig:PCA_mol_dist}
		\centering
		\includegraphics[scale=0.55]{plot/eigenvalue_mol_exp_all_1_4_18_false_false_distribution.png}
		\caption{The distribution of the eigenvalues with hyperparameter $n_{af} = 4$. Obtained from the PCA of ACSF on molecular level.}
	\end{figure}
\end{itemize}

\subsubsection{Timing and storage}
The timing breakdown of the molecular feature selection is as the following:
\begin{itemize}
	\item In the default setting, the molecular feature is only computed by the sums of the atomic features, for each molecule this has $\mathcal{O}(n_\text{atom} n_{af})$ order of operations, therefore for all molecules in the QM9 dataset the time complexity is $\mathcal{O}(N_\text{QM9}n_\text{atom} n_{af})$; the storage requirement is $\mathcal{O}(N_\text{QM9}n_{mf})$. 
	\item Additional features which increase the complexity noticeably is the binomial feature, each binomial feature matrix takes $\mathcal{O}(n^2_{af})$ order of computation and $\mathcal{O}(n^2_{af})$ storage, for $N_\text{QM9}$ this becomes $\mathcal{O}(N_\text{QM9}n_\text{atom}n^2_{af})$ time and storage requirement.
\end{itemize}
For molecular features, the PCA has similar time and storage complexity to the atomic feature counterpart, the only differences are the initial mean vector $s$, the symmetric matrix $S$ computation, and the final projection, since the feature is now a vector with length $n_{mf}$ for each molecule in contrast to the atomic features with $\mathbb{R}^{n_\text{atom} \times n_{af}}$ matrix for each molecule.


\section{Splines}
The splines (bspline) are used to increase the dimension of the feature space; with sufficient number of splines for each feature, it will lead to linearly separated data points.
\begin{itemize}
	\item Compute the univariate splines $\beta$ and its derivative $\beta'$ given $F_{ij}$ for $i = 1,...,n_\text{data}$, $j = 1,...,n_f$ (vectorized over matrix $F$):
	\begin{equation}
		\label{eq:spline}
		\begin{split}
			\Phi := \beta(F) \in \mathbb{R}^{n_\text{data} \times L}, \\
			\Phi' := \beta'(F) \in \mathbb{R}^{n_\text{data} \times L}.
		\end{split}
	\end{equation}
	where $n_L = n_fn_s$ and $n_s$ is the number of splines for each feature.
	\begin{figure}[h]
		\label{fig:spline}
		\centering
		\includegraphics[scale=0.55]{plot/f.png}
		\caption{The bspline with $n_s = 8$.}
	\end{figure}
	\begin{figure}[H]
		\label{fig:dspline}
		\centering
		\includegraphics[scale=0.55]{plot/df.png}
		\caption{The derivative of bspline with $n_s = 8$.}
	\end{figure}
\end{itemize}



\section{Data selection}
\label{sec:data}
The set of indices which refers to data points with known energies (or centers) $K$ is chosen by the farthest-distance-algorithm variations, where the challenge imposes $n_K = |K| = 100$. Here we pick $N = |T| > n_K$ centers as a hyperparameter. The flow of the data selection is as the following:
\begin{itemize}
	\item Compute and store the set of indices $K$ and the corresponding matrix of distances $D \in \mathbb{R}^{N_\text{QM9} \times N}$ by farthest-minimum-distance algorithm \at{cite Eldar} given $F$, $B$ and $N$.
	\begin{equation}
		(T, D) := f_{\text{fmd}}(F, B, M),
	\end{equation}
	in which the distance between two data points of index $k$ and $m$ is defined by
	\begin{equation}
		D_k(F_{m:}) := \|F_{m:} - F_{k:}\|^2_B := \|B(F_{m:} - F_{k:})\|^2_2.
	\end{equation}
	From this, we can obtain one data point which belongs to the set $T$ by indexing
	\begin{equation}
		w_k := F_{k:}, \text{ for } k \in T.
	\end{equation}
\end{itemize}

\section{The model}
\label{sec:model}

\subsection{Shepard model}
\label{subsec:shepard}
The generalized Shepard model has the form of 
\begin{equation}
	\label{eq:shepard}
	V_K(w):=R_K(w)/S_K(w) ~~~ \text{ for } w \in \mathbb{R}^m \setminus \{w_k\mid k\in K \},
\end{equation}
where
\begin{equation}
	R_K(w):=\sum_{k\in K} \frac{V_k(w)}{D_k(w)},~~~
	S_K(w):=\sum_{k\in K} \frac{1}{D_k(w)},
\end{equation}
and
\begin{equation}
	\label{eq:vk}
	V_k(w) = E_k + \sum_l \theta_{kl} \phi_{kl}(w).
\end{equation}
By (\ref{eq:vk}), (\ref{eq:shepard}) can be expanded into
\begin{equation}
	\label{eq:vk_expand}
	V_K(w) := \sum_{k\in K} \frac{E_k + \sum_l \theta_{kl} \phi_{kl}(w)}{D_k(w)} / S_K(w). 
\end{equation}
The quality of the prediction accuracy is measured by the \textbf{mean absolute deviation}
\begin{equation}
	\text{MAD}_K(w) := \frac{1}{|K|}\sum_{j\in K}|\Delta_{jK}(w)|
\end{equation}
and the \textbf{root mean square deviation}
\begin{equation}
	\text{RMSD}_K(w) := \sqrt{\frac{1}{|K|}\sum_{j\in K}\Delta_{jK}(w)^2},
\end{equation}
where by (\refeq{eq:vk}) and (\refeq{eq:vk_expand})
\begin{equation}
	\label{eq:delta}
	\Delta_{jK}(w):=\D\frac{V_K(w)-V_j(w)}{D_j(w)S_K(w)-1}.
\end{equation}
First we need to express (\refeq{eq:delta}) as a system of linear equations, let us simplify it by writing
\begin{equation*}
	\label{eq:simp}
	\begin{split}
		D_k &:= D_k(w), \\
		S_K &:= S_K(w), \\
		\phi_{kl} &:= \phi_{kl}(w),\\
		\psi_{kl} &:= \theta_{kl}\phi_{kl}, \\
		\gamma_k &:= D_kS_K, \\
		\alpha_j &:= \gamma_j-1.
	\end{split}
\end{equation*}
using this, (\refeq{eq:delta}) can be re-arranged into
\begin{equation}
	\label{eq:delta_split}
	\begin{split}
		\Delta_{jK}(w)&= \frac{\D \Big(\sum_{k\in K} \frac{E_k + \sum_l \psi_{kl}}{D_k} \Big)/ S_K - \Big(E_j + \sum_l \psi_{jl}\Big)}{\alpha_j} \\
		&= \sum_k \frac{E_k}{\gamma_k\alpha_j} + \sum_{kl} \frac{\psi_{kl}}{\gamma_k\alpha_j} - \sum_{l} \frac{\psi_{jl}}{\alpha_j} - \frac{E_j}{\alpha_j}\\
		&= \sum_k \frac{E_k}{\gamma_k\alpha_j} + \sum_{kl} \frac{\psi_{kl}}{\gamma_k\alpha_j} - \sum_{l} \frac{\delta_{jk}\psi_{kl}}{\alpha_j} - \frac{E_j}{\alpha_j}\\
		&= \sum_{kl} \frac{\psi_{kl} (1-\gamma_k\delta_{jk})}{\gamma_k\alpha_j} - \Big(E_j - \sum_k\frac{E_k}{\gamma_k}\Big)/\alpha_j,
	\end{split}
\end{equation}
where $\delta_{jk}$ is the Kronecker delta. Hence if we pick $w = w_m$, then
\begin{equation}
	\begin{split}
		\Delta_{jK}(w_m) &= \sum_{kl} A_{jm, kl} \theta_{kl} - b_{jm}, \\
		&= A_{jm,:}\theta - b_{jm} = (A\theta - b)_{jm}
	\end{split}	
\end{equation}
where
\begin{equation}
	A_{jm, kl} = \frac{\phi_{kl}(w_m)(1 - \gamma_k(w_m) \delta_{jk})}{\gamma_k(w_m) \alpha_j(w_m)},\\
\end{equation}
\begin{equation}
	b_{jm} = \Big(E_j - \sum_k\frac{E_k}{\gamma_k(w_m)}\Big)/\alpha_j(w_m).
\end{equation}

\subsection{Gaussian kernel}
\label{subsec:gaussian}
Let $A$ be the data matrix, if it is constructed by the Gaussian kernel then
\begin{equation}
	\label{eq:gaussian}
	A_{ij} = \text{exp}(-\|w_{i:} - w_{j:}\|^2 / 2\sigma), ~~\text{for }i \in K, j \in K,
\end{equation}
where
\begin{equation}
	\sigma = c\sigma_0,
\end{equation}
where $c \in \mathbb{R}$ is a hyperparameter, and
\begin{equation}
	\sigma_0 = \frac{1}{Nn_K}\sum_{i \in T, j \in K}\text{exp}(-\|w_{i:} - w_{j:}\|^2).
\end{equation} 
The target vector is the energies indexed by $K$:
\begin{equation}
	\label{eq:lineartargetvector}
	b_i := E_{i \in K}.
\end{equation}

\subsection{Linear least squares}
\label{subsec:LLS}
The linear least squares (LLS) model simply uses the feature matrix as the data matrix, hence
\begin{equation}
	\label{eq:lls}
	A_{ij} := F_{i\in K, j},
\end{equation}
with the target vector in (\ref{eq:lineartargetvector}).

\subsection{Neural network}
\label{subsec:NN}
\at{insert standard neural network formula here}


\iffalse
&= \frac{\D \left(\sum_{k\in K} \frac{E_k + \sum_l \theta_{kl} \phi_{kl}(w)}{D_k(w)} / S_K(w)\right) - \left(E_j + \sum_l \theta_{jl} \phi_{jl}(w)\right)}{D_j(w)S_K(w)-1}, \\
\fi

\section{Intermediate values}
\label{sec:intermediate}

\subsection{Shepard model}
Several intermediate values need to be computed and stored in the random access memory (RAM), since these intermediate values depend on several parameters which changes for each experiment and are fast to compute hence it is not necessary to store them in the non-volatile memory. Given the stored $F, D$, $T$, $\Phi$ and $\Phi'$:
\begin{itemize}
	\item Determine the set of labels of the initial trial points
	\begin{equation}
		K \subset T,
	\end{equation}
	for example, we can take the first 100 labels of $T$ as $K$.
	\item Compute the set of labels of the unsupervised data points
	\begin{equation}
		U := T \setminus K.
	\end{equation}
	\item Compute the set of of labels of the data points for evaluation
	\begin{equation}
		W := \{1,2,...,n_\text{data}\} \setminus K.
	\end{equation}
	\item Compute intermediate values:
	\begin{equation}
		\label{eq:intermediate}
		\begin{split}
			S_K &\in \mathbb{R}^{n_U} := S_K(F_{m:}) \text{ for } m \in U,\\
			\gamma &\in \mathbb{R}^{n_U \times n_K} := \gamma_k(F_{m:}) \text{ for } k \in K, m \in U, \\
			\alpha &:= \gamma - 1.
		\end{split}
	\end{equation}
	where $n_U = |U|.$
	\item Given the splines in (\ref{eq:spline}), compute
	\begin{equation} % ϕ[l,m] - ϕ[l, k] - dϕ[l, k]*(F[t,m]-F[t,k])
		\phi_{m, kl} := \Phi_{ml} - \Phi_{kl} - \Phi'_{kl}(F_{mt}-F_{kt}) \quad \text{for } m \in U, \text{ }k \in K, \text{ }l = 1,...,n_L,
	\end{equation}
	where
	\begin{equation}
		\begin{split}
			%t &\leftarrow \hat{t} = 0 \text{ ? } n_f : \hat{t},
			t = 
			\begin{cases}
				n_f,  & \text{if } \hat{t} = 0 \\
				\hat{t}, & \text{otherwise}
			\end{cases}
			~~ 
		\end{split}
	\end{equation}
	with
	\begin{equation}
		\hat{t} := l \text{ mod } n_f,
	\end{equation}
	this makes sure the correct column is indexed, since $\phi_b'(w)$ is only non zero at the $t$th column position. This results in matrix $\phi$ where the block column is indexed by $k,l$ indices.
\end{itemize}

\section{Fitting the model}
\subsection{Shepard model}
A good prediction model is reflected by small MAD or RMSD, hence a minimization routine is required. In order to solve the minimzation problem, a linear sysem or least squares formulation is needed. For example, if the RMSD is chosen as the objective function then the problem formulation would be
\begin{equation}
    \label{eq:min}
    \begin{split}
        f_{\text{obj}}(\theta):= |K| \sum_m \text{RMSD}_K(w_m)^2 = \sum_m \sum_j \Delta_{jK}(w_m)^2 = \sum_{jm} \left( A\theta - b\right)_{jm}^2
         = \|A \theta - b\|_2^2,
    \end{split}
\end{equation}
where $A$ is the data matrix defined in section (\ref{sec:model}) with $n_Kn_U$ rows and $n_Kn_L$ columns, $\theta$ is the coefficient vector, and $b$ is the target vector.

For storage efficiency, it is desirable to not explicitly form the data matrix $A$, since it grows quickly with more data points included in the training set $K$ and testing set $T$. 
We can alleviate the storage problem by writing a routine for computing $Au$ where $u$ is a variable vector, hence instead of having a matrix with $\mathcal{O}(M^2NL)$ storage we would have a vector with only $\mathcal{O}(N)$ storage. 
This can be done by augmenting the first sum of the last line of (\refeq{eq:delta_split}) with the pre-computed intermediate values in (\refeq{eq:intermediate}):
\begin{equation}
	Au = \sum_{kl} \frac{\phi_{m, kl} u_{kl} (1-\gamma_{m k}\delta_{jk})}{\gamma_{mk}\alpha_{mj}},
\end{equation}
Here, CGLS \at{cite Hestenes} method is used to minimize the objective function in (\refeq{eq:min}). The memory-less form of CGLS requires not only $Au$ but also $A^\top v$ routine, which is
\begin{equation}
	\begin{split}
		A^\top v = \sum_{jm} \frac{\phi_{m, kl} v_{jm} (1-\gamma_{m k}\delta_{jk})}{\gamma_{mk}\alpha_{mj}}.
	\end{split}
\end{equation}
Given the $Au$ and $A^\top v$ routine, and $b$, the CGLS is called as follows \at{cite Krylov.jl and LinearOperators.jl}:
\begin{equation*}
	\begin{split}
		\text{op} &\leftarrow \text{LinearOperators}(y, Au, A^\top v), \\
		\theta &\leftarrow \text{CGLS}(\text{op}, b, \text{itmax}),
	\end{split}
\end{equation*}
where $y$ is an arbitrary sized vector of real numbers (in Julia, $y$ can be initialized by \codeword{y = Vector{Float64}()}) and \codeword{itmax} is the maximum number of iteration.
\subsection{Linear models}
The Gaussian kernel and LLS models can be fitted in a straightforward manner, the linear system is
\begin{equation}
	A \theta = b,
\end{equation}
this can be implemented simply by
\begin{equation*}
	\theta \leftarrow \codeword{CGLS(}A, b,\codeword{itmax)},
\end{equation*}
$A$ is constructed explicitly here since it is cheap and fast to compute with only a maximum of 100 row size.

\subsection{Neural network}
\at{standard neural network fitting procedure}


\section{Program structure}
The main program is divided into two main subprograms: data setup and fitter. Data setup includes section (\ref{sec:feature}) up to section (\ref{sec:data}), except for the raw feature extraction, which is done only once for each feature set (for example, the extraction of ACSF upon QM9 dataset is only done once). In general, everything that are independent of $\theta$ and the indexing from set $T$ are included into the data setup. Meanwhile the fitter subprogram computes everything which depend on $\theta$ or set $T$.

\subsection{Data setup subprogram}
\label{sub:data}
The data setup function abstraction is
\begin{equation*}
	(F, T, D, \phi, \phi') \leftarrow \codeword{data_setup(indices}, n_{af}, n_{mf}, n_s, N, \codeword{feature_file}, \codeword{feature_mode}),
\end{equation*}
where \codeword{indices} is the set of labels which will be included, in case of full QM9 dataset then \codeword{indices} = $1:N_\text{QM9}$; \codeword{feature_file} is a string which indicates the file path of a file containing the atomic feature set computed on all of the QM9 data points; \codeword{feature_mode} is a list of boolean which indicates the inclusion of sums of squares or binomial features, e.g., \codeword{feature_mode = [true, false]} means only the sums of squares will be added to the molecular features. The flow of the data setup subprogram is:
\begin{enumerate}
	\item $F \leftarrow$ \codeword{PCA_atom(feature_file,} $n_{af}$\codeword{)}
	\item $F \leftarrow$ \codeword{extract_mol_feature(}$F$\codeword{, feature_mode)}
	\item $F \leftarrow$ \codeword{PCA_mol(}$F, n_{mf}$\codeword{)}
	\item $(\phi, \phi') \leftarrow$ \codeword{extract_bspline(}$F, n_s$\codeword{)} 
	\item $(T, D) \leftarrow$ \codeword{get_centers(}$F, N$\codeword{)}
\end{enumerate}

\subsection{Fitter subprogram}
\label{sub:}
Given the set of files from section (\ref{sub:data}), the fitter function abstraction is simplyh
\begin{equation}
	(\theta, \codeword{info}) \leftarrow \codeword{fitter(data_dir, t_limit, batchsize)}
\end{equation}
where \codeword{data_dir} is a string that refers to the path which contains the files resulted from the data setup subprogram, \codeword{t_limit} is the solver time limit (in second), \codeword{batchsize} is an integer representing the size of mini-batch of several variables, in some cases the total memory usage of the system is over 8GB (typical Random Access Memory (RAM) size of modern computers) hence mini-batching is necessary. On the output side, \codeword{info} contains the fitting errors, list of timings, and list of fitting hyperparameters (e.g., $n_{af}, n_{mf},$ etc). The processes executed in the fitter are:
\begin{enumerate}
	\item Compute variables for fitting:
	\begin{enumerate}
		\item $K \leftarrow $\codeword{select_centers(}$T$\codeword{)}
		\item $U \leftarrow T \setminus K$
		\item $W \leftarrow \{1,2,.., n_\text{data}\} \setminus K$ 
		\item $S_K \leftarrow\codeword{compute_SK(}D, K, U\codeword{)}$
		\item $\gamma \leftarrow \codeword{compute_gamma(}D, S_K, K, U\codeword{)}$
		\item $\alpha \leftarrow \gamma .- 1$
		\item $\phi \leftarrow \codeword{compute_phi(}\Phi, \Phi', F, K, U\codeword{)}$
	\end{enumerate}
	\item Fitting:
	\begin{enumerate}
		\item $b \leftarrow \codeword{compute_b(}E, \gamma, \alpha, K)$, where $E$ is a vector of energies.
		\item $\codeword{op} \leftarrow \codeword{LinearOperators(}y, Au, A^\top v \codeword{)}$
		\item $\theta \leftarrow \codeword{CGLS(op, b, itmax, t_limit)}$
	\end{enumerate}
	\item Energy prediction is computed in mini-batches in terms of the row numbers, i.e., for any vectors or matrices, the maximum row size is \codeword{batchsize}; e.g., $S_K$ vector for prediction has $N_\text{QM9}$ row size by default, however when computed in mini-batches, $S_K$ has \codeword{batchsize} rows for each mini-batch loop, where the total number of loops is $\codeword{ceil}(N_\text{QM9}/\codeword{batchsize})$.
	\begin{enumerate}
		\item $S_K \leftarrow\codeword{compute_SK(}D, K, W\codeword{)}$
		\item $\phi \leftarrow \codeword{compute_phi(}\Phi, \Phi', F, K, W\codeword{)}$
		\item $V_K \leftarrow \codeword{predict(}\theta, E, D, \phi, S_K, K, W\codeword{)}$
	\end{enumerate}
	\item Compute the errors:
	\begin{enumerate}
		\item $\text{MAE} \leftarrow \codeword{get_MAE(}V_K, E)$
		\item $\text{MAD} \leftarrow \codeword{get_MAD(}V_K, E, D, \theta, \phi, S_K, K, U\codeword{)}$, where MAD is a vector of length $n_U$. Note that the variables used to compute the MAD are the ones from step 1 (for fitting), not the ones from step 3.
	\end{enumerate}
\end{enumerate}


\section{Numerical experiments}
The numerical experiments are based on a set of main hyperparameters:
\begin{equation}
	N = 300,~~ n_K = 100, ~~ n_f = 28, ~~ n_s = 5, ~~ n_L = n_fn_s = 140, ~~ \codeword{t_limit} = 900
\end{equation}
which are then extended in a \textit{binary search} manner. Each \codeword{data_setup()} process roughly takes around 300s. 
The fitting is done using \codeword{CGLS()} with $\codeword{itmax} = 500$; specifically for the Shepard model, it requires the functions to compute $Au$ and $A^\top v$ as the input variable. 
Currently active learning scheme has not been tested.

The hardware and software specification used for the experiments are:
\begin{itemize}
	\item Processors: 4 $\times$ Intel\textregistered Core i5-4670S CPU @3.10 GHz
	\item Memory: 7.7 GB of RAM
	\item Programming language: Julia 1.7.3
\end{itemize}

\subsection{Fitting results}
\label{subsec:fitting}
The true error of the fitting is described by the mean absolute error (MAE) of the energies
\begin{equation}
	\text{MAE} := \frac{1}{N_{QM9}}\sum_{m \in W}|E_{m} - \hat{E}_m|,
\end{equation}
where $\hat{E}$ is the vector of predicted energies. Here $N_\text{QM9} = 133628$, $n_K = 100$, $n_U = 200$, and $N = 300$ for all of the experiments. 

\subsubsection{Shepard model}
\label{subsubsec:shepard}
Several hand-picked fitting experiment results of the with the lowest MAE are shown in table \ref{tab:shepard_exp}.

\begin{table}[H]
	\centering
	\caption{Selected fitting experiment results. \textbf{ft\_sos} indicates the inclusion of sums of squares features meanwhile \textbf{ft\_bin} indicates the inclusion of binomial features; \textbf{t\_solver} is the time spent by CGLS; and \textbf{t\_pred} is the energy prediction time. \textbf{MAE} is in kcal/mol.}
	\begin{tabular}{|l|l|l|l|l|l|l|l|}
		\hline
		\multicolumn{1}{|c|}{\textbf{$n_{af}$}} & \multicolumn{1}{c|}{\textbf{$n_{mf}$}} & \multicolumn{1}{c|}{\textbf{$n_s$}} &\multicolumn{1}{c|}{\textbf{MAE}} & \multicolumn{1}{c|}{\textbf{ft\_sos}} & \multicolumn{1}{c|}{\textbf{ft\_bin}} & \multicolumn{1}{c|}{\textbf{t\_solver}} & \multicolumn{1}{c|}{\textbf{t\_pred}} \\ \hline
		4                                    & 18                                  & 5                                      & 2335                        & 0                                     & 0                                     & 409                                   & 93                                  \\ \hline
		4                                    & 19                                  & 5                                      & 2413                        & 0                                     & 0                                     & 445                                   & 113                                 \\ \hline
		4                                    & 26                                  & 5                                      & 2623                        & 0                                     & 0                                     & 566                                   & 160                                 \\ \hline
		4                                    & 23                                  & 5                                      & 2647                        & 1                                     & 0                                     & 529                                   & 136                                 \\ \hline
		6                                    & 28                                  & 5                                      & 2651                        & 0                                     & 0                                     & 623                                   & 179                                 \\ \hline
	\end{tabular}
	\label{tab:shepard_exp}
\end{table}

\subsubsection{Fitting with reduced energies}
\label{subsubsec:ered}
Fitting the reduced molecular energy serves to reduce the molecular energy fitting difficulty by decreasing the order of magnitude of the fitted energy, since by default the molecular energy itself is quite large (e.g., the internal energy at $0$ Kelvin $U_0$ of QM9 molecules are between $\mathcal{O}(100)$ to $\mathcal{O}(1000)$ in Hartree unit). We can obtain the reduced energy of a molecule by removing the sum of atomic energies in each molecule
\begin{equation}
	E^\text{red} = E - \sum_{j \in P} N_j E^\text{atom}_j,
\end{equation}
where $P$ is a set of atom types, $N_j$ is the count of the atom type $j$ within a fixed molecule, and $E^\text{atom}_j$ is the the energy of atom type $j$.

The atomic energy itself can be obtained by fitting a LLS model. For QM9 challenge, (\ref{eq:lls}) can be used to obtain the atomic energies, explicitly by defining
\begin{equation}
	\begin{split}
		A_{i:} &:= [N^i_\text{H}, N^i_\text{C}, N^i_\text{O}, N^i_\text{N}, N^i_\text{F}], \\
		\theta &:= [E^\text{atom}_\text{H}, E^\text{atom}_\text{C}, E^\text{atom}_\text{O}, E^\text{atom}_\text{N}, E^\text{atom}_\text{F}], \\
		b_i &:= E_i, \\
		\text{for }i &\in K.
	\end{split}
\end{equation}
Using the set of index $K$ obtained in the same way as the one in the first row of table \ref{tab:shepard_exp}, the result of the atomic energies fitting is shown in table \ref{tab:atomic_exp}. The MAE from fitting the atomic energies is in fact refers to the mean absolute value of the $E_\text{red}$ of all molecules in the QM9 dataset itself.

\begin{table}[H]
	\centering
	\caption{Result of fitting for atomic energies. \textbf{MAE} and the energies are in kcal/mol.}
	\begin{tabular}{|l|l|l|l|l|l|}
		\hline
		\multicolumn{1}{|c|}{\textbf{MAE}} & \multicolumn{1}{c|}{$E^\text{atom}_\text{H}$} & \multicolumn{1}{c|}{$E^\text{atom}_\text{C}$} & \multicolumn{1}{c|}{$E^\text{atom}_\text{O}$} & \multicolumn{1}{c|}{$E^\text{atom}_\text{N}$} & \multicolumn{1}{c|}{$E^\text{atom}_\text{F}$} \\ \hline
		21.2  &	-376.5  & -23888.9 & -34349.3 & -47200.6 & -62662.2 \\ \hline
	\end{tabular}
	\label{tab:atomic_exp}
\end{table}

The fitting experiments with $E_\text{red}$ as the target are run using the best obtained feature set (first row of table \ref{tab:shepard_exp}) while varying the possible models (table \ref{tab:red_exp}). The neural network (NN) architecture used here is composed of \codeword{nodes }$ = [n_{mf}, 10, 1]$, where each entry represents the number of nodes in $i$th layer; ReLU is chosen as the activation function in the hidden layer (2nd layer); the NN is optimized using Adam with 0.01 momentum \at{cite Adam}. Note that after prediction, the MAE is computed from
\begin{equation}
	\text{MAE} = \frac{1}{N_{QM9}} \sum_{m \in W} |E_m - (\hat{E}_m + E_m^\text{red})|,
\end{equation}
hence it recovers the original order of magnitude of the energies.

\begin{table}[H]
	\centering
	\caption{Fitting using reduced energy $E_\text{red}$. RoSeMI refers to the Shepard model, KRR refers to the Gaussian kernel model, LLS is the linear least squares model, and NN is the neural network model. \textbf{MAE} is in kcal/mol.}
	\begin{tabular}{|l|l|}
		\hline
		\multicolumn{1}{|c|}{\textbf{model}} & \multicolumn{1}{|c|}{\textbf{MAE}} \\ \hline
		RoSeMI & 23.7 \\ \hline
		KRR & 21.3 \\ \hline
		NN & 21.1 \\ \hline
		LLS & 19.4 \\ \hline
	\end{tabular}
	\label{tab:red_exp}
\end{table}

\subsection{Memory usage}
The computation of fitting and prediction of QM9 challenge is resource-demanding, hence we need to allocate variables responsibly such that the OS does not throw an out-of-memory exception. The first ingredient is to approximate the size of data structures allocated within the program in megabytes (MB) order of magnitude; assuming the data structure holds \codeword{Float64} entries (which is the most common data type for numerical operations and is also the most memory consuming) this can be done by a general formula
\begin{equation}
	\codeword{mem_usage} = (8\times 10^{-6}) \times \prod_{i}\codeword{size(data_structure, i)}, \text{ for } i = 1,2,...
\end{equation}
where \codeword{size(data_structure, i)} returns the number of entries of the $i$th dimension of a data structure. For example, the distance matrix $D \in \mathbb{R}^{133628 \times 300}$ with \codeword{Float64} data type, then
\begin{equation}
	B_NX := B_N \times 300 = 320.7072 \text{ MB},
\end{equation}
where
\begin{equation}
	B_N := (8\times 10^{-6}) \times 133628 \approx 1.069
\end{equation}
is the storage needed for $N$ entries of \codeword{Float64} in MB.
Using Julia this can be confirmed easily by calling \codeword{Base.summarysize(D)*1e-6}, which gives the same result up to some roundoff error.

The variable which has the largest memory usage from the experiment in the last row of table \ref{tab:shepard_exp} (experiment with largest hyperparameters)  is the block matrix $\phi$  for prediction, with
\begin{equation}
	B_NXY := B_N \times 100 \times 140 = 14966.336 \text{ MB} = 14.9 \text{ GB}
\end{equation}
memory size. Hence mini-batching is necessary, with \codeword{batchsize = 10_000}, for each mini-batch loop the size of $\phi$ becomes $1/13$ of its actual size which is around 1GB. 


\iffalse
========= some comments========== \\
$b$ must of course be independent of $\theta$.

In (8) there is no argument $w$; instead you need to minimize the expression
\begin{equation}
    |K| \sum_m RMSD_K(w_m)^2,
\end{equation}
where $m$ labels the molecules. To find the linear system you need to write the right hand side of (7) as
\begin{equation}
    \sum_{kl} A_j(w)_{kl} \theta_{kl} -b_j(w)
\end{equation}
and find the expressions for $A_j(w)_{kl}$ and $b_j(w)$ by comparing the coefficients of $\theta_{kl}$ and the $\theta$-independent terms. For $A_j(w)_{kl}$ you get a sum of two terms, one involving a Kronecker delta. For $b_j(w)$ you get a sum over $k$ and an additional term.
The least squares problem in (8) then has a matrix $A$ with rows indexed by $j,m$ and columns indexed by $k,l$, with
\begin{equation}
    A_{jm,kl}:=A_j(w_m)_{kl},
\end{equation}
and something similar for the entries of $b$.

================== \\
You just need to extract from (7) the terms containing $\theta_{kl}$, which are

\begin{equation}
    \frac{\phi_{kl}}{D_kS_K(D_jS_K-1)}
\end{equation}


and
\begin{equation}
    \frac{\delta_{jk}\phi_{kl}D_kS_K(D_jS_K-1)}{D_jS_K-1}.
\end{equation}


Add the two and simplify, taking out common factors.

Similarly, you extract the terms without theta's by setting all $\theta$'s to zero

================== \\

Please write a program for calculating the vector v with components $v_{jm}:=\Delta_{jK}(w_m)$ using (3), (2), (1), and (7a) of your writeup, adding in (3) only the nonzero terms. Compare times with those for computing $A\theta-b$. The numerical values should be the same, up to roundoff.

\fi

\end{document}
