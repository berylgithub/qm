\documentclass[12pt]{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

%macros
\newcommand{\defeq}[2]{\stackrel{\mathclap{\normalfont\mbox{#1}}}{#2}}
\def\D{\displaystyle}
\def\att{                    % mark at the margin
        \marginpar[ \hspace*{\fill} \raisebox{-0.2em}{\rule{2mm}{1.2em}} ]
        {\raisebox{-0.2em}{\rule{2mm}{1.2em}} }
        }
\def\at#1{[*** \att #1 ***]}  % text needing attention
\def\spc{\hspace*{0.5cm}}



\title{RoSeMI: Robust Shepard model for interpolation}
%\author{Beryl Ramadhian Aribowo}

\begin{document}
\maketitle


\section{The model}
The generalized Shepard model has the form of 
\begin{equation}
    \label{eq:shepard}
    V_K(w):=R_K(w)/S_K(w) ~~~ \text{ for } w \in \mathbb{R}^m \setminus \{w_k\mid k\in K \},
\end{equation}
where
\begin{equation}
    R_K(w):=\sum_{k\in K} \frac{V_k(w)}{D_k(w)},~~~
    S_K(w):=\sum_{k\in K} \frac{1}{D_k(w)},
\end{equation}
and
\begin{equation}
    \label{eq:vk}
    V_k(w) = E_k + \sum_l \theta_{kl} \phi_{kl}(w).
\end{equation}
By (\ref{eq:vk}), (\ref{eq:shepard}) can be expanded into
\begin{equation}
    \label{eq:vk_expand}
    V_K(w) := \sum_{k\in K} \frac{E_k + \sum_l \theta_{kl} \phi_{kl}(w)}{D_k(w)} / S_K(w). 
\end{equation}
The quality of the prediction accuracy is measured by the \textbf{mean absolute deviation}
\begin{equation}
    \text{MAD}_K(w) := \frac{1}{|K|}\sum_{j\in K}|\Delta_{jK}(w)|
\end{equation}
and the \textbf{root mean square deviation}
\begin{equation}
    \text{RMSD}_K(w) := \sqrt{\frac{1}{|K|}\sum_{j\in K}\Delta_{jK}(w)^2},
\end{equation}
where
\begin{equation}
    \label{eq:delta}
    \begin{split}
        \Delta_{jK}(w)&:=\D\frac{V_K(w)-V_j(w)}{D_j(w)S_K(w)-1} \\
        &\defeq{(\refeq{eq:vk},\refeq{eq:vk_expand})}{=} \frac{\D \left(\sum_{k\in K} \frac{E_k + \sum_l \theta_{kl} \phi_{kl}(w)}{D_k(w)} / S_K(w)\right) - \left(E_j + \sum_l \theta_{jl} \phi_{jl}(w)\right)}{D_j(w)S_K(w)-1}, \\
    \end{split}
\end{equation}
and $l=1,2,...L$.
Let us simplify the expressions
\begin{equation*}
	\label{eq:simp}
    \begin{split}
        D_k &:= D_k(w), \\
        S_K &:= S_K(w), \\
        \phi_{kl} &:= \phi_{kl}(w),\\
        \psi_{kl} &:= \theta_{kl}\phi_{kl}, \\
        \gamma_k &:= D_kS_K, \\
        \alpha_j &:= D_jS_K-1.
    \end{split}
\end{equation*}
By continuing from the last line of (\refeq{eq:delta}) in a simplified expression, then (\refeq{eq:delta}) can be re-arranged into
\begin{equation}
    \label{eq:delta_split}
    \begin{split}
        \Delta_{j,K}(w)&= \frac{\D \left(\sum_{k\in K} \frac{E_k + \sum_l \psi_{kl}}{D_k} / S_K\right) - \left(E_j + \sum_l \psi_{jl}\right)}{\alpha_j} \\
        &= \sum_k \frac{E_k}{\gamma_k\alpha_j} + \sum_{k,l} \frac{\psi_{kl}}{\gamma_k\alpha_j} - \sum_{l} \frac{\psi_{jl}}{\alpha_j} - \frac{E_j}{\alpha_j}\\
        &= \sum_k \frac{E_k}{\gamma_k\alpha_j} + \sum_{k,l} \frac{\psi_{kl}}{\gamma_k\alpha_j} - \sum_{l} \frac{\delta_{jk}\psi_{kl}}{\alpha_j} - \frac{E_j}{\alpha_j}\\
        &= \sum_{kl} \frac{\psi_{kl} (1-\gamma_k\delta_{jk})}{\gamma_k\alpha_j} - \left(\frac{E_j}{\alpha_j} - \sum_k\frac{E_k}{\gamma_k\alpha_j}\right),
    \end{split}
\end{equation}
where $\delta_{jk}$ is the Kronecker delta. Hence if we pick $w = w_m$, then
\begin{equation}
	\begin{split}
		\Delta_{jK}(w_m) &= \sum_{kl} A_{(jm), (kl)} \theta_{kl} - b_{jm}, \\
	\end{split}	
\end{equation}
where
\begin{equation}
	\begin{split}
		A_{(jm), (kl)} &= \frac{\phi_{k,l}(w_m)(1 - \gamma_k(w_m) \delta_{jk})}{\gamma_k(w_m) \alpha_j(w_m)},\\
		b_{jm} &= \frac{E_j}{\alpha_j(w_m)} - \sum_k\frac{E_k}{\gamma_k(w_m)  \alpha_j(w_m)}.
	\end{split}
\end{equation}

\iffalse
In order to see the general pattern of (\refeq{eq:delta}), let us see a simple example with $j = 1$, $|K| = 2$, and $L = 2$; and using simplified expressions by omitting the arguments
\begin{equation*}
    \begin{split}
        D_k &:= D_k(w), \\
        S_K &:= S_K(w), \\
        \phi_{k,l} &:= \phi_{k,l}(w),
    \end{split}
\end{equation*}
hence
\begin{equation}
    \begin{split}
        \Delta_{1,K}(w)&:= \frac{\D \frac{ E_1 + \theta_{1,1}\phi_{1,1} + \theta_{1,2}\phi_{1,2}}{D_1S_K} + \frac{E_2 + \theta_{2,1}\phi_{2,1} + \theta_{2,2}\phi_{2,2}}{D_2S_K} - (E_1 + \theta_{1,1}\phi_{1,1} + \theta_{1,2}\phi_{1,2})}{D_1S_K-1} \\
        &= \D \frac{ E_1 + \theta_{1,1}\phi_{1,1} + \theta_{1,2}\phi_{1,2}}{D_1S_K(D_1S_K-1)} + \frac{E_2 + \theta_{2,1}\phi_{2,1} + \theta_{2,2}\phi_{2,2}}{D_2S_K(D_1S_K-1)} - \frac{(E_1 + \theta_{1,1}\phi_{1,1} + \theta_{1,2}\phi_{1,2})}{D_1S_K-1} \\
        &= \dots
    \end{split}
\end{equation}
\fi

A good prediction model is reflected by small MAD or RMSD, hence a minimization routine is required. In order to solve the minimzation problem, a linear sysem or least squares formulation is needed. For example, if the RMSD is chosen as the objective function then the problem formulation would be
\begin{equation}
    \label{eq:min}
    \begin{split}
        \min |K| \sum_m \text{RMSD}_K(w_m)^2 &= \min \sum_m \sum_j \Delta_{jK}(w_m)^2 \\
        &= \min \sum_m \left\| A_{(:,m),(:,:)}\theta - b_{:,m}\right\|_2^2 \\
        f_{\text{obj}} &:= \min \|A \theta - b\|_2^2,
    \end{split}
\end{equation}
where $A$ is the data matrix with $MN$ row size and $ML$ column size; where $M$ is the number of data points with known energies, $N$ is the number of data points without energies, and $L$ is the number of the basis features; meanwhile $\theta$ is the coefficient vector, and $b$ is the target vector.

The initial case study has the following parameters:
\begin{equation*}
	\begin{split}
		M &= 10,\\
		N &= 240,\\
		L &= 320, \\
		\text{row\_count} &= 2400,\\
		\text{col\_count} &= 3200, \\
		\text{mem\_size} &\approx 61.44 \text{ megabytes}, \\
		\text{M\_increment} &= 10, \\
		\text{fit\_iter} &= 15. \\
		\text{sparsity\_rate} &\approx 0.7.
	\end{split}
\end{equation*}
Hence in the final fitting-iteration, the linear system would have $15000 \times 48000$ sized data matrix $A$ with mem\_size $\approx 5.7$ gigabytes. The model is solved by using CGLS method with 400 maximum iterations. For the first two fitting-iteration this gives 5 and 20 seconds of fitting-time consecutively. 




\section{The program flow}
Below is the rough flow of the program for fitting the QM9 dataset.
\begin{itemize}
    \item Compute the molecular features using ACSF (\at{cite Behler}). This would give a vector with length $= 51$ (i.e., $n^\text{atom}_f = 51$) for each atom. Apply this to all molecules in the dataset hence there should be a total of $n_\text{mol}$ ACSF extractions.
    \item For each molecule containing $n_\text{atom}$ number of atoms, symmetrize the features by
        \begin{equation}
            \begin{split}
                F_{S_j} &= \sum_{i=1}^{n_{\text{atom}}} F_{ji},\\
                F_{Q_j} &= \sum_{i=1}^{n_{\text{atom}}} F_{ji}^2, \\
                \text{for }j &= 1,2,..., n^\text{atom}_f. 
            \end{split}
        \end{equation}
        then by concatenating the vectors such that
        \begin{equation}
            F := (F_{S_1}, F_{S_2}, ..., F_{S_{n^\text{atom}_f}}, F_{Q_1},F_{Q_2} ..., F_{Q_{n^\text{atom}_f}}) \in \mathbb{R}^{n_f}.
        \end{equation}
    	then we stack the feature vector $F$ of each molecule as a row in the $W \in \mathbb{R}^{n_\text{mol} \times n_f}$ matrix.
    \item Do feature selection to take the subset of the columns of $W$, this can be done by the following:
	    \begin{equation}
			\begin{split}
				\hat{s} &= \sum_i W_{i:}, \\
				S &= \sum_i W_{i:}W_{i:}^\top, \\
				\text{ for } i &= 1,2,...,n_{\text{mol}}. \\
			\end{split}
	    \end{equation}
		then we compute the mean feature vector and covariance matrix
		\begin{equation}
			\begin{split}
				\bar{u} &= \hat{s}/n_\text{mol}, \\
				C &= S/n_\text{mol} - \bar{u}\bar{u}^\top. \\			
			\end{split}
		\end{equation}
		We select the eigenvectors such that they correspond to the $n_\text{select} \in \mathbb{Z}^+$ largest eigenvalues, they are obtained by first doing the spectral decomposition of the covariance matrix
		\begin{equation}
			C = Q\Lambda Q^\top,
		\end{equation}
		where $\Lambda$ is the diagonal matrix containing the $k$th eigenvalue $\Lambda_{kk}$ and $Q$ is the matrix containing the $k$th eigenvector $Q_{:k}$, then we re-order the column index of $Q$ and the entries of $\Lambda$ such that
		\begin{equation}
			\Lambda_{11} \geq \Lambda_{22} \geq  ... \geq \Lambda_{n_fn_f},
		\end{equation}
		then we select
		\begin{equation}
			\hat{Q} = Q_{:, 1:n_\text{select}}.
		\end{equation}
		Finally the transformed feature can be obtained by
		\begin{equation}
			W_{i:} := \hat{Q}^\top(W_{i:} - \bar{u}), \text{ for }i = 1,2,...,n_\text{mol}.
		\end{equation}
	\item Scale $W$ such that 
	\begin{equation}
		W_{ij} \in [0, 1]
	\end{equation}
	by
	\begin{equation}
		\begin{split}
			W_{:j} := (W_{:j} - \underbar{$W_{:j}$})/ (\overline{W_{:j}} - \underbar{$W_{:j}$}), \text{ for } j = 1,2,..., n_f.
		\end{split}
	\end{equation}
	where \underbar{$W_{:j}$} is the smallest value of the $j$th feature and $\overline{W_{:j}}$ is the largest.
	
    \item Obtain the centers by using the farthest-distance-algorithm variations from the transformed $\hat{F}$ matrix:
    	\begin{equation}
			\begin{split}
				content...
			\end{split}
    	\end{equation}
    
     this would give $M$ centers which would be used as the supervised data training
\end{itemize}



\iffalse
========= some comments========== \\
$b$ must of course be independent of $\theta$.

In (8) there is no argument $w$; instead you need to minimize the expression
\begin{equation}
    |K| \sum_m RMSD_K(w_m)^2,
\end{equation}
where $m$ labels the molecules. To find the linear system you need to write the right hand side of (7) as
\begin{equation}
    \sum_{kl} A_j(w)_{kl} \theta_{kl} -b_j(w)
\end{equation}
and find the expressions for $A_j(w)_{kl}$ and $b_j(w)$ by comparing the coefficients of $\theta_{kl}$ and the $\theta$-independent terms. For $A_j(w)_{kl}$ you get a sum of two terms, one involving a Kronecker delta. For $b_j(w)$ you get a sum over $k$ and an additional term.
The least squares problem in (8) then has a matrix $A$ with rows indexed by $j,m$ and columns indexed by $k,l$, with
\begin{equation}
    A_{jm,kl}:=A_j(w_m)_{kl},
\end{equation}
and something similar for the entries of $b$.

================== \\
You just need to extract from (7) the terms containing $\theta_{kl}$, which are

\begin{equation}
    \frac{\phi_{kl}}{D_kS_K(D_jS_K-1)}
\end{equation}


and
\begin{equation}
    \frac{\delta_{jk}\phi_{kl}D_kS_K(D_jS_K-1)}{D_jS_K-1}.
\end{equation}


Add the two and simplify, taking out common factors.

Similarly, you extract the terms without theta's by setting all $\theta$'s to zero
\fi

\end{document}
