\documentclass[12pt]{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{xparse}
\usepackage{textcomp}
\usepackage[toc,page]{appendix}
\setcounter{section}{0}
\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}
\captionsetup[table]{skip=10pt} % table skip

%macros
\newcommand{\defeq}[2]{\stackrel{\mathclap{\normalfont\mbox{#1}}}{#2}}
\def\D{\displaystyle}
\def\att{                    % mark at the margin
        \marginpar[ \hspace*{\fill} \raisebox{-0.2em}{\rule{2mm}{1.2em}} ]
        {\raisebox{-0.2em}{\rule{2mm}{1.2em}} }
        }
\def\at#1{[*** \att #1 ***]}  % text needing attention
\def\spc{\hspace*{0.5cm}}

\NewDocumentCommand{\codeword}{v}{%
	\texttt{\textcolor{blue}{#1}}%
}
\lstset{language=C,keywordstyle={\bfseries \color{blue}}}


\title{RoSeMI: Robust Shepard model for interpolation}
%\author{Beryl Ramadhian Aribowo}

\begin{document}
\maketitle

\section{Feature extraction}
\label{sec:feature_extraction}

\subsection{Basic atomic features}
\label{subsec:basic_atomic_features}
The very first piece which needs to be computed is the array of atomic features. 
Here we describe several common techniques for extracting atomic features. In general, each set of features results in an array data structure with $(n_\text{mol}, n^l_\text{atom}, n_{af})$. Typically this is stored in a vector of matrices, where each matrix has different row size, due to the possibility of each molecule having different number of atoms $n^l_\text{atom}$; meanwhile $n_{af}$ is always uniform across the array, it is possible that $n_{af}$ itself indexes a matrix, i.e., $n_{af} := n_\text{row} \times n_\text{col}$. 

\subsubsection{ACSF}
Atom-centered symmetry functions (ACSF) (\at{cite Behler}) computes 4 symmetry functions for each atom, they are composed of radial and angular functions. ACSF only considers the set of atomic coordinates as the primary input. For each molecule $l$, ACSF returns a matrix with $(n^l_\text{atom}, n_{af})$ size, where $n^l_\text{atom}$ is the number of atoms in molecule $l$ and $n_{af}$ is the number of generated features. The hyperparameters include (but not limited to) \at{this is based on the paper, in the Julia library \url{https://github.com/DescriptorZoo/ACSF.jl}, the default settings are unknown/currently unable to be changed}:
\begin{itemize}
	\item cut-off radius $r_\text{cut}$ (in Angstrom),
	\item width of the Gaussian,
	\item damping scalar.
\end{itemize}
Given the default settings, the output gives $n_{af} = 51$.

\subsubsection{SOAP}
Smooth overlap of atomic positions (SOAP) \at{recursive cite to dscribe py} encodes the atomic representation by expansion of Gaussian atomic density, based on spherical harmonics and radial basis functions.
Overall, SOAP has similar idea and structure to ACSF. The hyperparameters include, but not limited to:
\begin{itemize}
	\item cut-off radius $r_\text{cut}$ (in Angstrom),
	\item the number of radial basis functions $n_\text{rbf}$,
	\item maximum degree of spherical harmonics $n_\text{sphere}$.
\end{itemize} 
The initial hyperparameters were set to $(r_\text{cut}, n_\text{rbf}, n_\text{sphere}) = (8.0, 2, 2)$, this gives $n_{af} = 165$.

\subsubsection{FCHL}
Faber--Christensen--Huang--Lilienfeld (FCHL) \at{cite Faber et al \url{https://aip.scitation.org/doi/10.1063/1.5126701}}. Essentially FCHL encodes radial distribution by the two-body term, and encodes the mean distances and angular information by three-body term. FCHL considers atomic coordinates and the nuclear charges as the primary inputs. For each molecule, FCHL returns an array of size $(\max_l n^l_\text{atom}, 5, n_\text{neigh})$, where $n_\text{neigh}$ is the number of atomic neighbours; almost in all cases $n_\text{neigh} = \max_l n^l_\text{atom}$, the only exception is when periodic boundary cell mode is used (for crystals). The hyperparameters are: 
\begin{itemize}
	\item cut-off radius (in Angstrom), the default is $r_\text{cutoff} = 5.0$,
	\item number of neighbours, the default is $n_\text{neigh} = 23$,
	\item periodic cells boundaries, the default is \codeword{pbc = None}.
\end{itemize}
For the QM9 dataset, FCHL returns an array with size $(133885, 29, 5, 29)$.

\subsection{Feature selection}
\label{sec:feature}
\subsubsection{Atomic feature selection}
\begin{itemize}
	\item In order to select the most relevant features, we do the \textbf{principal component analysis} (PCA), first by computing the mean vectors and the matrix $S$ by
	\begin{equation}
		\begin{split}
			s &= \frac{1}{N_\text{QM9}}\sum_{l} \frac{1}{n^l_\text{atom}}\sum_{i} f^l_{i:}, \\ 
			S &= \frac{1}{N_\text{QM9}}\sum_{l} \frac{1}{n^l_\text{atom}} \sum_{i} f^l_{i:} (f^l_{i:})^\top, \\ 
			\text{for } l &= 1,...,N_\text{QM9}, ~~ i = 1,2,...,n^l_\text{atom},
		\end{split}
	\end{equation}
	where $n^l_\text{atom}$ is the number of atom in molecule $l$; then compute the covariance matrix
	\begin{equation}
		\label{eq:pca_atom_start}
		C = S - ss^\top. \\			
	\end{equation}
	The correlation matrix can be formed by
	\begin{equation}
		C' := DCD, ~~ D = \text{Diag}(1/\sqrt{C_{ii}});
	\end{equation}
	alternatively, the sensitivity of the atomic features is viable as the scaling factor
	\begin{equation}
		D = \text{Diag}(1/\sigma_j),
	\end{equation}
	where the sensitivity vector's entries are
	\begin{equation}
		\sigma_j = \frac{1}{\sum_l n^l_\text{atom}}\sum_{li} d^l_{ij},
	\end{equation}
	where
	\begin{equation}
		d^l_{ij} := \max_k |f^l_{ij} - (\hat{f}_k)^l_{ij}|~~\text{ for }k = 1,2,...,
	\end{equation}
	where $\hat{f_k}$ is the set of perturbed atomic features, obtained by perturbing the atomic coordinates randomly
	\begin{equation}
		x \leftarrow x \pm 0.01,
	\end{equation}
	and using the perturbed coordinates to extract the atomic features $\hat{f}_k$.

	Select the eigenvectors such that they correspond to the $n_{af}$ largest eigenvalues. They are obtained by first doing the spectral decomposition of the correlation matrix
	\begin{equation}
		C' = Q\Lambda Q^\top,
	\end{equation}
	where $\Lambda$ is the diagonal matrix containing the $k$th eigenvalue $\Lambda_{kk}$ and $Q$ is the matrix containing the $k$th eigenvector $Q_{:k}$, then we permute the columns of $Q$ and the diagonal entries $\lambda_i$ of $\Lambda$ such that
	\begin{equation}
		\lambda_{1} \geq \lambda_{2} \geq  ... \geq \lambda_{n^\text{atom}_f},
	\end{equation}
	and select
	\begin{equation}
		\hat{Q} = Q_{:, 1:n_{af}}.
	\end{equation}
	Finally the transformed feature can be obtained by
	\begin{equation}
		\label{eq:pca_atom_end}
		f^l_{i:} \leftarrow \hat{Q}^\top(f^l_{i:} - s), \text{ for }l = 1,2,...,N_\text{QM9}.
	\end{equation}
	\item Scale
	\begin{equation}
		\label{eq:scale_1}
		f^l_{ij} \in [0, 1]
	\end{equation}
	by
	\begin{equation}
		\underbar{$f$}_j := \min_{li} f^l_{ij},
	\end{equation}
	\begin{equation}
		\overline{f}_j := \max_{li} f^l_{ij},
	\end{equation}
	\begin{equation}
		f^l_{ij} \leftarrow (f^l_{ij} - \underbar{$f$}_j)/ (\overline{f}_j - \underbar{$f$}_j), \text{ for } j = 1,..., n_f^\text{select}.
	\end{equation}
	\begin{figure}[H]
		\label{fig:PCA_atom_rat}
		\centering
		\includegraphics[scale=0.55]{plot/eigenvalue_atom_exp_all_1_4_18_false_false_ratio.png}
		\caption{The ratio of each eigenvalue against the largest eigenvalue, obtained from the PCA of ACSF on the atomic level.}
	\end{figure}
	\begin{figure}[H]
		\label{fig:PCA_atom_dist}
		\centering
		\includegraphics[scale=0.55]{plot/eigenvalue_atom_exp_all_1_4_18_false_false_distribution.png}
		\caption{$\mu_i := \sum_{i\geq k} \lambda_i \text{ for } k = 1,2,...,n_f$, obtained from the PCA of ACSF on the atomic level. This shows the distribution of the eigenvalues when $i$ amount of the (sorted) eigenvalues are dropped.}
	\end{figure}
\end{itemize}
\at{plot labelling nequidistance}

\paragraph{Timing and storage}
Features based on atomic symmetry functions in general has $\mathcal{O}(n_\text{atom}n_\text{cutoff}c)$ time complexity, where $c$ is a factor referring to the feature extractor's method of computation (e.g., ACSF requires $c$ amount of radial symmetry function computations), and $n_\text{cutoff}$ is the number of atoms included within the cutoff radius, as it is required to sweep through the atom and its neighbours within the cutoff radius each time an atomic symmetry quantity is queried. The resulting features of each atom is a vector with length $n_{af}$, therefore a molecule has an order of $\mathcal{O}(n_\text{atom}n_{af})$ storage complexity. In total for the QM9 challenge, each time an atomic feature extraction is queried, it has $\mathcal{O}(N_\text{QM9}n_\text{atom}n_\text{cutoff}c)$ time complexity and $\mathcal{O}(N_\text{QM9}n_\text{atom}n_{af})$ storage complexity.

In general, each query of PCA needs to sweep all entries of the feature matrix multiple times. Specific for atomic features, it can be broken down into:
\begin{itemize}
	\item Computing mean vector $s$ requires $\mathcal{O}(N_\text{QM9}n_\text{atom}n_{af})$ time and $\mathcal{O}(n_{af})$ storage.
	\item Symmetric matrix $S$ requires $\mathcal{O}(N_\text{QM9}n_\text{atom}n^2_{af})$ time and $\mathcal{O}(n^2_{af})$ storage.
	\item The covariance matrix can be computed with $\mathcal{O}(n^2_{af})$ operations given the mean vector $s$ and the symmetric matrix $S$, with also $\mathcal{O}(n^2_{af})$ storage complexity.
	\item The correlation matrix can be computed by $\mathcal{O}(n^2_{af})$ operations given the covariance matrix $C$ and the scaling factors $D$, requires $\mathcal{O}(n^2_{af})$ storage.
	\item Spectral decomposition of the symmetric matrix requires $\mathcal{O}(n^3_{af})$ time and $\mathcal{O}(n^2_{af})$ storage.
	\item Projecting the atomic features into lower dimensional space requires $\mathcal{O}(n^2_{af})$ time for each atom, hence for all of the QM9 molecules, it requires $\mathcal{O}(N_\text{QM9}n_\text{atom}n^2_{af})$; the storage complexity is $\mathcal{O}(N_\text{QM9}n_\text{atom}n_{af})$.
\end{itemize}
Overall, in terms of the timing and storage, the raw atomic feature extraction itself should be the upper bound of the whole atomic feature processing block, since the factor $c$ itself could also be a composition of many factors depending on the method of the feature extractor.


\subsubsection{Molecular feature selection}
The molecular feature matrix $F \in \mathbb{R}^{N_\text{QM9} \times n_f}$ where $N_\text{QM9}$ is the number of molecules in the QM9 database and $n_f$ is the number of features, needs to be precomputed and stored in the non-volatile storage in order to avoid re-computation.
The molecular feature matrix often can be obtained through processing the array of atomic features.
The atomic features is often in the form of row vector $f^l_{i}$ with length $n_{af}$ for each atom $i$ in a molecule $l$.
\begin{itemize}
    \item $l$th molecule contains $n^l_\text{atom}$ number of atoms and a set of atom types $P = \{\text{H}, \text{C}, \text{O}, ...\}$ with cardinality $n^l_\text{type}$,
	the molecular features are computed by
        \begin{equation}
			\label{eq:acsf}
            \begin{split}
                s^l_{j} &= \sum_{m \in M_j} f^l_{m:}, \\
                %S^l &= \sum_i f^l_{i:}(f^l_{i:})^\top,
            \end{split}
        \end{equation}
		where $M_j$ is the set of index which contains the row positions of atom type $j \in P$ in the $f^l$ matrix. 
		Furthermore we add
		\begin{equation}
			N_j = |M_j|,
		\end{equation}
		combined into
		\begin{equation}
			\hat{N} = [N_\text{H}, N_\text{C}, N_\text{O},..., 1/n^l_\text{atom}].
		\end{equation}
		The feature vector of the $l$th molecule becomes
		\begin{equation}
			F_l := [s^l_{j}, \hat{N} ] \in \mathbb{R}^{1 \times n_f^\text{mol}}, \forall j \in P,
        \end{equation}
		where
		\begin{equation}
    		n_f^\text{mol} := 5n_f + 6,
    	\end{equation}
		in most cases this is a sufficiently good feature vector for fitting.
		
		Additional features can be introduced, such as the sum of squares
		\begin{equation}
			\hat{s}^l_j = \sum_{m \in M_j} (f^l_{m:})^2;
		\end{equation}
		the binomial feature
		\begin{equation}
			S^l_j = \sum_{m \in M_j} f^l_{m:}(f^l_{m:})^\top;
		\end{equation}
		since $S^l_j$ is a symmetric matrix, only the upper triangular and the diagonal entries are needed. 
		Hence if we include both the sum of squares and the binomial features
		\begin{equation}
			F_l := [s^l_{j}, \hat{s}^l_{j}, S^l_j, \hat{N} ] \in \mathbb{R}^{1 \times n_f^\text{mol}}, \forall j \in P,
		\end{equation}
        where
		\begin{equation}
			n_f^\text{mol} := \frac{5}{2}(n_f^2+n_f)+6,
		\end{equation}
    	this defines the fingerprint matrix $F \in \mathbb{R}^{N \times n_f}$.
    \item Do feature selection by using PCA in a similar manner as (\refeq{eq:pca_atom_start}) -- (\refeq{eq:pca_atom_end}) to take the subset of the columns of $W$, this can be done by the following:
	    \begin{equation}
			\label{eq:pca_mol_start}
			\begin{split}
				s &= \frac{1}{N} \sum_l F_{l:}, \\
				S &= \frac{1}{N} \sum_l F_{l:}(F_{l:})^\top, \\
				\text{ for } l &= 1,2,...,N_\text{QM9}, \\
			\end{split}
	    \end{equation}
		compute the covariance matrix
		\begin{equation}
			C = S - ss^\top, \\			
		\end{equation}
		compute the correlation matrix
		\begin{equation}
			C' := DCD, ~~ D = \text{Diag}(1\sqrt{C_{ii}}),
		\end{equation}
		and its spectral decomposition
		\begin{equation}
			C' = Q\Lambda Q^\top,
		\end{equation}
		permute the columns of $Q$ and the entries of $\Lambda$ such that
		\begin{equation}
			\lambda_{1} \geq \lambda_{2} \geq  ... \geq \lambda_{n^\text{mol}_f},
		\end{equation}
		and select
		\begin{equation}
			\hat{Q} = Q_{:, 1:n_{mf}}.
		\end{equation}
		Obtain the transformed feature by
		\begin{equation}
			\label{eq:pca_mol_end}
			F_{l:} \leftarrow \hat{Q}^\top(F_{l:} - s), \text{ for }l = 1,2,...,N_\text{QM9}.
		\end{equation}
	\item Scale $F$ such that 
	\begin{equation}
		\label{eq:scale_mol_1}
		F_{lj} \in [0, 1]
	\end{equation}
	by
	\begin{equation}
		\label{eq:scale_mol_2}
		\underbar{$F$}_{j} := \min_{l} F_{lj}
	\end{equation}
	\begin{equation}
		\overline{F}_{j} := \max_{l} F_{lj}
	\end{equation}
	\begin{equation}
		F_{:j} \leftarrow (F_{:j} - \underbar{$F$}_{j})/ (\overline{F}_{j} - \underbar{$F$}_{j}), \text{ for } j = 1,2,..., n_{mf}.
	\end{equation}
	\begin{figure}[h]
		\label{fig:PCA_mol_rat}
		\centering
		\includegraphics[scale=0.55]{plot/eigenvalue_mol_exp_all_1_4_18_false_false_ratio.png}
		\caption{The ratio of each eigenvalue against the largest eigenvalue with hyperparameter $n_{af} = 4$, obtained from the PCA of ACSF on molecular level.}
	\end{figure}
	\begin{figure}[H]
		\label{fig:PCA_mol_dist}
		\centering
		\includegraphics[scale=0.55]{plot/eigenvalue_mol_exp_all_1_4_18_false_false_distribution.png}
		\caption{The distribution of the eigenvalues with hyperparameter $n_{af} = 4$. Obtained from the PCA of ACSF on molecular level.}
	\end{figure}
\end{itemize}

\paragraph{Timing and storage}
The timing breakdown of the molecular feature selection is as the following:
\begin{itemize}
	\item In the default setting, the molecular feature is only computed by the sums of the atomic features, for each molecule this has $\mathcal{O}(n_\text{atom} n_{af})$ order of operations, therefore for all molecules in the QM9 dataset the time complexity is $\mathcal{O}(N_\text{QM9}n_\text{atom} n_{af})$; the storage requirement is $\mathcal{O}(N_\text{QM9}n_{mf})$. 
	\item Additional features which increase the complexity noticeably is the binomial feature, each binomial feature matrix takes $\mathcal{O}(n^2_{af})$ order of computation and $\mathcal{O}(n^2_{af})$ storage, for $N_\text{QM9}$ this becomes $\mathcal{O}(N_\text{QM9}n_\text{atom}n^2_{af})$ time and storage requirement.
\end{itemize}
For molecular features, the PCA has similar time and storage complexity to the atomic feature counterpart, the only differences are the initial mean vector $s$, the symmetric matrix $S$ computation, and the final projection, since the feature is now a vector with length $n_{mf}$ for each molecule in contrast to the atomic features with $\mathbb{R}^{n_\text{atom} \times n_{af}}$ matrix for each molecule.

\subsection{Feature transformation by message passing}
The idea of \textit{message passing} is to make the atoms within a molecule to be "aware" of each other's features.  
The message passing techniques were first introduced by \at{cite Gilmer et al} in their Message Passing Neural Networks paper. 
\dots
A molecule is represented by undirected graph $G$ where the atoms are the nodes $v \in G$ where it indexes the node features $x_v$; and the edges are the interactions between the atoms (whether it is covalently bonded or not), it is also possible for each edge to contain edge features $e_{vw}$. Here it is assumed that the graph is fully connected with respect to a certain cutoff $r_\text{cutoff}$ radius for each node.

In general, there are three major phases for message passing transformation: \textbf{the aggregation phase, the update phase, and the readout phase}.
The aggregation and update phases are run for $T$ time steps, and the readout phase is applied in the end.
Suppose that $h_v^t$ is a hidden state which represents the features of node $v$ at time step $t$, we do an initialization at time step $t = 0$ as
\begin{equation}
	\label{eq:mpinit}
	h_v^{(0)} := f_i^l, ~~ e_{vw}^{(0)} := r_{ij};
\end{equation}
then for each time step $t$, we do an aggregation phase,
\begin{equation}
	\label{eq:mpagg}
	m^{t+1}_v = \frac{1}{|N(v)|}\sum_{w \in N(v)} M_t(h_v^t, h_w^t, e_{vw}^t),~~ \text{for }v \in G
\end{equation}
which is just the mean of the neighbours' features, with an aggregation function $M_t$ at time step $t$, which in the simplest case is
\begin{equation}
	\label{eq:mpaggex}
	M_t(h_v^t, h_w^t, e_{vw}^t) :=  \text{concat}\Big(h_v^t, h_w^t, e_{vw}^t\Big),
\end{equation}
where $\text{concat}(\cdot, \cdot)$ is the concatenation. The sum ensures that $m^{t+1}_v$ is invariant with respect to the set of neighbours $N(v)$.
The update phase for the hidden states is defined by
\begin{equation}
	\label{eq:mpupdate}
	h_v^{t+1} = U^t_h(h_v^t, m^{t+1}_v), ~~ e_{vw}^{t+1} = U^t_e(h_v^t, h_w^t, e_{vw}^t),
\end{equation}
where $U^t_h$ is an update function for hidden states at time step $t$, one of the most common example is
\begin{equation}
	\label{eq:mphupdateex}
	U^t_h(h_v^t, m^{t+1}_v) := \sigma(W^tm^{t+1}_v), 
\end{equation}
where $\sigma$ is an activation function which takes in the product of the learned "weight" matrix $W^t$ at time step $t$ with the aggregated vector $m^{t+1}_v$; 
it is also possible to have a fixed quantities instead of a learned matrix \at{will need to design the formula, or find a reference that do this} hence there exist minimal numbers of hyperparameters to be optimized. It is common to keep the edge features fixed throughout the time steps, i.e.,
\begin{equation}
	\label{eq:mpeupdateex}
	U^t_e(h_v^t, h_w^t, e_{vw}^t) := e_{vw}^t,
\end{equation}
in some cases it is possible to use a learned function analogous to (\ref{eq:mphupdateex}) \at{need some references}. After $T$ time steps, the readout phase is defined by
\begin{equation}
	\label{eq:mpreadout}
	f_i^l = R(h_v^T) ~~ \text{for }v \in G,
\end{equation}
where $R$ is the readout function, one of the simplest example is
\begin{equation}
	\label{eq:mpreadoutex}
	R(h) := h.
\end{equation}

\section{Splines}
\label{sec:spline}
Splines (\texttt{bspline}) are used to increase the dimension of the feature space; with sufficient number of splines for each feature, it will lead to linearly separated data points.
\begin{itemize}
	\item Compute the univariate splines $\beta$ and its derivative $\beta'$ given $F_{ij}$ for $i = 1,...,n_\text{data}$, $j = 1,...,n_f$ (vectorized over matrix $F$):
	\begin{equation}
		\label{eq:spline}
		\begin{split}
			\Phi := \beta(F) \in \mathbb{R}^{n_\text{data} \times L}, \\
			\Phi' := \beta'(F) \in \mathbb{R}^{n_\text{data} \times L}.
		\end{split}
	\end{equation}
	where $n_L = n_fn_s$ and $n_s$ is the number of splines for each feature.
	\at{add labels to the plots}
	\begin{figure}[h]
		\label{fig:spline}
		\centering
		\includegraphics[scale=0.55]{plot/f.png}
		\caption{The bspline with $n_s = 8$.}
	\end{figure}
	\begin{figure}[H]
		\label{fig:dspline}
		\centering
		\includegraphics[scale=0.55]{plot/df.png}
		\caption{The derivative of bspline with $n_s = 8$.}
	\end{figure}
\end{itemize}



\section{Data selection}
\label{sec:data}
The set of indices which refers to data points with known energies (or centers) $K$ is chosen by the farthest-distance-algorithm variations, one of which is the \textbf{usequence}. The QM9 challenge imposes $n_K = |K| = 100$. Here we pick $N = |T| > n_K$ centers as a hyperparameter. The flow of the data selection is as the following:
\begin{itemize}
	\item Compute and store the set of indices $K$ and the corresponding matrix of distances $D \in \mathbb{R}^{N_\text{QM9} \times N}$ by farthest-minimum-distance algorithm \at{cite Eldar} given $F$, $B$ and $N$.
	\begin{equation}
		(T, D) := f_{\text{fmd}}(F, B, M),
	\end{equation}
	in which the distance between two data points of index $k$ and $m$ is defined by
	\begin{equation}
		D_k(F_{m:}) := \|F_{m:} - F_{k:}\|^2_B := \|B(F_{m:} - F_{k:})\|^2_2.
	\end{equation}
	From this, we can obtain one data point which belongs to the set $T$ by indexing
	\begin{equation}
		w_k := F_{k:}, \text{ for } k \in T.
	\end{equation}
\end{itemize}

\section{The model}
\label{sec:model}

\subsection{Shepard model}
\label{subsec:shepard}
The generalized Shepard model has the form of 
\begin{equation}
	\label{eq:shepard}
	V_K(w):=R_K(w)/S_K(w) ~~~ \text{ for } w \in \mathbb{R}^m \setminus \{w_k\mid k\in K \},
\end{equation}
where
\begin{equation}
	R_K(w):=\sum_{k\in K} \frac{V_k(w)}{D_k(w)},~~~
	S_K(w):=\sum_{k\in K} \frac{1}{D_k(w)},
\end{equation}
and
\begin{equation}
	\label{eq:vk}
	V_k(w) = E_k + \sum_l \theta_{kl} \phi_{kl}(w).
\end{equation}
By (\ref{eq:vk}), (\ref{eq:shepard}) can be expanded into
\begin{equation}
	\label{eq:vk_expand}
	V_K(w) := \sum_{k\in K} \frac{E_k + \sum_l \theta_{kl} \phi_{kl}(w)}{D_k(w)} / S_K(w). 
\end{equation}
The quality of the prediction accuracy is measured by the \textbf{mean absolute deviation}
\begin{equation}
	\text{MAD}_K(w) := \frac{1}{|K|}\sum_{j\in K}|\Delta_{jK}(w)|
\end{equation}
and the \textbf{root mean square deviation}
\begin{equation}
	\text{RMSD}_K(w) := \sqrt{\frac{1}{|K|}\sum_{j\in K}\Delta_{jK}(w)^2},
\end{equation}
where by (\refeq{eq:vk}) and (\refeq{eq:vk_expand})
\begin{equation}
	\label{eq:delta}
	\Delta_{jK}(w):=\D\frac{V_K(w)-V_j(w)}{D_j(w)S_K(w)-1}.
\end{equation}
First we need to express (\refeq{eq:delta}) as a system of linear equations, let us simplify it by writing
\begin{equation*}
	\label{eq:simp}
	\begin{split}
		D_k &:= D_k(w), \\
		S_K &:= S_K(w), \\
		\phi_{kl} &:= \phi_{kl}(w),\\
		\psi_{kl} &:= \theta_{kl}\phi_{kl}, \\
		\gamma_k &:= D_kS_K, \\
		\alpha_j &:= \gamma_j-1.
	\end{split}
\end{equation*}
using this, (\refeq{eq:delta}) can be re-arranged into
\begin{equation}
	\label{eq:delta_split}
	\begin{split}
		\Delta_{jK}(w)&= \frac{\D \Big(\sum_{k\in K} \frac{E_k + \sum_l \psi_{kl}}{D_k} \Big)/ S_K - \Big(E_j + \sum_l \psi_{jl}\Big)}{\alpha_j} \\
		&= \sum_k \frac{E_k}{\gamma_k\alpha_j} + \sum_{kl} \frac{\psi_{kl}}{\gamma_k\alpha_j} - \sum_{l} \frac{\psi_{jl}}{\alpha_j} - \frac{E_j}{\alpha_j}\\
		&= \sum_k \frac{E_k}{\gamma_k\alpha_j} + \sum_{kl} \frac{\psi_{kl}}{\gamma_k\alpha_j} - \sum_{l} \frac{\delta_{jk}\psi_{kl}}{\alpha_j} - \frac{E_j}{\alpha_j}\\
		&= \sum_{kl} \frac{\psi_{kl} (1-\gamma_k\delta_{jk})}{\gamma_k\alpha_j} - \Big(E_j - \sum_k\frac{E_k}{\gamma_k}\Big)/\alpha_j,
	\end{split}
\end{equation}
where $\delta_{jk}$ is the Kronecker delta. Hence if we pick $w = w_m$, then
\begin{equation}
	\begin{split}
		\Delta_{jK}(w_m) &= \sum_{kl} A_{jm, kl} \theta_{kl} - b_{jm}, \\
		&= A_{jm,:}\theta - b_{jm} = (A\theta - b)_{jm}
	\end{split}	
\end{equation}
where
\begin{equation}
	\label{eq:rosemi_linear}
	A_{jm, kl} = \frac{\phi_{kl}(w_m)(1 - \gamma_k(w_m) \delta_{jk})}{\gamma_k(w_m) \alpha_j(w_m)},\\
\end{equation}
\begin{equation}
	b_{jm} = \Big(E_j - \sum_k\frac{E_k}{\gamma_k(w_m)}\Big)/\alpha_j(w_m).
\end{equation}

\subsection{Gaussian molecular kernel (KRR)} 
\label{subsec:gaussian}
Let $A$ be the data matrix, if it is constructed by the Gaussian kernel then
\begin{equation}
	\label{eq:gaussian_mol}
	A_{ij} = \text{exp}\Big(-\frac{\|w_{i:} - w_{j:}\|^2_2}{2\sigma^2}\Big),
\end{equation}
where $i$ and $j$ index the query molecules (or data points)
\begin{equation}
	\sigma = c\sigma_0,
\end{equation}
where $c \in \mathbb{R}$ is a hyperparameter, and
\begin{equation}
	\sigma_0 = \frac{1}{Nn_K}\sum_{ij}\|w_{i:} - w_{j:}\|^2_2.
\end{equation} 
The target vector is simply the energies:
\begin{equation}
	\label{eq:lineartargetvector}
	b_i := E_i.
\end{equation}

\subsection{Linear least squares (LLS)}
\label{subsec:LLS}
The linear least squares (LLS) model simply uses the feature matrix as the data matrix, hence
\begin{equation}
	\label{eq:lls}
	A_{ij} := F_{ij},
\end{equation}
where $i$ indexes the query molecule and $j$ indexes the feature column of $i$th molecule. Whereas the target vector is the same as (\ref{eq:lineartargetvector}).
\at{revision regarding to the stuffs that are solved by LLS}

\subsection{Neural network (NN)}
\label{subsec:NN}
\at{insert standard neural network formula here}

\subsection{Gaussian atomic kernel (GAK)}
The entries of the data matrix for the atomic Gaussian kernel are \at{\cite[Eq. (10)]{Faber et al} from \url{https://aip.scitation.org/doi/10.1063/1.5126701}}
\iffalse
\begin{equation}
	A_{l' l} = \sum_P \sum_{Z^{l'}_i = P} \sum_{Z^{l}_j = P} A^\text{atom}(f^{l'}_i, f^l_j)
	\label{eq:gaussian_atom}
\end{equation}
\fi
\begin{equation}
	A_{l'l} = \sum_j \sum_i \delta_{Z_i^{l'}Z_j^{l}} A^\text{atom}(f_i^{l'}, f_j^l)
	\label{eq:gaussian_atom}
\end{equation}
where $Z_i^l$ and $Z_j^{l'}$ are the nuclear charges of atom $i$ and $j$ in molecule $l'$ and molecule $l$ respectively. The Kronecker delta $\delta$ ensures that only the environment between the same atom types are compared. The Gaussian atomic kernel function is \at{\cite[Eq. (26)]{Faber et al}}
\begin{equation}
	A^\text{atom}(u, v) = \text{ exp}\Big( - \frac{\|u - v \|^2_2}{2 \sigma^2} \Big).
\end{equation}
where $\sigma \in \mathbb{R}$ is a hyperparameter, typically $\sigma = 2^k$ where $k \in \mathbb{Z}_{\geq 0}$.

\iffalse
&= \frac{\D \left(\sum_{k\in K} \frac{E_k + \sum_l \theta_{kl} \phi_{kl}(w)}{D_k(w)} / S_K(w)\right) - \left(E_j + \sum_l \theta_{jl} \phi_{jl}(w)\right)}{D_j(w)S_K(w)-1}, \\
\fi

\section{Intermediate values}
\label{sec:intermediate}

\subsection{Shepard model (ROSEMI)}
Several intermediate values need to be computed and stored in the random access memory (RAM), since these intermediate values depend on several parameters which changes for each experiment and are fast to compute hence it is not necessary to store them in the non-volatile memory. Given the stored $F$ (from Section \ref{sec:feature}); $\Phi$ and $\Phi'$ (from Section \ref{sec:spline}); and $D$ and $T$ (from Section \ref{sec:data}):
\begin{itemize}
	\item Determine the set of labels of the initial trial points
	\begin{equation}
		K \subset T,
	\end{equation}
	for example, we can take the first 100 labels of $T$ as $K$.
	\item Compute the set of labels of the unsupervised data points
	\begin{equation}
		U := T \setminus K.
	\end{equation}
	\item Compute the set of of labels of the data points for evaluation
	\begin{equation}
		W := \{1,2,...,n_\text{data}\} \setminus K.
	\end{equation}
	\item Compute intermediate values:
	\begin{equation}
		\label{eq:intermediate}
		\begin{split}
			S_K &\in \mathbb{R}^{n_U} := S_K(F_{m:}) \text{ for } m \in U,\\
			\gamma &\in \mathbb{R}^{n_U \times n_K} := \gamma_k(F_{m:}) \text{ for } k \in K, m \in U, \\
			\alpha &:= \gamma - 1.
		\end{split}
	\end{equation}
	where $n_U = |U|.$
	\item Given the splines in (\ref{eq:spline}), compute
	\begin{equation} % ϕ[l,m] - ϕ[l, k] - dϕ[l, k]*(F[t,m]-F[t,k])
		\phi_{m, kl} := \Phi_{ml} - \Phi_{kl} - \Phi'_{kl}(F_{mt}-F_{kt}) \quad \text{for } m \in U, \text{ }k \in K, \text{ }l = 1,...,n_L,
	\end{equation}
	where
	\begin{equation}
		\begin{split}
			%t &\leftarrow \hat{t} = 0 \text{ ? } n_f : \hat{t},
			t = 
			\begin{cases}
				n_f,  & \text{if } \hat{t} = 0 \\
				\hat{t}, & \text{otherwise}
			\end{cases}
			~~ 
		\end{split}
	\end{equation}
	with
	\begin{equation}
		\hat{t} := l \text{ mod } n_f,
	\end{equation}
	this makes sure the correct column is indexed, since $\phi_b'(w)$ is only non zero at the $t$th column position. This results in matrix $\phi$ where the block column is indexed by $k,l$ indices.
\end{itemize}

\section{Fitting the model}
\subsection{Shepard model}
A good prediction model is reflected by small MAD or RMSD, hence a minimization routine is required. In order to solve the minimzation problem, a linear sysem or least squares formulation is needed. For example, if the RMSD is chosen as the objective function then the problem formulation would be
\begin{equation}
    \label{eq:min}
    \begin{split}
        f_{\text{obj}}(\theta):= |K| \sum_m \text{RMSD}_K(w_m)^2 = \sum_m \sum_j \Delta_{jK}(w_m)^2 = \sum_{jm} \left( A\theta - b\right)_{jm}^2
         = \|A \theta - b\|_2^2,
    \end{split}
\end{equation}
where $A$ is the data matrix defined in section (\ref{sec:model}) with $n_Kn_U$ rows and $n_Kn_L$ columns, $\theta$ is the coefficient vector, and $b$ is the target vector.

For storage efficiency, it is desirable to not explicitly form the data matrix $A$, since it grows quickly with more data points included in the training set $K$ and testing set $T$. 
We can alleviate the storage problem by writing a routine for computing $Au$ where $u$ is a variable vector, hence instead of having a matrix with $\mathcal{O}(M^2NL)$ storage we would have a vector with only $\mathcal{O}(N)$ storage. 
This can be done by augmenting the first sum of the last line of (\refeq{eq:delta_split}) with the pre-computed intermediate values in (\refeq{eq:intermediate}):
\begin{equation}
	Au = \sum_{kl} \frac{\phi_{m, kl} u_{kl} (1-\gamma_{m k}\delta_{jk})}{\gamma_{mk}\alpha_{mj}},
\end{equation}
Here, CGLS \at{cite Hestenes} method is used to minimize the objective function in (\refeq{eq:min}). The memory-less form of CGLS requires not only $Au$ but also $A^\top v$ routine, which is
\begin{equation}
	\begin{split}
		A^\top v = \sum_{jm} \frac{\phi_{m, kl} v_{jm} (1-\gamma_{m k}\delta_{jk})}{\gamma_{mk}\alpha_{mj}}.
	\end{split}
\end{equation}
Given the $Au$ and $A^\top v$ routine, and $b$, the CGLS is called as follows \at{cite Krylov.jl and LinearOperators.jl}:
\begin{equation*}
	\begin{split}
		\text{op} &\leftarrow \text{LinearOperators}(y, Au, A^\top v), \\
		\theta &\leftarrow \text{CGLS}(\text{op}, b, \text{itmax}),
	\end{split}
\end{equation*}
where $y$ is an arbitrary sized vector of real numbers (in Julia, $y$ can be initialized by \codeword{y = Vector{Float64}()}) and \codeword{itmax} is the maximum number of iteration.
\subsection{Linear models}
The Gaussian kernels and LLS models can be fitted in a straightforward manner, the linear system is
\begin{equation}
	A \theta = b,
\end{equation}
where for Gaussian molecular kernel $A$ is defined in (\ref{eq:gaussian_mol}); for Gaussian atomic kernel, $A$ is defined in (\ref{eq:gaussian_atom}); meanwhile for LLS, $A$ is defined in (\ref{eq:lls}); $b$ is the target vector defined in (\ref{eq:lineartargetvector}); and $\theta$ is the coefficient vector. This can be implemented simply by
\begin{equation*}
	\theta \leftarrow \codeword{CGLS(}A, b,\codeword{itmax)},
\end{equation*}
$A$ is constructed explicitly here since it is cheap and fast to compute with only a maximum of 100 row size.

\subsection{Neural network}
\at{standard neural network fitting procedure}


\section{Program structure}
The main program is divided into two main subprograms: feature exctraction, data setup and fitter. The feature extraction and data setup subprogram is compatible with any types of model, while the fitter subprogram is specific for the Shepard model. Each subprogram is defined by:
\begin{itemize}
	\item Feature extraction refers to any operations included in extracting the raw array of features (usually on the atomic level) given the geometrical information of the molecules (e.g., coordinates, nuclear charges, etc), this is usually only done once for each set of molecular geometries.
	\item Data setup includes section (\ref{sec:feature}) up to section (\ref{sec:data}) except for the raw feature extraction; in general, everything that are independent of $\theta$ and the indexing from set $T$ are included into the data setup. 
	\item The fitter subprogram computes everything which depend on $\theta$ or set $T$. 
\end{itemize}

 





\subsection{Feature extraction}
\label{sub:fe}
Most of the feature extraction techniques for molecular geometries require similar set of inputs, e.g., the coordinates and nuclear charges; additionally with the list of hyperparameters. In most cases, each feature extraction technique is bundled within different libraries (and often on different programming languages); and usually each library wraps the molecular information in a different way, hence it is required to transform the raw molecular data into the the data structure demanded by the library. The abstraction of the feature extraction subprogram is
\begin{enumerate}
	\item $\codeword{mol_class} \leftarrow \codeword{transform_data(}x, Z, \codeword{mol_info)}$, where $x$ is a set of molecular coordinates, $Z$ is the set of nuclear charges and \codeword{mol_info} is a list of additional molecular information (which is usually not mandatory), such as the molecule name, number of atoms, etc.
	\item $f \leftarrow \codeword{extract_feature(mol_class, params)}$, where \codeword{mol_class} is an object that contains all of the required molecular information for feature extraction, and \codeword{params} is a list of the required hyperparameters, e.g., ACSF requires the cutoff radius, level of symmetry, periodicity of the atomic system, etc.
	\item $\codeword{save(feature_file,} f\codeword{)}$, where \codeword{feature_file} is a string describing the harddrive path to store the array $f$, this will be used for the \codeword{data_setup} process.
\end{enumerate}

\subsection{Data setup subprogram}
\label{sub:data}
The data setup function abstraction is
\begin{equation*}
	(F, T, D, \phi, \phi') \leftarrow \codeword{data_setup(indices}, n_{af}, n_{mf}, n_s, N, \codeword{feature_file}, \codeword{feature_mode)},
\end{equation*}
where \codeword{indices} is the set of labels which will be included, in case of full QM9 dataset then \codeword{indices} = $1:N_\text{QM9}$; \codeword{feature_file} is a string which indicates the file path of a file containing the atomic feature set computed on all of the QM9 data points; \codeword{feature_mode} is a list of boolean which indicates the inclusion of sums of squares or binomial features, e.g., \codeword{feature_mode = [true, false]} means only the sums of squares will be added to the molecular features. The flow of the data setup subprogram is:
\begin{enumerate}
	\item $F \leftarrow$ \codeword{PCA_atom(feature_file,} $n_{af}$\codeword{)}
	\item $F \leftarrow$ \codeword{extract_mol_feature(}$F$\codeword{, feature_mode)}
	\item $F \leftarrow$ \codeword{PCA_mol(}$F, n_{mf}$\codeword{)}
	\item $(\phi, \phi') \leftarrow$ \codeword{extract_bspline(}$F, n_s$\codeword{)} 
	\item $(T, D) \leftarrow$ \codeword{get_centers(}$F, N$\codeword{)}
\end{enumerate}

\subsection{Fitter subprogram}
\label{sub:}
Given the set of files from section (\ref{sub:data}), the fitter function abstraction is simply
\begin{equation}
	(\theta, \codeword{info}) \leftarrow \codeword{fitter(data_dir, t_limit, batchsize)}
\end{equation}
where \codeword{data_dir} is a string that refers to the path which contains the files resulted from the data setup subprogram, \codeword{t_limit} is the solver time limit (in second), \codeword{batchsize} is an integer representing the size of mini-batch of several variables, in some cases the total memory usage of the system is over 8GB (typical Random Access Memory (RAM) size of modern computers) hence mini-batching is necessary. On the output side, \codeword{info} contains the fitting errors, list of timings, and list of fitting hyperparameters (e.g., $n_{af}, n_{mf},$ etc). The processes executed in the fitter are:
\begin{enumerate}
	\item \at{add program info for selection using usequence (also describe usequence in section 3 data selction)}
	\item Compute variables for fitting:
	\begin{enumerate}
		\item $K \leftarrow $\codeword{select_centers(}$T$\codeword{)}
		\item $U \leftarrow T \setminus K$
		\item $W \leftarrow \{1,2,.., n_\text{data}\} \setminus K$ 
		\item $S_K \leftarrow\codeword{compute_SK(}D, K, U\codeword{)}$
		\item $\gamma \leftarrow \codeword{compute_gamma(}D, S_K, K, U\codeword{)}$
		\item $\alpha \leftarrow \gamma .- 1$
		\item $\phi \leftarrow \codeword{compute_phi(}\Phi, \Phi', F, K, U\codeword{)}$
	\end{enumerate}
	\item Fitting:
	\begin{enumerate}
		\item $b \leftarrow \codeword{compute_b(}E, \gamma, \alpha, K)$, where $E$ is a vector of energies.
		\item $\codeword{op} \leftarrow \codeword{LinearOperators(}y, Au, A^\top v \codeword{)}$
		\item $\theta \leftarrow \codeword{CGLS(op, b, itmax, t_limit)}$
	\end{enumerate}
	\item Energy prediction is computed in mini-batches in terms of the row numbers, i.e., for any vectors or matrices, the maximum row size is \codeword{batchsize}; e.g., $S_K$ vector for prediction has $N_\text{QM9}$ row size by default, however when computed in mini-batches, $S_K$ has \codeword{batchsize} rows for each mini-batch loop, where the total number of loops is $\codeword{ceil}(N_\text{QM9}/\codeword{batchsize})$.
	\begin{enumerate}
		\item $S_K \leftarrow\codeword{compute_SK(}D, K, W\codeword{)}$
		\item $\phi \leftarrow \codeword{compute_phi(}\Phi, \Phi', F, K, W\codeword{)}$
		\item $V_K \leftarrow \codeword{predict(}\theta, E, D, \phi, S_K, K, W\codeword{)}$
	\end{enumerate}
	\item Compute the errors:
	\begin{enumerate}
		\item $\text{MAE} \leftarrow \codeword{get_MAE(}V_K, E)$
		\item $\text{MAD} \leftarrow \codeword{get_MAD(}V_K, E, D, \theta, \phi, S_K, K, U\codeword{)}$, where MAD is a vector of length $n_U$. Note that the variables used to compute the MAD are the ones from step 1 (for fitting), not the ones from step 3.
	\end{enumerate}
\end{enumerate}

\section{Hyperparameter space}
\label{sec:hyperspace}
\subsection{Possible choices of hyperparameters}
We can construct the whole molecular fitting process from start to end by choosing many different possible combinations of hyperparameters.
Table \ref{tb:hyperparam_include} shows the list of variables included in the hyperparameter optimization, meanwhile Table \ref{tb:hyperparam_exclude} contains the excluded ones.
During feature selection, there exist several possible choices of hyperparameters:
\begin{itemize}
	\item \codeword{ftype} $\in$ \{ACSF, SOAP, FCHL\} (\textbf{categorical}), the choice of set of atomic features.
	\item $\codeword{pca_sf} \in \{\text{diag, sens}\}$ (\textbf{categorical}), the choice for the scaling factor of the correlation matrix: the diagonals of the covariance matrix or the sensitivity vector.  $n_{pf} \in \mathbb{Z}_{>0}$ (\textbf{integral}), which is the number of features involved for the sensitivity vector computation.
	\item $n_{mf} \leq n_{af} \in \mathbb{Z}_{>0}$ (\textbf{ordered integers}), the number of molecular features chosen $n_{mf}$ and the number of atomic features $n_{af}$.
	\item $n_{s}\in \mathbb{Z}_{>0}$ (\textbf{integral}), number of splines.
	\item $\codeword{mfem_ss} \in \{\codeword{false, true}\}$ (\textbf{categorical}), the feature extraction mode, whether to include sum of squares features.
	\item $\codeword{mfem_bf} \in \{\codeword{false, true}\}$ (\textbf{categorical}), the feature extraction mode, whether to include binomial features.
	\item \codeword{norm_af} $ \in \{\codeword{false, true}\}$ (\textbf{categorical}), the atomic features normalization mode, the choice of whether to normalize the atomic features.
	\item \codeword{norm_mf} $ \in \{\codeword{false, true}\}$ (\textbf{categorical}), the molecular features normalization mode, the choice of whether to normalize the molecular features.
\end{itemize}
For data selection, the possible hyperparameters are:
\begin{itemize}
	\item $N \in [1 : N_\text{QM9}] \subseteq \mathbb{Z}_{>0}$ (\textbf{integral}), the number of centers requested;
	\item $k \in \mathbb{Z}_{\geq 0}$ (\textbf{integral}), the index of precomputed set of selected centers by the \codeword{usequence}.
\end{itemize}
For linear model fitting, the possible choices of hyperparameters are:
\begin{itemize}
	\item \codeword{model} $\in$ \{RoSeMI, KRR, NN, LLS, GAK\} (\textbf{categorical}), the choice of models.
	\item $c \in \mathbb{R}_{\geq0}$ (\textbf{real}), the scaling factor for KRR and GAK kernels.
	\item \codeword{itmax} $\in \mathbb{Z}_{>0}$ (\textbf{integral}), number of CGLS iterations for the linear models.
\end{itemize}
For neural network, the possibilities are:
\begin{itemize}
	\item $n_\text{HL} \in \mathbb{Z}_{>0}$ (\textbf{integral}), the number of hidden layers for NN,
	\item $n_\text{nodes} \in \mathbb{Z}^{1 \times n_{HL}}_{>0}$ (\textbf{integral}), the number of nodes in each hidden layer,
	\item $\codeword{f_act} \in \{\text{ReLU, sigmoid, tanh}\}^{1 \times n_{HL}}$ (\textbf{categorical}), the choice of activation function for each node in each layer.
	\item $\codeword{NN_opt} \in \{\text{Adam, SGD, RMSProp}\}$ (\textbf{categorical}), the choice of NN optimizer.
\end{itemize}

All categorical variables are encoded in \textbf{one--hot--encoding} form, since there is no natural ordering of categorical variables (in contrast to variables with real or integer types). The encoding process transforms a value into a vector in which the length of the vector is equal to the size of the set variable, where the vector contains all 0 except at the relative position of the selected value within the set where we set it as 1. For example, if we choose "RoSeMI" as \codeword{model}, then
\begin{equation*}
	\codeword{f_encode("RoSeMI", model)} = [1,0,0,0,0],
\end{equation*}
another example would be if we choose "SOAP" as the feature type then
\begin{equation*}
	\codeword{f_encode("SOAP", ftype)} = [0,1,0],
\end{equation*}
here \codeword{f_encode(val, var)} is a function which encodes the value \codeword{val} given a set variable \codeword{var} into one--hot vector format.

\begin{table}[H]
	\centering
	\caption{The hyperparameters passed into the hyperparameter optimization routine with its data type and possible range. 
	Under the \textbf{type} column, \textbf{Int} means integer, \textbf{Cat} means categorical, and \textbf{OInt} means ordered integers. }
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{hyperparameter}	& \textbf{type}	& \textbf{range}	\\ \hline
		$n_{mf} \leq n_{af}$	& \codeword{OInt}	&	[16, 20]	\\ \hline
		$n_s$	& \codeword{Int}	& [1,10]			\\ \hline
		$c$	& \codeword{Real}	& $[0,20]$				\\ \hline
		\codeword{ftype}	& \codeword{Cat}	& [1,3]	\\ \hline
		\codeword{norm_af}	& \codeword{Cat}	& [0,1]	\\ \hline
		\codeword{norm_mf}	& \codeword{Cat}	& [0,1]	\\ \hline
		\codeword{model}	& \codeword{Cat}	& [1,5]	\\ \hline
	\end{tabular}
	\label{tb:hyperparam_include}
\end{table}

\begin{table}[H]
	\centering
	\caption{The list of fixed hyperparameters throughout all experiments.}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{hyperparameter}	& \textbf{type}	& \textbf{range}	& \textbf{value}	\\ \hline
		\codeword{itmax}	&	\codeword{Int}	&	$[1, \infty]$	&	500	\\ \hline
		$n_{pf}$	& \codeword{Int}	& $[0,\infty]$	& 0	\\ \hline
		$N$	& \codeword{Int}	& $[1,N_{QM9}]$	& 300	\\ \hline
		$k$	& \codeword{Int}	& $[0,95]$	& 38	\\ \hline
		$n_{HL}$	&	\codeword{Int}	& [1,50]	& 2	\\ \hline
		$n_\text{nodes}$	&	\codeword{Int}	& 	$[1,100]^{n_{HL}}$	& [10, 10]	\\ \hline
		\codeword{f_act}	&	\codeword{Cat}	&	$[1, 3]^{n_{HL}}$	&	[1, 1]	\\ \hline
		\codeword{NN_opt}	&	\codeword{Cat}	&	[1,3]	&	1	\\	\hline
		\codeword{pca_sf}	& \codeword{Cat}	& [0,1]	& 0	\\ \hline
		\codeword{mfem_ss}	& \codeword{Cat}	& $[0,1]$	& 0	\\ \hline		
		\codeword{mfem_bf}	& \codeword{Cat}	& $[0,1]$	& 0	\\ \hline		
	\end{tabular}
	\label{tb:hyperparam_exclude}
\end{table}

\subsection{Special treatments for special--typed variables}
Typically variables with \textbf{real} and \textbf{integral} types can be easily passed into an optimization routine given the bounds and types, as most optimization softwares handle both as mandatory types (e.g. in mixed--integer solvers). 
This is not the case for types such as \textbf{categorical} and \textbf{ordered integers}, hence special treatments are needed.

\subsubsection{Treating categorical variables}
\label{sec:hyperopt_cat}
Given a variable with \textbf{categorical} type, as mentioned in the previous subsection, we can encode it as a one--hot vector. The most general way, i.e., a way of passing the variable such that the variable can be handled by most optimization routines is by setting the one--hot vector type as \textbf{real}. A mechanism to handle the categorical variable, which is called "round to simplex", says: given $x \in \mathbb{R}^n$, find $p$ such that 
\begin{equation}
	\begin{split}
		&\min \|p-x\|^2_2 \\	
		&\text{s.t } p \geq 0, ~~ e^\top p = 1,
	\end{split}
	\label{eq:categorical_rounding}
\end{equation}
where $e$ is an all--1 vector. In other words, we want to find a "probability" vector $p$ that is closest to $x$, here $x$ can be an output of the optimization solver or an arbitrary initial point which corresponds to the one--hot vector. (\ref{eq:categorical_rounding}) can be solved analytically in order to find $p$. By considering the first order optimality condition of (\ref{eq:categorical_rounding}), which gives
\begin{equation*}
	\text{inf}(p-x-\lambda e, p) = 0,
\end{equation*}
this implies
\begin{equation*}
	p_i = p_i(\lambda) := (x_i - \lambda)_+,
\end{equation*}
where $\lambda$ is such that
\begin{equation*}
	S(\lambda) := e^\top p(\lambda) = \sum_i (x_i - \lambda)_+ = 1,
\end{equation*}
where $S(\lambda)$ is piecewise linear with breakpoints at $x_i$. From here, in order to solve $S(\lambda)$ for $\lambda$, we do the following steps:
\begin{itemize}
	\item Let $z = \codeword{sort}(\bar{x} - x)$, where $\bar{x} = \max_i(x_i)$, now we have: $0=z_1\leq z_2 \leq ... \leq z_n$.
	\item Let $\hat{x}_i = \bar{x} - z_i$, and $\mu = \bar{x} - \lambda$, then
	\begin{equation*}
		\begin{split}
			&S(\lambda) = \sum_i (\hat{x}_i - \lambda)_+ = \sum_i (\bar{x} - \lambda - z_i)_+ = \sum_i (\mu - z_i)_+ = \sum_{i=1}^k (\mu - z_i)_+ = k\mu - S_k, \\
			&\text{ if } z_k \leq \mu \leq z_{k+1}, \text{where }z_0 := -\infty, z_{k+1} := \infty.
		\end{split}
	\end{equation*}
	\item Since $k=0$ gives $\mu \leq 0$ and $S(\lambda) = 0$, thus we need to find $k>0$ in order to satisfy the $S(\lambda) = 1$:
	\begin{equation*}
		\begin{split}
			&k\mu - S_k=1, \\
			&\mu = \frac{1+S_k}{k}, \text{where $k$ is the smallest with } 1 \leq kz_{k+1} - S_k
		\end{split}
	\end{equation*}
	\item Given the smallest possible $k$, finally we can obtain feasible $p$
	\begin{equation*}
		\begin{split}
			\lambda &= \bar{x} - \mu, \\
			p_i &= (x_i - \lambda)_+.	
		\end{split}
	\end{equation*}
\end{itemize}
Using the feasible $p$ from the steps above, we can pick the value $y$ of the categorical variable by
\begin{equation*}
	q_i = \sum_{j\leq i} p_i,~~ \text{for }i = 0,1,2,...
\end{equation*}
where $q_0 = 0$. We generate a random value $r \in \mathbb{R} \cap [0,1]$, then we pick $y := i$ which satisfies
\begin{equation*}
	q_i \leq r \leq q_{i+1}.
\end{equation*}

\subsubsection{Treating ordered integer variables}
\label{sec:hyperopt_oint}
Analogous to the categorical variables, we encode $n$ ordered integers into an arbitrary vector $p \in \mathbb{R}^{n+1}$. 
It is allowed for either the initial point or the output of the optimizer to yield infeasible $p$, i.e., $e^\top p \neq 1$, thus we need to apply the "round to simplex" procedure for $p$, then we can obtain the values of the $n$ ordered integers by
\begin{equation*}
	x_i = x_{i-1} + p_idx, ~~ \text{for }i=0,1,...,n
\end{equation*}
where
\begin{equation}
	x_0 := \underbar{$x$}, ~~ x_{n+1} := \bar{x}, ~~ dx = \bar{x} - \underbar{$x$}.
\end{equation}

On the opposite direction, suppose $x$ is known, and we want to compute $p$ instead
\begin{equation*}
	\underbar{$x$} = x_0 \leq x_1 \leq ... \leq x_n \leq x_{n+1} = \bar{x},
\end{equation*}
then we encode them into a "probability" vector of size $n+1$ by
\begin{equation*}
	p_i = \frac{x_i - x_{i-1}}{dx}.
\end{equation*}


\section{Numerical experiments}
The hardware and software specification used for the experiments are:
\begin{itemize}
	\item Processors: AMD Ryzen 9 5900X 12-Core Processor @4.20 GHz
	\item Memory: 32 GB of RAM
	\item Programming language: Julia 1.7.2
\end{itemize}
Additionally, a secondary computer is often used with specification:
\begin{itemize}
	\item Processors: 4 $\times$ Intel\textregistered Core i5-4670S CPU @3.10 GHz
	\item Memory: 16 GB of RAM
\end{itemize}

\subsection{Memory usage}
The computation of fitting and prediction of QM9 challenge is resource-demanding, hence we need to allocate variables responsibly such that the OS does not throw an out-of-memory exception. The first ingredient is to approximate the size of data structures allocated within the program in megabytes (MB) order of magnitude; assuming the data structure holds \codeword{Float64} entries (which is the most common data type for numerical operations and is also the most memory consuming) this can be done by a general formula
\begin{equation}
	\codeword{mem_usage} = (8\times 10^{-6}) \times \prod_{i}\codeword{size(data_structure, i)}, \text{ for } i = 1,2,...
\end{equation}
where \codeword{size(data_structure, i)} returns the number of entries of the $i$th dimension of a data structure. For example, the distance matrix $D \in \mathbb{R}^{133628 \times 300}$ with \codeword{Float64} data type, then
\begin{equation}
	B_NX := B_N \times 300 = 320.7072 \text{ MB},
\end{equation}
where
\begin{equation}
	B_N := (8\times 10^{-6}) \times 133628 \approx 1.069
\end{equation}
is the storage needed for $N$ entries of \codeword{Float64} in MB.
Using Julia this can be confirmed simply by calling \codeword{Base.summarysize(D)*1e-6}, which gives the same result up to some roundoff error. For example, given $n_K = 100,n_f=28$, and $n_b = 5$ for ROSEMI, the block matrix $\phi$ for prediction has the memory size of
\begin{equation}
	B_NXY := B_N \times 100 \times 140 = 14966.336 \text{ MB} = 14.9 \text{ GB}.
\end{equation}
With \codeword{batchsize = 10_000}, for each mini-batch loop the size of $\phi$ becomes $1/13$ of its actual size which is around 1GB. 


\subsection{Fitting results}
\label{subsec:fitting}
The true error of the fitting is described by the mean absolute error (MAE) of the energies
\begin{equation}
	\text{MAE} := \frac{1}{N_{QM9}}\sum_{m \in W}|E_{m} - \hat{E}_m|,
\end{equation}
where $\hat{E}$ is the vector of predicted energies.
The initial set of the fixed hyperparameters across all experiments is:
\begin{equation*}
	N_\text{QM9} = 133628, ~~ N = 300,~~ n_K = 100, ~~\codeword{t_limit} = 900.
\end{equation*}
Each \codeword{data_setup()} process roughly takes around 300s. 
The fitting is done using \codeword{CGLS()} with $\codeword{itmax} = 500$; Specifically for ROSEMI, it requires the functions to compute $Au$ and $A^\top v$ as the input variable. 
Currently active learning scheme has not been tested.

\iffalse
\begin{table}[H]
	\centering
	\caption{Selected fitting experiment results. \textbf{ft\_sos} indicates the inclusion of sums of squares features meanwhile \textbf{ft\_bin} indicates the inclusion of binomial features; \textbf{t\_solver} is the time in seconds spent by CGLS; and \textbf{t\_pred} is the energy prediction time. \textbf{MAE} is in kcal/mol.}
	\begin{tabular}{|l|l|l|l|l|l|l|l|}
		\hline
		\multicolumn{1}{|c|}{\textbf{$n_{af}$}} & \multicolumn{1}{c|}{\textbf{$n_{mf}$}} & \multicolumn{1}{c|}{\textbf{$n_s$}} &\multicolumn{1}{c|}{\textbf{MAE}} & \multicolumn{1}{c|}{\textbf{ft\_sos}} & \multicolumn{1}{c|}{\textbf{ft\_bin}} & \multicolumn{1}{c|}{\textbf{t\_solver}} & \multicolumn{1}{c|}{\textbf{t\_pred}} \\ \hline
		4                                    & 18                                  & 5                                      & 2335                        & 0                                     & 0                                     & 409                                   & 93                                  \\ \hline
		4                                    & 19                                  & 5                                      & 2413                        & 0                                     & 0                                     & 445                                   & 113                                 \\ \hline
		4                                    & 26                                  & 5                                      & 2623                        & 0                                     & 0                                     & 566                                   & 160                                 \\ \hline
		4                                    & 23                                  & 5                                      & 2647                        & 1                                     & 0                                     & 529                                   & 136                                 \\ \hline
		6                                    & 28                                  & 5                                      & 2651                        & 0                                     & 0                                     & 623                                   & 179                                 \\ \hline
	\end{tabular}
	\label{tab:shepard_exp}
\end{table}
\fi

\subsubsection{Null model and reduced energies}
\label{subsubsec:ered}
The null model serves to reduce the molecular energy fitting difficulty by decreasing the order of magnitude of the fitted energy, since by default the molecular energy itself has a large order of magnitude (e.g., the internal energy at $0^{\circ}$ Kelvin $U_0$ of QM9 molecules are between $\mathcal{O}(100)$ to $\mathcal{O}(1000)$ in Hartree unit). We can obtain the reduced energy of a molecule by removing the null energy
\begin{equation}
	\label{eq:ered}
	E^\text{red} = E - E^\text{null},
\end{equation}
where the null energy is
\begin{equation}
	E^\text{null} = \sum_{j \in P} N_j E^\text{atom}_j,
\end{equation}
where $P$ is a set of atom types, $N_j$ is the count of the atom type $j$ within a fixed molecule, and $E^\text{atom}_j$ is the the energy of atom type $j$.

The atomic energy itself can be obtained by fitting a LLS model. For QM9 challenge, (\ref{eq:lls}) can be used to obtain the atomic energies, explicitly by defining
\begin{equation}
	\label{eq:enull2}
	\begin{split}
		A_{i:} &:= [N^i_\text{H}, N^i_\text{C}, N^i_\text{O}, N^i_\text{N}, N^i_\text{F}], \\
		\theta &:= [E^\text{atom}_\text{H}, E^\text{atom}_\text{C}, E^\text{atom}_\text{O}, E^\text{atom}_\text{N}, E^\text{atom}_\text{F}], \\
		b_i &:= E_i, \\
		\text{for }i &\in K.
	\end{split}
\end{equation}
\subsubsection{Choice of training sets}
The set $K$ of training labels depends on the set of features computed during the feature selection process. 
The complete sets of features used to generate the 95 sets of labels are displayed in Table \ref{tab:app_gen_labels} in the appendix. 
The scatter plot of the training MAE against the full MAE of the whole 95 sets of labels is shown in figure \ref{fig:atomic_MAE}
Each set is generated by \codeword{usequence}, here each set of features is used to generate 5 distinct sets of training labels. 10 results of the atomic energies fitting against the molecular energies $E$ which gives the lowest training MAE is shown in Table \ref{tab:null_exp}. 
The labels from the 10 sets are displayed in Table \ref{tab:app_10_sets_labels} in the appendix.
The full MAE from fitting the atomic energies is in fact refers to the mean absolute value of the $E^\text{red}$ of all molecules in the QM9 dataset itself.

\begin{table}[H]
	\centering
	\caption{10 best (lowest training MAE) numerical experiments of fitting the null model to predict the null energies picked fro 95 experiments. The \textbf{MAE}s and the atomic energies are in kcal/mol. The last row shows the experiment of null model fitted to $N_{QM9}$ data points.}
	\begin{tabular}{|c|r|r|r|r|r|r|r|}
		\hline
		$k$	& $E_H^\text{atom}$	& $E_C^\text{atom}$	& $E_O^\text{atom}$	& $E_N^\text{atom}$	& $E_F^\text{atom}$	&  \textbf{\begin{tabular}[c]{@{}c@{}}Train \\ MAE\end{tabular}}	&  \textbf{\begin{tabular}[c]{@{}c@{}}Full \\ MAE\end{tabular}}	\\ \hline
		38	& -378.435	& -23890.923	& -34354.419	& -47206.604	& -62689.712	& 15.960	& 20.243	\\ \hline
		95	& -379.462	& -23890.242	& -34352.997	& -47203.710	& -62663.338	& 16.004	& 20.079	\\ \hline
		31	& -379.173	& -23891.959	& -34354.331	& -47199.134	& -62666.546	& 16.134	& 20.280	\\ \hline
		48	& -378.438	& -23891.821	& -34352.819	& -47204.099	& -62661.154	& 16.201	& 20.035	\\ \hline
		83	& -380.433	& -23888.399	& -34349.627	& -47206.065	& -62689.308	& 16.458	& 21.250	\\ \hline
		8	& -377.764	& -23894.064	& -34354.560	& -47202.149	& -62684.920	& 16.912	& 20.726	\\ \hline
		1	& -378.731	& -23892.146	& -34353.273	& -47204.459	& -62671.670	& 17.109	& 20.435	\\ \hline
		45	& -380.123	& -23889.676	& -34354.064	& -47203.101	& -62656.640	& 17.287	& 20.146	\\ \hline
		26	& -378.354	& -23892.807	& -34351.090	& -47202.015	& -62663.825	& 17.315	& 20.341	\\ \hline
		91	& -376.980	& -23894.187	& -34352.579	& -47204.336	& -62658.886	& 17.421	& 20.226	\\ \hline
		all	& -378.666  & -23891.253 	& -34354.806	& -47203.592 	& -62665.984	& 19.886	& 19.886	\\ \hline
	\end{tabular}
	\label{tab:null_exp}
\end{table}

\begin{figure}[H]
	\label{fig:atomic_MAE}
	\centering
	\includegraphics[scale=0.6]{plot/MAE_scatter.png}
	\caption{Scatter plot of training MAE against full MAE for 95 training sets.}
\end{figure}

\subsubsection{Fitting reduced energies}
The fitting experiments with $E^\text{red}$ as the target are run by varying the models, the set of features and the training labels. In total there are 50 instances, 10 of which with the lowest MAE are shown in Table \ref{tab:best_10_exp}.
One set of features and one set of training labels from the lowest fitting MAE is chosen to compare the models, the results are shown in Table \ref{tab:model_comparison}. Here the MAEs are computed from the original energies, since the predicted energies are actually the predicted reduced energies,
\begin{equation}
	\text{MAE} := \frac{1}{N_{QM9}}\sum_{m \in W}|E_{m} - (\hat{E}_m + E^\text{null}_m)|
\end{equation}

The models' hyperparameters are fixed across all experiments. ROSEMI uses $n_s = 6$, KRR uses scaler $c=1$, GAK uses $\sigma = 2048.$; and the neural network (NN) architecture used here is composed of \codeword{nodes }$ = [n_{mf}, 10, 1]$, where each entry represents the number of nodes in $i$th layer; ReLU is chosen as the activation function in the hidden layer (2nd layer); the NN is optimized using Adam with 0.01 momentum \at{cite Adam}. 

FCHL \at{cite Lilienfeld et al} is also independently tested, it results in 9.3 kcal/mol MAE. The feature extraction, fitting, and prediction is done using the quantum machine learning (QML) package \at{cite QML \url{https://www.qmlcode.org}} in Python. 
The training set indices $K$ is computed from the processed ACSF set of features with $n_{af} = 4$ and $n_{mf} = 18$.
The hyperparameters used are: cutoff radius $r_\text{cut} = 8.0$ for both the FCHL feature extraction and the local kernel generation; and Gaussian scaler $\sigma = 32.0$ for the local kernel generation.

\begin{table}[H]
	\centering
	\caption{10 numerical experiments with the lowest \textbf{MAE} picked from 50 experiments, which is 5 model experiments for each selected index $k$. The MAEs are in kcal/mol. Null MAE refers to the MAE of the null model (in kcal/mol). $k$ refers to the index of set of labels used for training. KRR refers to (\ref{eq:gaussian_mol}), GAK refers to (\ref{eq:gaussian_atom}), and LLS refers to (\ref{eq:lls}). $f$ is the type of atomic features, $n_{af}$ is the number of atomic features, $n_{mf}$ is the number of molecular features, $t_s$ is the solver time to train the model in seconds, and $t_p$ is the prediction time of $N_\text{qm9}$ molecules in seconds.}
	\begin{tabular}{|c|c|c|c|c|c|c|c|r|}
		\hline
		\textbf{MAE}	& \textbf{\begin{tabular}[c]{@{}c@{}}Null \\ train \\MAE\end{tabular}}	& \textbf{model}	& $k$	& $f$	& $n_{af}$	& $n_{mf}$	& $t_s$	& $t_p$	\\ \hline
		11.626	& 15.960	& GAK	& 38	& ACSF	& 20	& 16	& 0.043	& 731.476	\\ \hline
		11.626	& 17.287	& GAK	& 45	& ACSF	& 20	& 16	& 0.044	& 676.670	\\ \hline
		12.188	& 17.315	& GAK	& 26	& ACSF	& 30	& 29	& 0.044	& 352.096	\\ \hline
		12.971	& 16.912	& GAK	& 8	& ACSF	& 40	& 37	& 0.043	& 401.476	\\ \hline
		13.227	& 16.201	& GAK	& 48	& ACSF	& 20	& 15	& 0.044	& 700.244	\\ \hline
		13.575	& 17.315	& LLS	& 26	& ACSF	& 30	& 29	& 0.002	& 0.011	\\ \hline
		13.582	& 16.134	& GAK	& 31	& ACSF	& 30	& 20	& 0.043	& 980.447	\\ \hline
		15.059	& 17.315	& KRR	& 26	& ACSF	& 30	& 29	& 0.045	& 8.874	\\ \hline
		15.328	& 16.912	& KRR	& 8	& ACSF	& 40	& 37	& 0.044	& 8.846	\\ \hline
		16.245	& 16.134	& LLS	& 31	& ACSF	& 30	& 20	& 0.001	& 0.006	\\ \hline
	\end{tabular}
	\label{tab:best_10_exp}
\end{table}

\begin{table}[H]
	\centering
	\caption{Model comparison under the same sets of training label and feature sets (the feature set in the first row of Table \ref{tab:best_10_exp}). ROSEMI refers to (\ref{eq:rosemi_linear}), and NN refers to (\ref{eq:NN})}.
	\begin{tabular}{|c|c|r|r|}
		\hline
		\textbf{MAE}	& \textbf{model}	& $t_s$	& $t_p$	\\ \hline
		19.776	& ROSEMI	& 906.468	& 268.563	\\ \hline
		21.521	& KRR	& 0.045	& 9.921	\\ \hline
		20.244	& NN	& 16.238	& 0.008	\\ \hline
		18.507	& LLS	& 0.001	& 0.004	\\ \hline
		11.626	& GAK	& 0.043	& 731.476	\\ \hline
	\end{tabular}
	\label{tab:model_comparison}
\end{table}


\subsubsection{Hyperparameter optimization (Hyperopt)}
To find the best model configuration, here we conducted numerical experiments using hyperparameters optimized by a derivative--free optimizer \at{cite Morteza's software}. 
\paragraph{Hyperopt with box constraints}
Infeasible solutions exist as the outputs of unconstrained solvers, one such example is when $n_{mf} \leq 0$. Here infeasible solutions are treated in a straightforward manner by these options:
\begin{itemize}
	\item Set a very high MAE for infeasible solutions.
	\item Round infeasible solutions to the boundary. This only applies for box--constraints.
\end{itemize}

The set of hyperparameters chosen to be optimized is the subset of hyperparameters explained in section \ref{sec:hyperspace}, which are:
\begin{equation*}
	H := \{p_{af}, p_{mf}, n_s, \codeword{ftype}, \codeword{norm_af}, \codeword{norm_mf}, k, \codeword{model}, c\},
\end{equation*}
where $p_{af}, p_{mf} \in (0,1] \cap \mathbb{R}$ are the percentage of the features selected, 
\begin{equation*}
	n_{af} = p_{af} \bar{n}_{af},
\end{equation*}
where $\bar{n}_{af}$ is the maximum possible number of atomic feature selected, e.g., for ACSF $\bar{n}_{af} = 51$; this is also analogous for the percentage of molecular features $p_{mf}$.
$H$ is initialized by the set of hyperparameters which produced the lowest MAE in table \ref{tab:best_10_exp} (1st row) with perturbation
\begin{equation*}
	H = \{0.392, 0.800, 3, \text{ACSF}, \codeword{true}, \codeword{true}, 38, \text{GAK}, 32\}
\end{equation*}
A series of experiments by setting infeasible solutions to have $\text{MAE} = 100$ is done, 30 latest hyperparameters found are shown in table \ref{tab:hyperopt}.
The second series of experiments is done by projecting the infeasible solutions into the boundaries of feasible solutions, the bounds are
\begin{equation}
	H_\text{bounds} =
	\begin{pmatrix}
		0 & 0 & 1  & 1  & 0  & 0 &  0  & 1 &  1 \\
		1 & 1 & \codeword{Inf} & 3  & 1  & 1 &  \codeword{Inf}& 5 &  \codeword{Inf}
	\end{pmatrix},
	\label{eq:Hbound}
\end{equation}
where each column corresponds to the each hyperparameter boundaries. 25 hyperparameters found are in Table \ref{tab:hyperopt2}.


\begin{table}[H]
	\centering
	\caption{Sets of hyperparameters and their MAE found during the hyperparameter optimization process. \textbf{MAE} is in kcal/mol. Here infeasible points are set to have $\text{MAE} = 100$.} 
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\textbf{MAE}	& $p_{af}$ & $p_{mf}$ & $n_s$ & \codeword{ftype} & \codeword{norm_af} & \codeword{norm_mf} & $k$ & \codeword{model} & $c$	\\ \hline
		11.785	& 0.389	& 0.798	& 2	& ACSF	& true	& true	& 38	& GAK	& 33	\\ \hline
		11.785	& 0.393	& 0.789	& 3	& ACSF	& true	& true	& 38	& GAK	& 33	\\ \hline
		11.785	& 0.390	& 0.798	& 2	& ACSF	& true	& true	& 38	& GAK	& 33	\\ \hline
		11.785	& 0.390	& 0.798	& 3	& ACSF	& true	& true	& 38	& GAK	& 33	\\ \hline
		11.872	& 0.396	& 0.797	& 3	& ACSF	& true	& true	& 38	& GAK	& 32	\\ \hline
		11.881	& 0.393	& 0.802	& 3	& ACSF	& true	& true	& 38	& GAK	& 31	\\ \hline
		12.070	& 0.374	& 0.925	& 2	& ACSF	& true	& true	& 38	& GAK	& 33	\\ \hline
		12.572	& 0.390	& 0.798	& 2	& ACSF	& true	& true	& 37	& GAK	& 33	\\ \hline
		12.572	& 0.390	& 0.799	& 2	& ACSF	& true	& true	& 37	& GAK	& 33	\\ \hline
		12.572	& 0.389	& 0.798	& 3	& ACSF	& true	& true	& 37	& GAK	& 33	\\ \hline
		12.572	& 0.390	& 0.798	& 2	& ACSF	& true	& true	& 37	& GAK	& 33	\\ \hline
		12.583	& 0.391	& 0.798	& 3	& ACSF	& true	& true	& 37	& GAK	& 32	\\ \hline
		12.738	& 0.466	& 0.708	& 2	& ACSF	& true	& true	& 39	& GAK	& 31	\\ \hline
		17.959	& 0.503	& 0.989	& 3	& ACSF	& false	& true	& 38	& GAK	& 32	\\ \hline
		18.024	& 0.396	& 0.800	& 3	& ACSF	& false	& true	& 38	& GAK	& 32	\\ \hline
		18.408	& 0.392	& 0.800	& 4	& ACSF	& true	& true	& 37	& LLS	& 32	\\ \hline
		15.975	& 0.390	& 0.798	& 2	& SOAP	& true	& true	& 38	& GAK	& 32	\\ \hline
		15.975	& 0.389	& 0.799	& 3	& SOAP	& true	& true	& 38	& GAK	& 32	\\ \hline
		15.975	& 0.390	& 0.799	& 3	& SOAP	& true	& true	& 38	& GAK	& 32	\\ \hline
		15.975	& 0.387	& 0.798	& 4	& SOAP	& true	& true	& 38	& GAK	& 32	\\ \hline
		15.975	& 0.391	& 0.799	& 3	& SOAP	& true	& true	& 38	& GAK	& 32	\\ \hline
		16.002	& 0.390	& 0.799	& 2	& SOAP	& true	& true	& 38	& GAK	& 33	\\ \hline
		16.002	& 0.390	& 0.799	& 2	& SOAP	& true	& true	& 38	& GAK	& 33	\\ \hline
		16.492	& 0.390	& 0.799	& 2	& SOAP	& true	& true	& 37	& GAK	& 33	\\ \hline
		16.492	& 0.387	& 0.795	& 3	& SOAP	& true	& true	& 37	& GAK	& 33	\\ \hline
		16.492	& 0.388	& 0.798	& 2	& SOAP	& true	& true	& 37	& GAK	& 33	\\ \hline
		20.237	& 0.312	& 0.998	& 3	& SOAP	& false	& true	& 38	& GAK	& 33	\\ \hline
		20.240	& 0.403	& 0.782	& 4	& SOAP	& false	& true	& 38	& GAK	& 31	\\ \hline
		21.886	& 0.164	& 0.977	& 4	& SOAP	& true	& false	& 38	& LLS	& 32	\\ \hline
		25.816	& 0.250	& 0.867	& 3	& SOAP	& false	& true	& 38	& LLS	& 31	\\ \hline
	\end{tabular}
	\label{tab:hyperopt}
\end{table}

\begin{table}[H]
	\centering
	\caption{Sets of hyperparameters and their MAE found during the hyperparameter optimization process. \textbf{MAE} is in kcal/mol. Here infeasible points are projected into the boundaries of feasible solutions.} 
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\textbf{MAE}	& $p_{af}$ & $p_{mf}$ & $n_s$ & \codeword{ftype} & \codeword{norm_af} & \codeword{norm_mf} & $k$ & \codeword{model} & $c$	\\ \hline
		12.063	& 0.465	& 0.926	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		12.580	& 0.392	& 0.800	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		12.736	& 0.377	& 0.811	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		12.869	& 0.392	& 0.800	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		12.962	& 0.445	& 0.815	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		12.977	& 0.366	& 0.773	& 3	& ACSF	& true	& false	& 0	& GAK	& 32.0	\\ \hline
		13.091	& 0.409	& 0.805	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		13.192	& 0.376	& 0.795	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		13.295	& 0.392	& 0.800	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		13.313	& 0.093	& 1.000	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		13.512	& 0.392	& 0.800	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		13.628	& 0.418	& 0.827	& 3	& ACSF	& true	& true	& 0	& GAK	& 31.0	\\ \hline
		13.660	& 0.392	& 0.800	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		13.728	& 0.393	& 0.799	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		13.729	& 0.691	& 0.415	& 4	& ACSF	& true	& true	& 0	& GAK	& 33.0	\\ \hline
		13.768	& 0.392	& 0.801	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		13.866	& 0.394	& 0.795	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		13.869	& 0.392	& 0.800	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		14.144	& 0.392	& 0.800	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		14.383	& 0.394	& 0.793	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		14.474	& 0.392	& 0.800	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		14.813	& 0.391	& 0.805	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		15.130	& 0.407	& 0.789	& 3	& ACSF	& true	& true	& 0	& GAK	& 32.0	\\ \hline
		19.037	& 0.340	& 0.785	& 3	& ACSF	& true	& true	& 0	& LLS	& 32.0	\\ \hline
		20.871	& 0.320	& 0.674	& 4	& SOAP	& false	& true	& 0	& GAK	& 33.0	\\ \hline
	\end{tabular}
	\label{tab:hyperopt2}
\end{table}

\paragraph{Hyperopt with special constraints}
As explained in section \ref{sec:hyperspace}, several variables have special constraints that are implied by the types, for example, the relationship $n_{mf} \leq n_{af}$. 
In particular, here we treat categorical (\textbf{Cat}) and ordered integer variables (\textbf{OInt}) using the mechanisms in section \ref{sec:hyperopt_cat} and \ref{sec:hyperopt_oint} respectively.
The middle--point between the lower and upper bound of each variable is used as the initial starting point as shown in Table \ref{tb:hyperopt_spec}, this is done to promote more exploration.
Table \ref{tb:hyperopt_spec_res} shows the samples of hyperparameters found and their MAE.

\begin{table}[H]
	\centering
	\caption{The augmented version of Table \ref{tb:hyperparam_include}. 
	The hyperparameters passed into the hyperparameter optimization routine with its data type, possible range, best found value, and starting value. 
	Under the \textbf{type} column, \textbf{Int} means integer, \textbf{Cat} means categorical, and \textbf{OInt} means ordered integers. 
	The hyperparameters in the \textbf{best} column give the current best found MAE = 11.626.}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{hyperparameter}	& \textbf{type}	& \textbf{range}	& \textbf{best}	& \textbf{start}	\\ \hline
		$n_{mf} \leq n_{af}$	& \codeword{OInt}	& [1,50]	& [16, 20]	& 	[1/3, 1/3, 1/3] \\ \hline
		$n_s$	& \codeword{Int}	& [1,10]	& 3	& 5.5	\\ \hline
		$c$	& \codeword{Real}	& $[0,20]$	& 11	& 10.5	\\ \hline
		\codeword{ftype}	& \codeword{Cat}	& [1,3]	& 1	& $[1/3, 1/3, 1/3]$	\\ \hline
		\codeword{norm_af}	& \codeword{Cat}	& [0,1]	& 1	& $[1/2, 1/2]$	\\ \hline
		\codeword{norm_mf}	& \codeword{Cat}	& [0,1]	& 1	& $[1/2, 1/2]$	\\ \hline
		\codeword{model}	& \codeword{Cat}	& [1,5]	& 3	& $[1/5, 1/5, 1/5, 1/5, 1/5]$	\\ \hline
	\end{tabular}
	\label{tb:hyperopt_spec}
\end{table}

\begin{table}[H]
	\centering
	\caption{Sets of hyperparameters and their MAE found during the hyperparameter optimization process using the special mechanisms for \textbf{OInt} and \textbf{Cat}.} 
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
		\hline
		\textbf{MAE}	& $n_{mf}$ & $n_{af}$ & $n_s$ & \codeword{ftype} & \codeword{norm_af} & \codeword{norm_mf} & \codeword{model} & $c$	\\ \hline
		12.053	& 22	& 38	& 6	& ACSF	& 1	& 0	& GAK	& 10.356	\\ \hline
		12.053	& 22	& 38	& 1	& ACSF	& 1	& 1	& GAK	& 10.356	\\ \hline
		12.053	& 22	& 38	& 5	& ACSF	& 1	& 0	& GAK	& 10.356	\\ \hline
		15.552	& 21	& 34	& 6	& ACSF	& 0	& 1	& GAK	& 10.435	\\ \hline
		15.634	& 22	& 38	& 3	& ACSF	& 1	& 0	& LLS	& 10.356	\\ \hline
		15.655	& 22	& 38	& 6	& ACSF	& 1	& 1	& LLS	& 10.356	\\ \hline
		15.805	& 22	& 38	& 6	& ACSF	& 0	& 0	& GAK	& 10.356	\\ \hline
		15.805	& 22	& 38	& 10	& ACSF	& 0	& 1	& GAK	& 10.356	\\ \hline
		19.889	& 22	& 38	& 5	& ACSF	& 0	& 0	& ROSEMI	& 10.356	\\ \hline
		19.918	& 22	& 38	& 10	& ACSF	& 0	& 0	& ROSEMI	& 10.356	\\ \hline
		20.046	& 22	& 38	& 7	& ACSF	& 1	& 0	& ROSEMI	& 10.356	\\ \hline
		20.050	& 21	& 35	& 6	& ACSF	& 1	& 0	& ROSEMI	& 10.380	\\ \hline
		20.050	& 22	& 38	& 10	& ACSF	& 1	& 0	& ROSEMI	& 10.356	\\ \hline
		20.053	& 22	& 38	& 3	& ACSF	& 1	& 1	& ROSEMI	& 10.356	\\ \hline
		20.131	& 22	& 38	& 8	& ACSF	& 1	& 1	& ROSEMI	& 10.356	\\ \hline
		20.204	& 22	& 38	& 9	& ACSF	& 1	& 1	& ROSEMI	& 10.356	\\ \hline
		20.253	& 19	& 32	& 5	& ACSF	& 1	& 1	& NN	& 10.162	\\ \hline
		21.240	& 14	& 31	& 6	& SOAP	& 0	& 1	& ROSEMI	& 10.584	\\ \hline
		20.295	& 27	& 40	& 6	& FCHL	& 1	& 1	& NN	& 10.581	\\ \hline
		20.367	& 25	& 34	& 5	& FCHL	& 0	& 0	& GAK	& 10.269	\\ \hline
		20.367	& 22	& 38	& 5	& FCHL	& 0	& 1	& GAK	& 10.356	\\ \hline
		20.367	& 23	& 37	& 6	& FCHL	& 0	& 0	& GAK	& 10.322	\\ \hline
		20.367	& 22	& 38	& 1	& FCHL	& 0	& 1	& GAK	& 10.356	\\ \hline
		30.173	& 22	& 38	& 4	& FCHL	& 1	& 0	& GAK	& 10.356	\\ \hline
		99.999	& 22	& 38	& 1	& FCHL	& 1	& 0	& LLS	& 10.356	\\ \hline
		NaN	& 22	& 38	& 5	& FCHL	& 1	& 0	& ROSEMI	& 10.356	\\ \hline
		NaN	& 22	& 38	& 4	& FCHL	& 1	& 1	& ROSEMI	& 10.356	\\ \hline
		NaN	& 22	& 38	& 5	& FCHL	& 0	& 0	& ROSEMI	& 10.356	\\ \hline
		NaN	& 19	& 33	& 6	& FCHL	& 1	& 1	& ROSEMI	& 10.271	\\ \hline
		NaN	& 21	& 37	& 5	& FCHL	& 1	& 1	& ROSEMI	& 10.378	\\ \hline
	\end{tabular}
	\label{tb:hyperopt_spec_res}
\end{table}


\newpage
\begin{appendices}	

\begin{table}[H]
	\centering
	\caption{The feature sets which are used to generate the sets of training labels}
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline
		$k$	& $f$	& $n_{af}$	& $n_{mf}$	& \textbf{\begin{tabular}[c]{@{}c@{}}Mean \\ train \\MAE\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Std \\ train \\MAE\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Mean \\ full \\MAE\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Std \\ full \\MAE\end{tabular}} \\ \hline
		1-5	& SOAP	& 20	& 10	& 18.364	& 1.458	& 20.365	& 0.296	\\ \hline
		6-10	& ACSF	& 40	& 37	& 19.417	& 1.655	& 20.521	& 0.420	\\ \hline
		11-15	& ACSF	& 40	& 30	& 20.914	& 0.860	& 20.224	& 0.193	\\ \hline
		16-20	& ACSF	& 40	& 20	& 19.756	& 1.137	& 20.115	& 0.205	\\ \hline
		21-25	& ACSF	& 30	& 25	& 20.314	& 0.315	& 20.369	& 0.346	\\ \hline
		26-30	& ACSF	& 30	& 29	& 18.410	& 1.170	& 20.338	& 0.215	\\ \hline
		31-35	& ACSF	& 30	& 20	& 19.391	& 2.209	& 20.295	& 0.196	\\ \hline
		36-40	& ACSF	& 20	& 16	& 18.885	& 1.800	& 20.224	& 0.155	\\ \hline
		41-45	& ACSF	& 20	& 16	& 19.565	& 2.107	& 20.134	& 0.142	\\ \hline
		46-50	& ACSF	& 20	& 15	& 19.012	& 1.886	& 20.388	& 0.468	\\ \hline
		51-55	& SOAP	& 75	& 72	& 20.114	& 1.521	& 20.156	& 0.206	\\ \hline
		56-60	& SOAP	& 75	& 50	& 19.044	& 1.361	& 20.479	& 0.086	\\ \hline
		61-65	& SOAP	& 75	& 25	& 19.351	& 0.601	& 20.319	& 0.178	\\ \hline
		66-70	& SOAP	& 50	& 48	& 19.760	& 1.513	& 20.560	& 0.351	\\ \hline
		71-75	& SOAP	& 50	& 45	& 19.955	& 1.973	& 20.178	& 0.153	\\ \hline
		76-80	& SOAP	& 50	& 25	& 19.327	& 1.307	& 20.378	& 0.419	\\ \hline
		81-85	& SOAP	& 25	& 22	& 18.933	& 2.011	& 20.442	& 0.468	\\ \hline
		86-90	& SOAP	& 25	& 23	& 20.110	& 0.595	& 20.215	& 0.250	\\ \hline
		91-95	& SOAP	& 25	& 22	& 17.677	& 1.157	& 20.222	& 0.224	\\ \hline
	\end{tabular}
	\label{tab:app_gen_labels}
\end{table}


\newpage
\begin{table}[H]
	\captionsetup{font=scriptsize}
	\tiny
	\centering
	\caption{The molecular labels from 10 sets which result in the lowest null training MAE.}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
		\hline
		$k=38$	& $k=95$	& $k=31$	& $k=48$	& $k=83$	& $k=8$	& $k=1$	& $k=45$	& $k=26$	& $k=91$	\\ \hline
		95178	& 80056	& 8298	& 58768	& 80368	& 61182	& 57345	& 58000	& 109079	& 71281	\\ \hline
		29069	& 101483	& 23686	& 23821	& 21951	& 133215	& 23785	& 99841	& 109455	& 47253	\\ \hline
		130049	& 81831	& 85743	& 101692	& 128149	& 121095	& 97934	& 118703	& 24927	& 123121	\\ \hline
		100842	& 123043	& 26524	& 124027	& 121875	& 75876	& 66699	& 825	& 131233	& 133311	\\ \hline
		132549	& 124471	& 124024	& 133355	& 96178	& 126818	& 5648	& 130229	& 23772	& 127756	\\ \hline
		52422	& 133437	& 82915	& 37794	& 37940	& 24457	& 120963	& 72552	& 133653	& 635	\\ \hline
		99118	& 35013	& 122995	& 128305	& 99977	& 29717	& 77829	& 56137	& 122533	& 83165	\\ \hline
		125747	& 14633	& 130107	& 126263	& 40	& 116729	& 130734	& 33311	& 23910	& 25312	\\ \hline
		132240	& 21166	& 35099	& 263	& 2603	& 62472	& 1884	& 7125	& 126564	& 128742	\\ \hline
		122757	& 41	& 97474	& 10199	& 122517	& 21575	& 130219	& 62183	& 66334	& 100178	\\ \hline
		4369	& 57362	& 133402	& 23902	& 129880	& 21868	& 32471	& 124477	& 117715	& 26643	\\ \hline
		9349	& 122592	& 151	& 38450	& 117718	& 14984	& 25270	& 113900	& 41287	& 101578	\\ \hline
		63712	& 95105	& 97808	& 10067	& 51902	& 83254	& 187	& 165	& 3933	& 125428	\\ \hline
		47245	& 122161	& 36755	& 42080	& 56600	& 84194	& 122852	& 19603	& 122726	& 21812	\\ \hline
		695	& 118440	& 72556	& 53069	& 132211	& 121448	& 35457	& 117520	& 71475	& 60127	\\ \hline
		67489	& 94425	& 123178	& 62308	& 105982	& 1750	& 49178	& 20452	& 23610	& 132141	\\ \hline
		27535	& 31552	& 20217	& 125168	& 7125	& 17578	& 129002	& 28073	& 133073	& 71152	\\ \hline
		78263	& 169	& 55208	& 23757	& 77823	& 39927	& 106017	& 23368	& 133295	& 132243	\\ \hline
		34115	& 48146	& 101558	& 111105	& 47200	& 57611	& 80898	& 70514	& 133447	& 96178	\\ \hline
		33741	& 74619	& 99978	& 394	& 34122	& 97455	& 122285	& 45109	& 35140	& 2680	\\ \hline
		21100	& 6028	& 16237	& 1532	& 119847	& 123396	& 126452	& 456	& 125	& 133383	\\ \hline
		14741	& 44216	& 126768	& 21339	& 78955	& 58014	& 31023	& 122106	& 25168	& 52428	\\ \hline
		43498	& 2823	& 131915	& 62119	& 574	& 3869	& 57087	& 21461	& 25316	& 34949	\\ \hline
		102520	& 56489	& 57964	& 119133	& 99845	& 132472	& 83255	& 102345	& 37550	& 128823	\\ \hline
		126043	& 2521	& 2480	& 100017	& 124068	& 121469	& 2875	& 52825	& 275	& 9391	\\ \hline
		1378	& 66798	& 130001	& 119281	& 26616	& 16016	& 26360	& 61016	& 116057	& 126606	\\ \hline
		46227	& 72807	& 55429	& 97942	& 38625	& 35398	& 72080	& 1200	& 32172	& 65	\\ \hline
		2687	& 110881	& 9683	& 22154	& 131459	& 127694	& 122658	& 4790	& 7397	& 482	\\ \hline
		14603	& 58151	& 108678	& 17640	& 58375	& 52948	& 3861	& 71547	& 92005	& 126771	\\ \hline
		684	& 86714	& 30946	& 101678	& 9282	& 14551	& 87118	& 104319	& 52428	& 117408	\\ \hline
		121974	& 2551	& 3950	& 38816	& 102061	& 46471	& 13948	& 77091	& 40775	& 100875	\\ \hline
		96711	& 23881	& 55400	& 3018	& 109682	& 104571	& 102186	& 2100	& 131558	& 36249	\\ \hline
		115203	& 86269	& 37221	& 56237	& 33524	& 37707	& 125445	& 122640	& 14653	& 63761	\\ \hline
		81070	& 129192	& 22013	& 53425	& 6841	& 113687	& 102294	& 103946	& 36734	& 42624	\\ \hline
		3457	& 71872	& 77624	& 52437	& 6385	& 1313	& 79101	& 97383	& 109567	& 85921	\\ \hline
		127186	& 133082	& 122947	& 34909	& 25312	& 97836	& 132648	& 29981	& 7829	& 55175	\\ \hline
		13887	& 116929	& 1925	& 17172	& 275	& 51557	& 44825	& 48117	& 100354	& 111021	\\ \hline
		11556	& 271	& 11252	& 105754	& 64011	& 130078	& 52736	& 55620	& 406	& 13664	\\ \hline
		80801	& 116831	& 81013	& 124785	& 31058	& 27963	& 131841	& 39026	& 67944	& 92343	\\ \hline
		97541	& 118415	& 56291	& 90368	& 133205	& 4273	& 103691	& 19902	& 21414	& 747	\\ \hline
		83224	& 29549	& 21583	& 33497	& 4200	& 124023	& 9888	& 125625	& 122222	& 119927	\\ \hline
		564	& 17533	& 118016	& 19194	& 121593	& 97576	& 13837	& 33279	& 56367	& 18782	\\ \hline
		26372	& 110750	& 73307	& 122829	& 97495	& 88982	& 36656	& 2272	& 41894	& 122007	\\ \hline
		82796	& 685	& 61427	& 68682	& 119623	& 115042	& 34611	& 28375	& 100204	& 93129	\\ \hline
		38533	& 115076	& 7391	& 78111	& 72175	& 32155	& 21272	& 4223	& 3110	& 102339	\\ \hline
		99376	& 120323	& 55675	& 132359	& 97522	& 111463	& 96554	& 2080	& 33583	& 21735	\\ \hline
		59370	& 472	& 28815	& 129902	& 105899	& 113693	& 132244	& 120975	& 3719	& 124537	\\ \hline
		42900	& 132470	& 122620	& 43009	& 102498	& 19731	& 9218	& 99313	& 38606	& 66719	\\ \hline
		27403	& 47997	& 51838	& 6751	& 30692	& 100597	& 79340	& 36748	& 132264	& 74072	\\ \hline
		1414	& 89327	& 119895	& 102394	& 123066	& 99859	& 132822	& 117331	& 74674	& 133291	\\ \hline
		59758	& 121867	& 77590	& 10341	& 43055	& 28773	& 133424	& 114722	& 99567	& 133126	\\ \hline
		102208	& 13378	& 88837	& 71652	& 115670	& 1012	& 62528	& 114632	& 59433	& 60279	\\ \hline
		132885	& 16767	& 38733	& 88589	& 494	& 4836	& 20793	& 20973	& 9396	& 14752	\\ \hline
		130619	& 95590	& 80119	& 27570	& 19502	& 30346	& 99945	& 127056	& 30996	& 79556	\\ \hline
		53704	& 125540	& 112780	& 20771	& 57610	& 48036	& 9413	& 110720	& 2731	& 71795	\\ \hline
		17737	& 25648	& 81746	& 1495	& 52722	& 27836	& 129054	& 50831	& 22420	& 21141	\\ \hline
		108667	& 71359	& 93973	& 112122	& 1306	& 23656	& 2937	& 99177	& 46804	& 19845	\\ \hline
		23728	& 75117	& 68054	& 116361	& 95408	& 6781	& 22269	& 33499	& 15185	& 124485	\\ \hline
		2291	& 122487	& 130970	& 97920	& 125416	& 6216	& 101020	& 11970	& 80506	& 6994	\\ \hline
		117900	& 24025	& 133273	& 34928	& 133728	& 130104	& 82731	& 98907	& 84877	& 131775	\\ \hline
		41240	& 60385	& 6111	& 88775	& 122627	& 90260	& 76238	& 133502	& 132308	& 24246	\\ \hline
		36811	& 78433	& 46351	& 42005	& 9938	& 2166	& 11332	& 132034	& 8779	& 120611	\\ \hline
		81262	& 118463	& 126933	& 35233	& 37205	& 122255	& 34709	& 47813	& 50576	& 268	\\ \hline
		24519	& 127219	& 90675	& 117331	& 79300	& 42903	& 43904	& 104648	& 53406	& 1749	\\ \hline
		18280	& 39337	& 131905	& 132011	& 1395	& 3410	& 132485	& 102942	& 129731	& 93575	\\ \hline
		33676	& 37547	& 83702	& 101673	& 76300	& 55522	& 49734	& 38330	& 14241	& 113749	\\ \hline
		98982	& 117317	& 20702	& 2283	& 117006	& 126787	& 23270	& 34571	& 98780	& 65758	\\ \hline
		111511	& 3526	& 12646	& 126400	& 116162	& 33800	& 23669	& 81376	& 128868	& 10001	\\ \hline
		3125	& 98136	& 131763	& 121358	& 14541	& 117966	& 40587	& 634	& 133634	& 106993	\\ \hline
		73149	& 22027	& 122810	& 133031	& 106155	& 61436	& 59890	& 25692	& 42586	& 23913	\\ \hline
		116856	& 52530	& 543	& 11763	& 133120	& 950	& 84789	& 120559	& 101568	& 2829	\\ \hline
		55638	& 97350	& 28725	& 8562	& 112913	& 53798	& 124648	& 102241	& 129119	& 71359	\\ \hline
		66568	& 9996	& 9846	& 48152	& 22055	& 43516	& 52773	& 68491	& 38450	& 118900	\\ \hline
		21924	& 1457	& 76210	& 83481	& 100720	& 49583	& 123199	& 80381	& 13881	& 57378	\\ \hline
		62552	& 75928	& 117093	& 5569	& 85724	& 32832	& 56071	& 132974	& 76803	& 271	\\ \hline
		46082	& 62614	& 35250	& 132736	& 131158	& 25401	& 56062	& 21553	& 99613	& 119588	\\ \hline
		79786	& 129082	& 131421	& 123165	& 81499	& 108416	& 127432	& 11338	& 28194	& 95309	\\ \hline
		48159	& 99640	& 1659	& 86981	& 33010	& 37104	& 95604	& 3265	& 18056	& 125746	\\ \hline
		128542	& 84595	& 132370	& 131028	& 81410	& 49447	& 9586	& 58200	& 21228	& 15254	\\ \hline
		103920	& 55677	& 82806	& 34074	& 6705	& 114718	& 102941	& 1386	& 107274	& 108344	\\ \hline
		28261	& 9376	& 109642	& 114831	& 72427	& 64648	& 58974	& 40982	& 35682	& 6082	\\ \hline
		36824	& 117282	& 394	& 101598	& 2456	& 100528	& 112380	& 56044	& 86685	& 16147	\\ \hline
		6915	& 35915	& 3631	& 7292	& 129123	& 13347	& 132280	& 172	& 29787	& 8791	\\ \hline
		2531	& 116988	& 34991	& 61846	& 32825	& 84009	& 58967	& 121005	& 33339	& 48614	\\ \hline
		34923	& 104097	& 125555	& 85643	& 47989	& 77329	& 17327	& 93121	& 120600	& 49135	\\ \hline
		16548	& 39341	& 52686	& 132797	& 99893	& 36135	& 128642	& 19116	& 34152	& 118201	\\ \hline
		2565	& 133594	& 22298	& 55544	& 97145	& 23616	& 100463	& 79129	& 132457	& 124022	\\ \hline
		129844	& 48443	& 83213	& 2229	& 12868	& 54807	& 31602	& 116804	& 102844	& 55520	\\ \hline
		16224	& 36205	& 120498	& 55509	& 21374	& 131344	& 125132	& 32570	& 115507	& 122317	\\ \hline
		122072	& 27867	& 4623	& 43084	& 36806	& 47953	& 42437	& 61052	& 22177	& 69241	\\ \hline
		13257	& 130131	& 35381	& 71634	& 6911	& 39550	& 19062	& 96597	& 7992	& 133228	\\ \hline
		57820	& 130888	& 30660	& 133215	& 66210	& 126232	& 56270	& 24860	& 86077	& 90165	\\ \hline
		129689	& 126676	& 127832	& 100822	& 56322	& 83838	& 39555	& 68343	& 79904	& 12473	\\ \hline
		100240	& 17814	& 35726	& 68021	& 10226	& 31505	& 96278	& 42869	& 20523	& 99299	\\ \hline
		33383	& 45233	& 1855	& 8950	& 57618	& 58012	& 61267	& 57985	& 124953	& 126770	\\ \hline
		35465	& 111766	& 57380	& 128220	& 91203	& 126508	& 19987	& 92259	& 97829	& 116488	\\ \hline
		30805	& 131683	& 78097	& 126376	& 108938	& 61063	& 39548	& 112311	& 29744	& 29911	\\ \hline
		15817	& 68890	& 42312	& 101342	& 83843	& 123780	& 35694	& 116649	& 20752	& 52938	\\ \hline
		46276	& 104265	& 16832	& 22215	& 116692	& 120972	& 93410	& 54335	& 98280	& 84026	\\ \hline
		105833	& 36573	& 93851	& 13489	& 75434	& 94516	& 117036	& 118309	& 70773	& 6739	\\ \hline
	\end{tabular}
	\label{tab:app_10_sets_labels}
\end{table}

\end{appendices}


\iffalse
========= some comments========== \\
$b$ must of course be independent of $\theta$.

In (8) there is no argument $w$; instead you need to minimize the expression
\begin{equation}
    |K| \sum_m RMSD_K(w_m)^2,
\end{equation}
where $m$ labels the molecules. To find the linear system you need to write the right hand side of (7) as
\begin{equation}
    \sum_{kl} A_j(w)_{kl} \theta_{kl} -b_j(w)
\end{equation}
and find the expressions for $A_j(w)_{kl}$ and $b_j(w)$ by comparing the coefficients of $\theta_{kl}$ and the $\theta$-independent terms. For $A_j(w)_{kl}$ you get a sum of two terms, one involving a Kronecker delta. For $b_j(w)$ you get a sum over $k$ and an additional term.
The least squares problem in (8) then has a matrix $A$ with rows indexed by $j,m$ and columns indexed by $k,l$, with
\begin{equation}
    A_{jm,kl}:=A_j(w_m)_{kl},
\end{equation}
and something similar for the entries of $b$.

================== \\
You just need to extract from (7) the terms containing $\theta_{kl}$, which are

\begin{equation}
    \frac{\phi_{kl}}{D_kS_K(D_jS_K-1)}
\end{equation}


and
\begin{equation}
    \frac{\delta_{jk}\phi_{kl}D_kS_K(D_jS_K-1)}{D_jS_K-1}.
\end{equation}


Add the two and simplify, taking out common factors.

Similarly, you extract the terms without theta's by setting all $\theta$'s to zero

================== \\

Please write a program for calculating the vector v with components $v_{jm}:=\Delta_{jK}(w_m)$ using (3), (2), (1), and (7a) of your writeup, adding in (3) only the nonzero terms. Compare times with those for computing $A\theta-b$. The numerical values should be the same, up to roundoff.

===========================
OLD/UNUSED STUFFS
\begin{itemize}
	\item Processors: 4 $\times$ Intel\textregistered Core i5-4670S CPU @3.10 GHz
	\item Memory: 7.7 GB of RAM
	\item Programming language: Julia 1.7.3
\end{itemize}

\begin{table}[H]
	\centering
	\caption{Result of fitting to predict the atomic energies. \textbf{MAE} and the energies are in kcal/mol.}
	\begin{tabular}{|l|l|l|l|l|l|}
		\hline
		\multicolumn{1}{|c|}{\textbf{MAE}} & \multicolumn{1}{c|}{$E^\text{atom}_\text{H}$} & \multicolumn{1}{c|}{$E^\text{atom}_\text{C}$} & \multicolumn{1}{c|}{$E^\text{atom}_\text{O}$} & \multicolumn{1}{c|}{$E^\text{atom}_\text{N}$} & \multicolumn{1}{c|}{$E^\text{atom}_\text{F}$} \\ \hline
		20.9  &	-379.318  & -23891.528 & -34349.643 & -47199.882 & -62659.836 \\ \hline
	\end{tabular}
	\label{tab:atomic_exp}
\end{table}


\begin{table}[H]
	\centering
	\caption{5 numerical experiments with the lowest \textbf{MAE} picked from 70 experiments.
		The MAEs are in kcal/mol. KRR refers to (\ref{eq:gaussian_mol}), GAK refers to (\ref{eq:gaussian_atom}), and LLS refers to (\ref{eq:lls}).
		$f$ is the type of atomic features, $n_{af}$ is the number of atomic features, $n_{mf}$ is the number of molecular features, $t_s$ is the solver time to train the model in seconds, and $t_p$ is the prediction time of $N_\text{qm9}$ molecules in seconds.}
	\label{tab:exp_reduced_energy_1}
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline
		\textbf{MAE} & \textbf{\begin{tabular}[c]{@{}c@{}}Null \\ MAE\end{tabular}} & \textbf{model} & \textbf{$f$} & \textbf{$n_{af}$} & \textbf{$n_{mf}$} & \textbf{$t_s$} & \textbf{$t_p$} \\ \hline
		16.7         & 20.9                                                         & KRR            & SOAP         & 75                & 40                   & 0.04           & 4.72           \\ \hline
		17.0         & 20.9                                                         & GAK            & SOAP         & 75                & 40                   & 0.04           & 608.08         \\ \hline
		17.5         & 21.6                                                         & GAK            & SOAP         & 29                & 20                   & 0.04           & 823.03         \\ \hline
		18.0         & 21.5                                                         & LLS            & SOAP         & 50                & 40                   & 0.002          & 0.15           \\ \hline
		18.1         & 22.3                                                         & GAK            & SOAP         & 50                & 10                   & 0.04           & 1168.78        \\ \hline
	\end{tabular}
\end{table}

\begin{table}[H]
	\centering
	\caption{Numerical experiments with the lowest mean MAE across different models by fixing the set of features (first row of table \ref{tab:exp_reduced_energy_1}). ROSEMI refers to (\ref{eq:rosemi_linear}), NN refers to (\ref{eq:NN}).}
	\label{tab:exp_reduced_energy_2}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{MAE} & \textbf{model} & \textbf{$t_s$} & \textbf{$t_p$} \\ \hline
		16.7         & KRR            & 0.04           & 4.7            \\ \hline
		17.0         & GAK            & 0.04           & 608.08         \\ \hline
		19.4         & ROSEMI         & 641.7          & 152.8          \\ \hline
		20.4         & LLS            & 0.002          & 0.4            \\ \hline
		20.8         & NN             & 1.74           & 0.03           \\ \hline
	\end{tabular}
\end{table}


\fi

\end{document}
