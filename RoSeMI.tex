\documentclass[12pt]{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

%macros
\newcommand{\defeq}[2]{\stackrel{\mathclap{\normalfont\mbox{#1}}}{#2}}
\def\D{\displaystyle}
\def\att{                    % mark at the margin
        \marginpar[ \hspace*{\fill} \raisebox{-0.2em}{\rule{2mm}{1.2em}} ]
        {\raisebox{-0.2em}{\rule{2mm}{1.2em}} }
        }
\def\at#1{[*** \att #1 ***]}  % text needing attention
\def\spc{\hspace*{0.5cm}}

\title{RoSeMI: Robust Shepard model for interpolation}
%\author{Beryl Ramadhian Aribowo}

\begin{document}
\maketitle
The generalized Shepard model has the form of 
\begin{equation}
    \label{eq:shepard}
    V_K(w):=R_K(w)/S_K(w) ~~~ \text{ for } w \in \mathbb{R}^m \setminus \{w_k\mid k\in K \},
\end{equation}
where
\begin{equation}
    R_K(w):=\sum_{k\in K} \frac{V_k(w)}{D_k(w)},~~~
    S_K(w):=\sum_{k\in K} \frac{1}{D_k(w)},
\end{equation}
and
\begin{equation}
    \label{eq:vk}
    V_k(w) = E_k + \sum_l \theta_{kl} \phi_{kl}(w).
\end{equation}
By (\ref{eq:vk}), (\ref{eq:shepard}) can be expanded into
\begin{equation}
    \label{eq:vk_expand}
    V_K(w) := \sum_{k\in K} \frac{E_k + \sum_l \theta_{kl} \phi_{kl}(w)}{D_k(w)} / S_K(w). 
\end{equation}
The quality of the prediction accuracy is measured by the \textbf{mean absolute deviation}
\begin{equation}
    \text{MAD}_K(w) := \frac{1}{|K|}\sum_{j\in K}|\Delta_{jK}(w)|
\end{equation}
and the \textbf{root mean square deviation}
\begin{equation}
    \text{RMSD}_K(w) := \sqrt{\frac{1}{|K|}\sum_{j\in K}\Delta_{jK}(w)^2},
\end{equation}
where
\begin{equation}
    \label{eq:delta}
    \begin{split}
        \Delta_{jK}(w)&:=\D\frac{V_K(w)-V_j(w)}{D_j(w)S_K(w)-1} \\
        &\defeq{(\refeq{eq:vk},\refeq{eq:vk_expand})}{=} \frac{\D \left(\sum_{k\in K} \frac{E_k + \sum_l \theta_{kl} \phi_{kl}(w)}{D_k(w)} / S_K(w)\right) - \left(E_j + \sum_l \theta_{jl} \phi_{jl}(w)\right)}{D_j(w)S_K(w)-1}, \\
    \end{split}
\end{equation}
and $l=1,2,...L$.
Let us simplify the expressions
\begin{equation*}
    \begin{split}
        D_k &:= D_k(w), \\
        S_K &:= S_K(w), \\
        \phi_{k,l} &:= \phi_{k,l}(w),\\
        \gamma_k &:= D_kS_K, \\
        \alpha_j &:= D_jS_K-1.
    \end{split}
\end{equation*}
By continuing from the last line of (\refeq{eq:delta}) in a simplified expression, then (\refeq{eq:delta}) can be re-arranged into
\begin{equation}
    \label{eq:delta_split}
    \begin{split}
        \Delta_{j,K}(w)&= \frac{\D \left(\sum_{k\in K} \frac{E_k + \sum_l \theta_{kl} \phi_{kl}}{D_k} / S_K\right) - \left(E_j + \sum_l \theta_{jl} \phi_{jl}\right)}{\alpha_j} \\
        &= \sum_k \frac{E_k}{\gamma_k\alpha_j} + \sum_{k,l} \frac{\theta_{k,l} \phi_{k,l} (1-\gamma_k)}{\gamma_k\alpha_j} +  \sum_{k,l} \frac{\delta_{j,k}\theta_{k,l} \phi_{k,l}}{\gamma_k\alpha_j} - \frac{E_j}{\alpha_j}\\
        &= \sum_{k,l} \frac{\theta_{k,l} \phi_{k,l} (1-\gamma_k + \delta_{j,k})}{\gamma_k\alpha_j} - \left(\frac{E_j}{\alpha_j} - \sum_k\frac{E_k}{\gamma_k\alpha_j}\right),
    \end{split}
\end{equation}
where $\delta_{jk}$ is the Kronecker delta. Hence if we pick $w = w_m$, then
\begin{equation}
	\begin{split}
		\Delta_{jK}(w_m) &= \sum_{k,l} A_{(j,m), (k,l)} \theta_{k,l} - b_{j,m}, \\
	\end{split}	
\end{equation}
where
\begin{equation}
	\begin{split}
		A_{(j,m), (k,l)} &= \frac{\phi_{k,l}(w_m)\times(1 - \gamma_k(w_m) + \delta_{j,k})}{\gamma_k(w_m) \times\alpha_j(w_m)},\\
		b_{j,m} &= \frac{E_j}{\alpha_j(w_m)} - \sum_k\frac{E_k}{\gamma_k(w_m) \times \alpha_j(w_m)}.
	\end{split}
\end{equation}

\iffalse
In order to see the general pattern of (\refeq{eq:delta}), let us see a simple example with $j = 1$, $|K| = 2$, and $L = 2$; and using simplified expressions by omitting the arguments
\begin{equation*}
    \begin{split}
        D_k &:= D_k(w), \\
        S_K &:= S_K(w), \\
        \phi_{k,l} &:= \phi_{k,l}(w),
    \end{split}
\end{equation*}
hence
\begin{equation}
    \begin{split}
        \Delta_{1,K}(w)&:= \frac{\D \frac{ E_1 + \theta_{1,1}\phi_{1,1} + \theta_{1,2}\phi_{1,2}}{D_1S_K} + \frac{E_2 + \theta_{2,1}\phi_{2,1} + \theta_{2,2}\phi_{2,2}}{D_2S_K} - (E_1 + \theta_{1,1}\phi_{1,1} + \theta_{1,2}\phi_{1,2})}{D_1S_K-1} \\
        &= \D \frac{ E_1 + \theta_{1,1}\phi_{1,1} + \theta_{1,2}\phi_{1,2}}{D_1S_K(D_1S_K-1)} + \frac{E_2 + \theta_{2,1}\phi_{2,1} + \theta_{2,2}\phi_{2,2}}{D_2S_K(D_1S_K-1)} - \frac{(E_1 + \theta_{1,1}\phi_{1,1} + \theta_{1,2}\phi_{1,2})}{D_1S_K-1} \\
        &= \dots
    \end{split}
\end{equation}
\fi

A good prediction model is reflected by small MAD or RMSD, hence a minimization routine is required. In order to solve the minimzation problem, a linear sysem or least squares formulation is needed. For example, if the RMSD is chosen as the objective function then the problem formulation would be
\begin{equation}
    \label{eq:min}
    \begin{split}
        \min |K| \sum_m \text{RMSD}_K(w_m)^2 &= \min \sum_m \sum_j \Delta_{jK}(w_m)^2 \\
        &= \min \sum_m \left\| A_{(:,m),(:,:)}\theta - b_{:,m}\right\|_2^2 \\
        &= \min \|A \theta - b\|_2^2.
    \end{split}
\end{equation}
where $A$ is the data matrix with $N \times K$ row size and $K \times L$ column size, $\theta$ is the coefficient vector, and $b$ is the target vector.

\iffalse
% no longer needed:
With regards to (\ref{eq:min}), (\ref{eq:delta_split}) can be rewritten into linear system form
\begin{equation}
    \label{eq:delta_matrix}
    \begin{split}
        &\Delta_{jK}(w) := \\
        &\begin{bmatrix}
            \frac{\phi_{1,1}(1-\gamma_1 + \delta_{1,1})}{\gamma_1\alpha_1} & \frac{\phi_{1,2}(1-\gamma_1+ \delta_{1,1})}{\gamma_1\alpha_1} & \dots & \frac{\phi_{2,1}(1-\gamma_2+ \delta_{1,2})}{\gamma_2\alpha_1} & \frac{\phi_{2,1}(1-\gamma_2+ \delta_{1,2})}{\gamma_2\alpha_1} & \dots & \frac{\phi_{k,l}(1-\gamma_k+ \delta_{j,k})}{\gamma_k\alpha_j} & \dots \\
            \frac{\phi_{1,1}(1-\gamma_1 + \delta_{1,1})}{\gamma_1\alpha_2} & \frac{\phi_{1,2}(1-\gamma_1+ \delta_{1,1})}{\gamma_1\alpha_2} & \dots & \frac{\phi_{2,1}(1-\gamma_2+ \delta_{1,2})}{\gamma_2\alpha_2} & \frac{\phi_{2,1}(1-\gamma_2+ \delta_{1,2})}{\gamma_2\alpha_2} & \dots & \frac{\phi_{k,l}(1-\gamma_k+ \delta_{j,k})}{\gamma_k\alpha_j} & \dots \\
            \vdots & \dots & \dots & \dots & \dots & \dots & \dots & \dots \\
            \frac{\phi_{k,l}(1-\gamma_k+ \delta_{j,k})}{\gamma_k\alpha_j} & \dots & \dots & \dots & \dots & \dots & \dots & \dots\\
            \vdots & \dots & \dots & \dots & \dots & \dots & \dots & \dots
        \end{bmatrix}
        \begin{bmatrix}
            \theta_{1,1} \\
            \theta_{1,2} \\
            \vdots \\
            \theta_{2,1} \\
            \theta_{2,2} \\
            \vdots \\
            \theta_{k,l} \\
            \vdots \\
        \end{bmatrix}\\
        &-
        \begin{bmatrix}
            E_1/\alpha_1 \\
            E_2/\alpha_2 \\
            \vdots \\
            E_j/\alpha_j \\
            \vdots
        \end{bmatrix}
        .
    \end{split}
\end{equation}
and in the end for each row $j$ the final value will be summed with the energy terms $\sum_k \frac{E_k}{\gamma_k\alpha_j}$.
\fi

\iffalse
For all $w_v, v = 1,2...N$, the matrices can be stacked into one large data matrix, where the row count is $N \times L$; while the entries of $b$ repeat for each $w_v$, hence
\begin{equation}
    \label{eq:A_matrix}
    \begin{split}
        &A\theta - b = \\
        &\frac{
            \begin{bmatrix}
                \frac{1}{D_1(w_1)} & \frac{\phi_{1, 1}(w_1)}{D_1(w_1)} &  & \dots & \frac{\phi_{1,l}(w_1)}{D_1(w_1)} & \dots & \frac{\phi_{2,1}(w_1)}{D_2(w_1)} & \dots & \frac{\phi_{k,l}(w_1)}{D_k(w_1)} & \dots \\
                \vdots & \vdots & \vdots & & \vdots & & \vdots & & \vdots &\\
                \frac{1}{D_1(w_1)} & \frac{\phi_{1, 1}(w_1)}{D_1(w_1)} &  & \dots & \frac{\phi_{1,l}(w_1)}{D_1(w_1)} & \dots & \frac{\phi_{2,1}(w_1)}{D_2(w_1)} & \dots & \frac{\phi_{k,l}(w_1)}{D_k(w_1)} & \dots \\
                \frac{1}{D_1(w_2)} & \frac{\phi_{1, 1}(w_2)}{D_1(w_2)} &  & \dots & \frac{\phi_{1,l}(w_2)}{D_1(w_2)} & \dots & \frac{\phi_{2,1}(w_2)}{D_2(w_2)} & \dots & \frac{\phi_{k,l}(w_2)}{D_k(w_2)} & \dots \\
                \vdots & \vdots & \vdots & & \vdots & & \vdots & & \vdots &\\
                \frac{1}{D_1(w_v)} & \frac{\phi_{1, 1}(w_v)}{D_1(w_v)} &  & \dots & \frac{\phi_{1,l}(w_v)}{D_1(w_v)} & \dots & \frac{\phi_{2,1}(w_v)}{D_2(w_v)} & \dots & \frac{\phi_{k,l}(w_v)}{D_k(w_v)} & \dots \\
                \vdots & \vdots & \vdots & & \vdots & & \vdots & & \vdots &
            \end{bmatrix}
            \begin{bmatrix}
                E_1 \\
                \theta_{1,1} \\
                \theta_{1,2} \\
                \vdots \\
                E_2 \\
                \theta_{2,1} \\
                \theta_{2,2} \\
                \vdots \\
                E_k \\
                \theta_{k,l} \\
                \vdots \\
            \end{bmatrix}
        }{S_K(w)}
        - 
        \begin{bmatrix}
            E_1 + \sum_l \theta_{1,l} \phi_{1,l}(w_1) \\
            \vdots \\
            E_j + \sum_l \theta_{j,l} \phi_{j,l}(w_1) \\
            \vdots \\
            E_1 + \sum_l \theta_{1,l} \phi_{1,l}(w_v) \\
            \vdots \\
            E_j + \sum_l \theta_{j,l} \phi_{j,l}(w_v) \\
            \vdots \\
        \end{bmatrix}
        .
    \end{split}
\end{equation}
\fi

\iffalse
========= some comments========== \\
$b$ must of course be independent of $\theta$.

In (8) there is no argument $w$; instead you need to minimize the expression
\begin{equation}
    |K| \sum_m RMSD_K(w_m)^2,
\end{equation}
where $m$ labels the molecules. To find the linear system you need to write the right hand side of (7) as
\begin{equation}
    \sum_{kl} A_j(w)_{kl} \theta_{kl} -b_j(w)
\end{equation}
and find the expressions for $A_j(w)_{kl}$ and $b_j(w)$ by comparing the coefficients of $\theta_{kl}$ and the $\theta$-independent terms. For $A_j(w)_{kl}$ you get a sum of two terms, one involving a Kronecker delta. For $b_j(w)$ you get a sum over $k$ and an additional term.
The least squares problem in (8) then has a matrix $A$ with rows indexed by $j,m$ and columns indexed by $k,l$, with
\begin{equation}
    A_{jm,kl}:=A_j(w_m)_{kl},
\end{equation}
and something similar for the entries of $b$.

================== \\
You just need to extract from (7) the terms containing $\theta_{kl}$, which are

\begin{equation}
    \frac{\phi_{kl}}{D_kS_K(D_jS_K-1)}
\end{equation}


and
\begin{equation}
    \frac{\delta_{jk}\phi_{kl}D_kS_K(D_jS_K-1)}{D_jS_K-1}.
\end{equation}


Add the two and simplify, taking out common factors.

Similarly, you extract the terms without theta's by setting all $\theta$'s to zero
\fi

\end{document}
