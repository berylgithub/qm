\documentclass[12pt]{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

%macros
\newcommand{\defeq}[2]{\stackrel{\mathclap{\normalfont\mbox{#1}}}{#2}}
\def\D{\displaystyle}
\def\att{                    % mark at the margin
        \marginpar[ \hspace*{\fill} \raisebox{-0.2em}{\rule{2mm}{1.2em}} ]
        {\raisebox{-0.2em}{\rule{2mm}{1.2em}} }
        }
\def\at#1{[*** \att #1 ***]}  % text needing attention
\def\spc{\hspace*{0.5cm}}



\title{RoSeMI: Robust Shepard model for interpolation}
%\author{Beryl Ramadhian Aribowo}

\begin{document}
\maketitle


\section{The model}
The generalized Shepard model has the form of 
\begin{equation}
    \label{eq:shepard}
    V_K(w):=R_K(w)/S_K(w) ~~~ \text{ for } w \in \mathbb{R}^m \setminus \{w_k\mid k\in K \},
\end{equation}
where
\begin{equation}
    R_K(w):=\sum_{k\in K} \frac{V_k(w)}{D_k(w)},~~~
    S_K(w):=\sum_{k\in K} \frac{1}{D_k(w)},
\end{equation}
and
\begin{equation}
    \label{eq:vk}
    V_k(w) = E_k + \sum_l \theta_{kl} \phi_{kl}(w).
\end{equation}
By (\ref{eq:vk}), (\ref{eq:shepard}) can be expanded into
\begin{equation}
    \label{eq:vk_expand}
    V_K(w) := \sum_{k\in K} \frac{E_k + \sum_l \theta_{kl} \phi_{kl}(w)}{D_k(w)} / S_K(w). 
\end{equation}
The quality of the prediction accuracy is measured by the \textbf{mean absolute deviation}
\begin{equation}
    \text{MAD}_K(w) := \frac{1}{|K|}\sum_{j\in K}|\Delta_{jK}(w)|
\end{equation}
and the \textbf{root mean square deviation}
\begin{equation}
    \text{RMSD}_K(w) := \sqrt{\frac{1}{|K|}\sum_{j\in K}\Delta_{jK}(w)^2},
\end{equation}
where by (\refeq{eq:vk}) and (\refeq{eq:vk_expand})
\begin{equation}
    \label{eq:delta}
    \begin{split}
        \Delta_{jK}(w)&:=\D\frac{V_K(w)-V_j(w)}{D_j(w)S_K(w)-1} \\
        &= \frac{\D \left(\sum_{k\in K} \frac{E_k + \sum_l \theta_{kl} \phi_{kl}(w)}{D_k(w)} / S_K(w)\right) - \left(E_j + \sum_l \theta_{jl} \phi_{jl}(w)\right)}{D_j(w)S_K(w)-1}, \\
    \end{split}
\end{equation}
and $l=1,2,...L$.
Let us simplify the expressions
\begin{equation*}
	\label{eq:simp}
    \begin{split}
        D_k &:= D_k(w), \\
        S_K &:= S_K(w), \\
        \phi_{kl} &:= \phi_{kl}(w),\\
        \psi_{kl} &:= \theta_{kl}\phi_{kl}, \\
        \gamma_k &:= D_kS_K, \\
        \alpha_j &:= D_jS_K-1.
    \end{split}
\end{equation*}
By continuing from the last line of (\refeq{eq:delta}) in a simplified expression, then (\refeq{eq:delta}) can be re-arranged into
\begin{equation}
    \label{eq:delta_split}
    \begin{split}
        \Delta_{j,K}(w)&= \frac{\D \left(\sum_{k\in K} \frac{E_k + \sum_l \psi_{kl}}{D_k} / S_K\right) - \left(E_j + \sum_l \psi_{jl}\right)}{\alpha_j} \\
        &= \sum_k \frac{E_k}{\gamma_k\alpha_j} + \sum_{k,l} \frac{\psi_{kl}}{\gamma_k\alpha_j} - \sum_{l} \frac{\psi_{jl}}{\alpha_j} - \frac{E_j}{\alpha_j}\\
        &= \sum_k \frac{E_k}{\gamma_k\alpha_j} + \sum_{k,l} \frac{\psi_{kl}}{\gamma_k\alpha_j} - \sum_{l} \frac{\delta_{jk}\psi_{kl}}{\alpha_j} - \frac{E_j}{\alpha_j}\\
        &= \sum_{kl} \frac{\psi_{kl} (1-\gamma_k\delta_{jk})}{\gamma_k\alpha_j} - \left(\frac{E_j}{\alpha_j} - \sum_k\frac{E_k}{\gamma_k\alpha_j}\right),
    \end{split}
\end{equation}
where $\delta_{jk}$ is the Kronecker delta. Hence if we pick $w = w_m$, then
\begin{equation}
	\begin{split}
		\Delta_{jK}(w_m) &= \sum_{kl} A_{jm, kl} \theta_{kl} - b_{jm}, \\
	\end{split}	
\end{equation}
where
\begin{equation}
	\begin{split}
		A_{(jm), (kl)} &= \frac{\phi_{k,l}(w_m)(1 - \gamma_k(w_m) \delta_{jk})}{\gamma_k(w_m) \alpha_j(w_m)},\\
		b_{jm} &= \frac{E_j}{\alpha_j(w_m)} - \sum_k\frac{E_k}{\gamma_k(w_m)  \alpha_j(w_m)}.
	\end{split}
\end{equation}

\iffalse
In order to see the general pattern of (\refeq{eq:delta}), let us see a simple example with $j = 1$, $|K| = 2$, and $L = 2$; and using simplified expressions by omitting the arguments
\begin{equation*}
    \begin{split}
        D_k &:= D_k(w), \\
        S_K &:= S_K(w), \\
        \phi_{k,l} &:= \phi_{k,l}(w),
    \end{split}
\end{equation*}
hence
\begin{equation}
    \begin{split}
        \Delta_{1,K}(w)&:= \frac{\D \frac{ E_1 + \theta_{1,1}\phi_{1,1} + \theta_{1,2}\phi_{1,2}}{D_1S_K} + \frac{E_2 + \theta_{2,1}\phi_{2,1} + \theta_{2,2}\phi_{2,2}}{D_2S_K} - (E_1 + \theta_{1,1}\phi_{1,1} + \theta_{1,2}\phi_{1,2})}{D_1S_K-1} \\
        &= \D \frac{ E_1 + \theta_{1,1}\phi_{1,1} + \theta_{1,2}\phi_{1,2}}{D_1S_K(D_1S_K-1)} + \frac{E_2 + \theta_{2,1}\phi_{2,1} + \theta_{2,2}\phi_{2,2}}{D_2S_K(D_1S_K-1)} - \frac{(E_1 + \theta_{1,1}\phi_{1,1} + \theta_{1,2}\phi_{1,2})}{D_1S_K-1} \\
        &= \dots
    \end{split}
\end{equation}
\fi

A good prediction model is reflected by small MAD or RMSD, hence a minimization routine is required. In order to solve the minimzation problem, a linear sysem or least squares formulation is needed. For example, if the RMSD is chosen as the objective function then the problem formulation would be
\begin{equation}
    \label{eq:min}
    \begin{split}
        \min |K| \sum_m \text{RMSD}_K(w_m)^2 &= \min \sum_m \sum_j \Delta_{jK}(w_m)^2 \\
        &= \min \sum_m \left\| A_{(:,m),(:,:)}\theta - b_{:,m}\right\|_2^2 \\
        f_{\text{obj}} &:= \min \|A \theta - b\|_2^2,
    \end{split}
\end{equation}
where $A$ is the data matrix with $MN$ row size and $ML$ column size; where $M$ is the number of data points with known energies, $N$ is the number of data points without energies, and $L$ is the number of the basis features; meanwhile $\theta$ is the coefficient vector, and $b$ is the target vector.

The initial case study has the following parameters:
\begin{equation*}
	\begin{split}
		M &= 10,\\
		N &= 240,\\
		L &= 320, \\
		\text{row\_count} &= 2400,\\
		\text{col\_count} &= 3200, \\
		\text{mem\_size} &\approx 61.44 \text{ megabytes}, \\
		\text{M\_increment} &= 10, \\
		\text{fit\_iter} &= 15. \\
		\text{sparsity\_rate} &\approx 0.7.
	\end{split}
\end{equation*}
Hence in the final fitting-iteration, the linear system would have $15000 \times 48000$ sized data matrix $A$ with mem\_size $\approx 5.7$ gigabytes. The model is solved by using CGLS method with 400 maximum iterations. For the first two fitting-iteration this gives 5 and 20 seconds of fitting-time consecutively. 




\section{Feature extraction}
The feature matrix $W \in \mathbb{R}^{n_\text{data} \times n_f}$ needs to be pre-computed and stored in the non-volatile storage in order to avoid re-computation. Below is roughly the flow of feature extraction used for fitting the QM9 dataset.
\begin{itemize}
    \item Compute the molecular features using ACSF (\at{cite Behler}). This would give a vector with length $= 51$ (i.e., $n^\text{atom}_f = 51$) for each atom. Apply this to all molecules in the dataset hence there should be a total of $n_\text{data}$ ACSF extractions.
    \item For each molecule containing $n_\text{atom}$ number of atoms, symmetrize the features by
        \begin{equation}
            \begin{split}
                F_{S_j} &= \sum_{i=1}^{n_{\text{atom}}} F_{j,i},\\
                F_{Q_j} &= \sum_{i=1}^{n_{\text{atom}}} F_{j,i}^2, \\
                \text{for }j &= 1,2,..., n^\text{atom}_f. 
            \end{split}
        \end{equation}
        then by concatenating the vectors such that
        \begin{equation}
            F := (F_{S_1}, F_{S_2}, ..., F_{S_{n^\text{atom}_f}}, F_{Q_1},F_{Q_2} ..., F_{Q_{n^\text{atom}_f}}) \in \mathbb{R}^{n_f}.
        \end{equation}
    	then we stack the feature vector $F$ of each molecule as a row in the $W \in \mathbb{R}^{n_\text{data} \times n_f}$ matrix.
    \item Do feature selection to take the subset of the columns of $W$, this can be done by the following:
	    \begin{equation}
			\begin{split}
				\hat{s} &= \sum_i W_{i,:}, \\
				S &= \sum_i W_{i,:}W_{i,:}^\top, \\
				\text{ for } i &= 1,2,...,n_{\text{data}}. \\
			\end{split}
	    \end{equation}
		then we compute the mean feature vector and covariance matrix
		\begin{equation}
			\begin{split}
				\bar{u} &= \hat{s}/n_\text{data}, \\
				C &= S/n_\text{data} - \bar{u}\bar{u}^\top. \\			
			\end{split}
		\end{equation}
		We select the eigenvectors such that they correspond to the $n_\text{select} \in \mathbb{Z}^+$ largest eigenvalues, they are obtained by first doing the spectral decomposition of the covariance matrix
		\begin{equation}
			C = Q\Lambda Q^\top,
		\end{equation}
		where $\Lambda$ is the diagonal matrix containing the $k$th eigenvalue $\Lambda_{kk}$ and $Q$ is the matrix containing the $k$th eigenvector $Q_{:k}$, then we permute the columns of $Q$ and the entries of $\Lambda$ such that
		\begin{equation}
			\Lambda_{11} \geq \Lambda_{22} \geq  ... \geq \Lambda_{n_fn_f},
		\end{equation}
		and select
		\begin{equation}
			\hat{Q} = Q_{:, 1:n_\text{select}}.
		\end{equation}
		Finally the transformed feature can be obtained by
		\begin{equation}
			W_{i,:} := \hat{Q}^\top(W_{i,:} - \bar{u}), \text{ for }i = 1,2,...,n_\text{data}.
		\end{equation}
	\item Scale $W$ such that 
	\begin{equation}
		W_{ij} \in [0, 1]
	\end{equation}
	by
	\begin{equation}
		\begin{split}
			W_{:,j} := (W_{:,j} - \underbar{$W_{:,j}$})/ (\overline{W_{:,j}} - \underbar{$W_{:,j}$}), \text{ for } j = 1,2,..., n_f.
		\end{split}
	\end{equation}
	where \underbar{$W_{:,j}$} is the smallest value of the $j$th feature and $\overline{W_{:,j}}$ is the largest.
	\item Store $W$, the storage should be bounded by $O(N)$ for most cases unless much more features are needed (although currently from experiment this causes overfitting).
\end{itemize}

\section{Data selection}
The set of indices which refers to data points with known energies (or training data points) $K$ is chosen by the farthest-distance-algorithm variations, where the imposed challenge is $M = |K| = 100$. The flow of the data selection is as the following:
\begin{itemize}
	\item Compute and store distance matrix $D$ given $W$
	\begin{equation}
		D := f_{\text{dist}}(B, W) \in \mathbb{R}^{n_\text{data} \times n_\text{data}},
	\end{equation}
	where $D$ is symmetric with zero diagonal \at{might be necessary to be stored in sparse (reduces the matrix size, instead of $n \times n$ it becomes $n(n-1)/2$) for full computation of 113k molecules}, and $B$ is a matrix which transforms the feature space (currently $B := I$). The distance between two data points of index $k$ and $m$ can also be defined by
	\begin{equation}
		D_k(W_{m,:}) := \|W_{m,:} - W_{k,:}\|^2_B := \|B(W_{m,:} - W_{k,:})\|^2_2
	\end{equation}
	\item Compute and store the set of indices $K$ by farthest-minimum-distance algorithm \at{cite Eldar} given $D$, $B$ and $M$.
	\begin{equation}
		K := f_{\text{fmd}}(D, B, M) \in \mathbb{Z}^{+M},
	\end{equation}
	for example, we can obtain one data point which belongs to the training set by indexing
	\begin{equation}
		w_k := W_{k,:}, \text{ for } k \in K.
	\end{equation}
\end{itemize}
\section{Fitting procedure}
We need to compute several intermediate values and store it in the random access memory (RAM), since these intermediate values depend on several parameters which changes for each experiment and are relatively quite fast to compute hence it is not necessary to store them into the non-volatile memory. Given the stored $W, D$ and $K$:
\begin{itemize}
	\item Compute the set of indices of the data points with unknown energy (or the test set):
	\begin{equation}
		T := \{1,2,...,n_\text{data}\} \setminus K.
	\end{equation}
	\item Compute intermediate values:
	\begin{equation}
		\begin{split}
			S_K &\in \mathbb{R}^{T} := S_K(W_{m,:}) \text{ for } m \in T,\\
			\gamma &\in \mathbb{R}^{T \times M} := \gamma_k(W_{m,:}) \text{ for } k \in K, m \in T, \\
			\alpha &:= \gamma - 1.
		\end{split}
	\end{equation}
	\item Compute the univariate splines $\beta$ and its derivative $\beta'$ given $W_{i,j}$ for $i = 1,...,n_\text{data}$, $j = 1,...,n_f$ (vectorized over matrix $W$ for the spline computation and its derivative):
	\begin{equation}
		\begin{split}
			\Phi := \beta(W) \in \mathbb{R}^{n_\text{data} \times L}, \\
			\Phi' := \beta'(W) \in \mathbb{R}^{n_\text{data} \times L}.
		\end{split}
	\end{equation}
	where $L = n_fn_b$ and $n_b$ is the number of splines for each feature.
	\item Compute
	\begin{equation} % ϕ[l,m] - ϕ[l, k] - dϕ[l, k]*(W[t,m]-W[t,k])
		\phi_{m, kl} := \Phi_{m,l} - \Phi_{k,l} - \Phi'_{k,l}(W_{m,t}-W_{k,t}) \quad \text{for } k \in K, \text{ }l = 1,...,L,
	\end{equation}
	where
	\begin{equation}
		\begin{split}
			\hat{t} &:= l \text{ mod } n_f, \\
			t &\leftarrow \hat{t} = 0 \text{ ? } n_f : \hat{t}.
		\end{split}
	\end{equation} 
\end{itemize}




\iffalse
========= some comments========== \\
$b$ must of course be independent of $\theta$.

In (8) there is no argument $w$; instead you need to minimize the expression
\begin{equation}
    |K| \sum_m RMSD_K(w_m)^2,
\end{equation}
where $m$ labels the molecules. To find the linear system you need to write the right hand side of (7) as
\begin{equation}
    \sum_{kl} A_j(w)_{kl} \theta_{kl} -b_j(w)
\end{equation}
and find the expressions for $A_j(w)_{kl}$ and $b_j(w)$ by comparing the coefficients of $\theta_{kl}$ and the $\theta$-independent terms. For $A_j(w)_{kl}$ you get a sum of two terms, one involving a Kronecker delta. For $b_j(w)$ you get a sum over $k$ and an additional term.
The least squares problem in (8) then has a matrix $A$ with rows indexed by $j,m$ and columns indexed by $k,l$, with
\begin{equation}
    A_{jm,kl}:=A_j(w_m)_{kl},
\end{equation}
and something similar for the entries of $b$.

================== \\
You just need to extract from (7) the terms containing $\theta_{kl}$, which are

\begin{equation}
    \frac{\phi_{kl}}{D_kS_K(D_jS_K-1)}
\end{equation}


and
\begin{equation}
    \frac{\delta_{jk}\phi_{kl}D_kS_K(D_jS_K-1)}{D_jS_K-1}.
\end{equation}


Add the two and simplify, taking out common factors.

Similarly, you extract the terms without theta's by setting all $\theta$'s to zero

================== \\

Please write a program for calculating the vector v with components $v_{jm}:=\Delta_{jK}(w_m)$ using (3), (2), (1), and (7a) of your writeup, adding in (3) only the nonzero terms. Compare times with those for computing $A\theta-b$. The numerical values should be the same, up to roundoff.

\fi

\end{document}
