\documentclass[12pt]{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{caption}
\captionsetup[table]{skip=10pt} % table skip

%macros
\newcommand{\defeq}[2]{\stackrel{\mathclap{\normalfont\mbox{#1}}}{#2}}
\def\D{\displaystyle}
\def\att{                    % mark at the margin
        \marginpar[ \hspace*{\fill} \raisebox{-0.2em}{\rule{2mm}{1.2em}} ]
        {\raisebox{-0.2em}{\rule{2mm}{1.2em}} }
        }
\def\at#1{[*** \att #1 ***]}  % text needing attention
\def\spc{\hspace*{0.5cm}}



\title{RoSeMI: Robust Shepard model for interpolation}
%\author{Beryl Ramadhian Aribowo}

\begin{document}
\maketitle


\section{Feature extraction}
The very first piece which needs to be computed is the collection of molecular features. The feature matrix $F \in \mathbb{R}^{N_\text{QM9} \times n_f}$ where $N_\text{QM9}$ is the number of molecules in the QM9 database and $n_f$ is the number of features, needs to be precomputed and stored in the non-volatile storage in order to avoid re-computation.
\subsection{Atomic feature selection}
\begin{itemize}
	\item Compute the atomic features using ACSF (\at{cite Behler}). This gives a row vector $f^l_{i:}$ with length $n^\text{atom}_f = 51$ for each atom $i$ in a molecule $l$. Apply this to all molecules in the dataset, hence there are a total of $N$ ACSF matrices $f^l$.
	\item In order to select the most relevant features, we do the \textbf{principal component analysis} (PCA), first by computing the mean vectors and the matrix $S$ by
	\begin{equation}
		\begin{split}
			s &= \frac{1}{N_\text{QM9}}\sum_{l} \frac{1}{n^l_\text{atom}}\sum_{i} f^l_{i:}, \\ 
			S &= \frac{1}{N_\text{QM9}}\sum_{l} \frac{1}{n^l_\text{atom}} \sum_{i} f^l_{i:} (f^l_{i:})^\top, \\ 
			\text{for } l &= 1,...,N_\text{QM9}, ~~ i = 1,2,...,n^l_\text{atom},
		\end{split}
	\end{equation}
	where $n^l_\text{atom}$ is the number of atom in molecule $l$; then compute the covariance matrix
	\begin{equation}
		\label{eq:pca_atom_start}
		C = S - ss^\top. \\			
	\end{equation}
	The correlation matrix can be formed by
	\begin{equation}
		C' := DCD, ~~ D = \text{Diag}(1\sqrt{C_{ii}})
	\end{equation}
	Select the eigenvectors such that they correspond to the $n_{af}$ largest eigenvalues. They are obtained by first doing the spectral decomposition of the correlation matrix
	\begin{equation}
		C' = Q\Lambda Q^\top,
	\end{equation}
	where $\Lambda$ is the diagonal matrix containing the $k$th eigenvalue $\Lambda_{kk}$ and $Q$ is the matrix containing the $k$th eigenvector $Q_{:k}$, then we permute the columns of $Q$ and the diagonal entries $\lambda_i$ of $\Lambda$ such that
	\begin{equation}
		\lambda_{1} \geq \lambda_{2} \geq  ... \geq \lambda_{n^\text{atom}_f},
	\end{equation}
	and select
	\begin{equation}
		\hat{Q} = Q_{:, 1:n_{af}}.
	\end{equation}
	Finally the transformed feature can be obtained by
	\begin{equation}
		\label{eq:pca_atom_end}
		f^l_{i:} \leftarrow \hat{Q}^\top(f^l_{i:} - s), \text{ for }i = 1,2,...,N_\text{QM9}.
	\end{equation}
	\item Scale
	\begin{equation}
		\label{eq:scale_1}
		f^l_{ij} \in [0, 1]
	\end{equation}
	by
	\begin{equation}
		\underbar{$f$}_j := \min_{li} f^l_{ij},
	\end{equation}
	\begin{equation}
		\overline{f}_j := \max_{li} f^l_{ij},
	\end{equation}
	\begin{equation}
		f^l_{ij} \leftarrow (f^l_{ij} - \underbar{$f$}_j)/ (\overline{f}_j - \underbar{$f$}_j), \text{ for } j = 1,..., n_f^\text{select}.
	\end{equation}
	
\end{itemize}
	
\subsection{Molecular feature selection}
\begin{itemize}
    \item $l$th molecule contains $n^l_\text{atom}$ number of atoms and a set of atom types $P = \{\text{H}, \text{C}, \text{O}, ...\}$ with cardinality $n^l_\text{type}$,
	the molecular features are computed by
        \begin{equation}
			\label{eq:acsf}
            \begin{split}
                s^l_{j} &= \sum_{m \in M_j} f^l_{m:}, \\
                %S^l &= \sum_i f^l_{i:}(f^l_{i:})^\top,
            \end{split}
        \end{equation}
		where $M_j$ is the set of index which contains the row positions of atom type $j \in P$ in the $f^l$ matrix. 
		Furthermore we add
		\begin{equation}
			N_j = |M_j|,
		\end{equation}
		combined into
		\begin{equation}
			\hat{N} = [N_\text{H}, N_\text{C}, N_\text{O},..., 1/n^l_\text{atom}].
		\end{equation}
		The feature vector of the $l$th molecule becomes
		\begin{equation}
			F_l := [s^l_{j}, \hat{N} ] \in \mathbb{R}^{1 \times n_f^\text{mol}}, \forall j \in P,
        \end{equation}
		where
		\begin{equation}
    		n_f^\text{mol} := 5n_f + 6,
    	\end{equation}
		in most cases this is a sufficiently good feature vector for fitting.
		
		Additional features can be introduced, such as the sum of squares
		\begin{equation}
			\hat{s}^l_j = \sum_{m \in M_j} (f^l_{m:})^2;
		\end{equation}
		the binomial feature
		\begin{equation}
			S^l_j = \sum_{m \in M_j} f^l_{m:}(f^l_{m:})^\top;
		\end{equation}
		since $S^l_j$ is a symmetric matrix, only the upper triangular and the diagonal entries are needed. 
		Hence if we include both the sum of squares and the binomial features
		\begin{equation}
			F_l := [s^l_{j}, \hat{s}^l_{j}, S^l_j, \hat{N} ] \in \mathbb{R}^{1 \times n_f^\text{mol}}, \forall j \in P,
		\end{equation}
        where
		\begin{equation}
			n_f^\text{mol} := \frac{5}{2}(n_f^2+n_f)+6,
		\end{equation}
    	this defines the fingerprint matrix $F \in \mathbb{R}^{N \times n_f}$.
    \item Do feature selection by using PCA in a similar manner as (\refeq{eq:pca_atom_start}) -- (\refeq{eq:pca_atom_end}) to take the subset of the columns of $W$, this can be done by the following:
	    \begin{equation}
			\label{eq:pca_mol_start}
			\begin{split}
				s &= \frac{1}{N} \sum_l F_{l:}, \\
				S &= \frac{1}{N} \sum_l F_{l:}(F_{l:})^\top, \\
				\text{ for } l &= 1,2,...,N_\text{QM9}, \\
			\end{split}
	    \end{equation}
		compute the covariance matrix
		\begin{equation}
			C = S - ss^\top, \\			
		\end{equation}
		compute the correlation matrix
		\begin{equation}
			C' := DCD, ~~ D = \text{Diag}(1\sqrt{C_{ii}}),
		\end{equation}
		and its spectral decomposition
		\begin{equation}
			C' = Q\Lambda Q^\top,
		\end{equation}
		permute the columns of $Q$ and the entries of $\Lambda$ such that
		\begin{equation}
			\lambda_{1} \geq \lambda_{2} \geq  ... \geq \lambda_{n^\text{mol}_f},
		\end{equation}
		and select
		\begin{equation}
			\hat{Q} = Q_{:, 1:n_{mf}}.
		\end{equation}
		Obtain the transformed feature by
		\begin{equation}
			\label{eq:pca_mol_end}
			F_{l:} \leftarrow \hat{Q}^\top(F_{l:} - s), \text{ for }l = 1,2,...,N.
		\end{equation}
	\item Scale $F$ such that 
	\begin{equation}
		\label{eq:scale_mol_1}
		F_{lj} \in [0, 1]
	\end{equation}
	by
	\begin{equation}
		\label{eq:scale_mol_2}
		\underbar{$F$}_{j} := \min_{l} F_{lj}
	\end{equation}
	\begin{equation}
		\overline{F}_{j} := \max_{l} F_{lj}
	\end{equation}
	\begin{equation}
		F_{:j} \leftarrow (F_{:j} - \underbar{$F$}_{j})/ (\overline{F}_{j} - \underbar{$F$}_{j}), \text{ for } j = 1,2,..., n_{mf}.
	\end{equation}
\end{itemize}


\section{Splines}
The splines are used to increase the dimension of the feature space; with sufficient number of splines for each feature, it will lead to linearly separated data points.
\begin{itemize}
	\item Compute the univariate splines $\beta$ and its derivative $\beta'$ given $F_{ij}$ for $i = 1,...,n_\text{data}$, $j = 1,...,n_f$ (vectorized over matrix $F$):
	\begin{equation}
		\label{eq:spline}
		\begin{split}
			\Phi := \beta(F) \in \mathbb{R}^{n_\text{data} \times L}, \\
			\Phi' := \beta'(F) \in \mathbb{R}^{n_\text{data} \times L}.
		\end{split}
	\end{equation}
	where $n_L = n_fn_s$ and $n_s$ is the number of splines for each feature.
\end{itemize}



\section{Data selection}
The set of indices which refers to data points with known energies (or centers) $K$ is chosen by the farthest-distance-algorithm variations, where the imposed challenge is $n_K = |K| = 100$. Here we pick $N = |T| > n_K$ centers as a hyperparameter. The flow of the data selection is as the following:
\begin{itemize}
	\item Compute and store the set of indices $K$ and the corresponding matrix of distances $D \in \mathbb{R}^{N_\text{QM9} \times N}$ by farthest-minimum-distance algorithm \at{cite Eldar} given $F$, $B$ and $N$.
	\begin{equation}
		(T, D) := f_{\text{fmd}}(F, B, M),
	\end{equation}
	in which the distance between two data points of index $k$ and $m$ is defined by
	\begin{equation}
		D_k(F_{m:}) := \|F_{m:} - F_{k:}\|^2_B := \|B(F_{m:} - F_{k:})\|^2_2.
	\end{equation}
	From this, we can obtain one data point which belongs to the set $T$ by indexing
	\begin{equation}
		w_k := F_{k:}, \text{ for } k \in T.
	\end{equation}
\end{itemize}

\section{The model}
The generalized Shepard model has the form of 
\begin{equation}
	\label{eq:shepard}
	V_K(w):=R_K(w)/S_K(w) ~~~ \text{ for } w \in \mathbb{R}^m \setminus \{w_k\mid k\in K \},
\end{equation}
where
\begin{equation}
	R_K(w):=\sum_{k\in K} \frac{V_k(w)}{D_k(w)},~~~
	S_K(w):=\sum_{k\in K} \frac{1}{D_k(w)},
\end{equation}
and
\begin{equation}
	\label{eq:vk}
	V_k(w) = E_k + \sum_l \theta_{kl} \phi_{kl}(w).
\end{equation}
By (\ref{eq:vk}), (\ref{eq:shepard}) can be expanded into
\begin{equation}
	\label{eq:vk_expand}
	V_K(w) := \sum_{k\in K} \frac{E_k + \sum_l \theta_{kl} \phi_{kl}(w)}{D_k(w)} / S_K(w). 
\end{equation}
The quality of the prediction accuracy is measured by the \textbf{mean absolute deviation}
\begin{equation}
	\text{MAD}_K(w) := \frac{1}{|K|}\sum_{j\in K}|\Delta_{jK}(w)|
\end{equation}
and the \textbf{root mean square deviation}
\begin{equation}
	\text{RMSD}_K(w) := \sqrt{\frac{1}{|K|}\sum_{j\in K}\Delta_{jK}(w)^2},
\end{equation}
where by (\refeq{eq:vk}) and (\refeq{eq:vk_expand})
\begin{equation}
	\label{eq:delta}
	\Delta_{jK}(w):=\D\frac{V_K(w)-V_j(w)}{D_j(w)S_K(w)-1}.
\end{equation}
First we need to express (\refeq{eq:delta}) as a system of linear equations, let us simplify it by writing
\begin{equation*}
	\label{eq:simp}
	\begin{split}
		D_k &:= D_k(w), \\
		S_K &:= S_K(w), \\
		\phi_{kl} &:= \phi_{kl}(w),\\
		\psi_{kl} &:= \theta_{kl}\phi_{kl}, \\
		\gamma_k &:= D_kS_K, \\
		\alpha_j &:= \gamma_j-1.
	\end{split}
\end{equation*}
using this, (\refeq{eq:delta}) can be re-arranged into
\begin{equation}
	\label{eq:delta_split}
	\begin{split}
		\Delta_{jK}(w)&= \frac{\D \Big(\sum_{k\in K} \frac{E_k + \sum_l \psi_{kl}}{D_k} \Big)/ S_K - \Big(E_j + \sum_l \psi_{jl}\Big)}{\alpha_j} \\
		&= \sum_k \frac{E_k}{\gamma_k\alpha_j} + \sum_{kl} \frac{\psi_{kl}}{\gamma_k\alpha_j} - \sum_{l} \frac{\psi_{jl}}{\alpha_j} - \frac{E_j}{\alpha_j}\\
		&= \sum_k \frac{E_k}{\gamma_k\alpha_j} + \sum_{kl} \frac{\psi_{kl}}{\gamma_k\alpha_j} - \sum_{l} \frac{\delta_{jk}\psi_{kl}}{\alpha_j} - \frac{E_j}{\alpha_j}\\
		&= \sum_{kl} \frac{\psi_{kl} (1-\gamma_k\delta_{jk})}{\gamma_k\alpha_j} - \Big(E_j - \sum_k\frac{E_k}{\gamma_k}\Big)/\alpha_j,
	\end{split}
\end{equation}
where $\delta_{jk}$ is the Kronecker delta. Hence if we pick $w = w_m$, then
\begin{equation}
	\begin{split}
		\Delta_{jK}(w_m) &= \sum_{kl} A_{jm, kl} \theta_{kl} - b_{jm}, \\
		&= A_{jm,:}\theta - b_{jm} = (A\theta - b)_{jm}
	\end{split}	
\end{equation}
where
\begin{equation}
	A_{jm, kl} = \frac{\phi_{kl}(w_m)(1 - \gamma_k(w_m) \delta_{jk})}{\gamma_k(w_m) \alpha_j(w_m)},\\
\end{equation}
\begin{equation}
	b_{jm} = \Big(E_j - \sum_k\frac{E_k}{\gamma_k(w_m)}\Big)/\alpha_j(w_m).
\end{equation}



\iffalse
&= \frac{\D \left(\sum_{k\in K} \frac{E_k + \sum_l \theta_{kl} \phi_{kl}(w)}{D_k(w)} / S_K(w)\right) - \left(E_j + \sum_l \theta_{jl} \phi_{jl}(w)\right)}{D_j(w)S_K(w)-1}, \\
\fi

\section{Intermediate values}
\label{sec:intermediate}
Several intermediate values need to be computed and stored in the random access memory (RAM), since these intermediate values depend on several parameters which changes for each experiment and are fast to compute hence it is not necessary to store them in the non-volatile memory. Given the stored $F, D$, $T$, $\Phi$ and $\Phi'$:
\begin{itemize}
	\item Determine the set of labels of the initial trial points
	\begin{equation}
		K \subset T,
	\end{equation}
	for example, we can take the first 100 labels of $T$ as $K$.
	\item Compute the set of labels of the unsupervised data points
	\begin{equation}
		U := T \setminus K.
	\end{equation}
	\item Compute the set of of labels of the data points for evaluation
	\begin{equation}
		W := \{1,2,...,N_\text{QM9}\} \setminus K.
	\end{equation}
	\item Compute intermediate values:
	\begin{equation}
		\label{eq:intermediate}
		\begin{split}
			S_K &\in \mathbb{R}^{n_U} := S_K(F_{m:}) \text{ for } m \in U,\\
			\gamma &\in \mathbb{R}^{n_U \times n_K} := \gamma_k(F_{m:}) \text{ for } k \in K, m \in U, \\
			\alpha &:= \gamma - 1.
		\end{split}
	\end{equation}
	where $n_U = |U|.$
	\item Given the splines in (\ref{eq:spline}), compute
	\begin{equation} % ϕ[l,m] - ϕ[l, k] - dϕ[l, k]*(F[t,m]-F[t,k])
		\phi_{m, kl} := \Phi_{ml} - \Phi_{kl} - \Phi'_{kl}(F_{mt}-F_{kt}) \quad \text{for } m \in U, \text{ }k \in K, \text{ }l = 1,...,n_L,
	\end{equation}
	where
	\begin{equation}
		\begin{split}
			%t &\leftarrow \hat{t} = 0 \text{ ? } n_f : \hat{t},
			t = 
			\begin{cases}
				n_f,  & \text{if } \hat{t} = 0 \\
				\hat{t}, & \text{otherwise}
			\end{cases}
			~~ 
		\end{split}
	\end{equation}
	with
	\begin{equation}
		\hat{t} := l \text{ mod } n_f,
	\end{equation}
	this makes sure the desired column is indexed, since $\phi_b'(w)$ is only non zero at the $t$th column position. This results in matrix $\phi$ where the block column is indexed by $k,l$ indices.
\end{itemize}

\section{Fitting the model}
A good prediction model is reflected by small MAD or RMSD, hence a minimization routine is required. In order to solve the minimzation problem, a linear sysem or least squares formulation is needed. For example, if the RMSD is chosen as the objective function then the problem formulation would be
\begin{equation}
    \label{eq:min}
    \begin{split}
        \min |K| \sum_m \text{RMSD}_K(w_m)^2 &= \min \sum_m \sum_j \Delta_{jK}(w_m)^2 \\
        &= \min \sum_{jm} \left( A\theta - b\right)_{jm}^2 \\
        f_{\text{obj}} &:= \min \|A \theta - b\|_2^2,
    \end{split}
\end{equation}
the data matrix $A$ with $MN$ rows and $ML$ columns where $N = |T|$, the coefficient vector $\theta$, and the target vector $b$.

For storage efficiency, it is desirable to not explicitly form the data matrix $A$, since it grows quickly with more data points included in the training set $K$ and testing set $T$. 
We can alleviate the storage problem by writing a routine for computing $Au$ where $u$ is a variable vector, hence instead of having a matrix with $\mathcal{O}(M^2NL)$ storage we would have a vector with only $\mathcal{O}(N)$ storage. 
This can be done by augmenting the first sum of the last line of (\refeq{eq:delta_split}) with the pre-computed intermediate values in (\refeq{eq:intermediate}):
\begin{equation}
	Au = \sum_{kl} \frac{\phi_{m, kl} u_{kl} (1-\gamma_{m k}\delta_{jk})}{\gamma_{mk}\alpha_{mj}},
\end{equation}
Here, CGLS \at{cite Hestenes} method is used to minimize the objective function in (\refeq{eq:min}). The memory-less form of CGLS requires not only $Au$ but also $A^\top v$ routine, which is
\begin{equation}
	\begin{split}
		A^\top v = \sum_{jm} \frac{\phi_{m, kl} v_{jm} (1-\gamma_{m k}\delta_{jk})}{\gamma_{mk}\alpha_{mj}}.
	\end{split}
\end{equation}
Given the $Au$ and $A^\top v$ routine, and $b$, the CGLS is called as follows \at{cite Krylov.jl and LinearOperators.jl}:
\begin{equation*}
	\begin{split}
		\text{op} &\leftarrow \text{LinearOperators}(y, Au, A^\top v), \\
		\theta &\leftarrow \text{CGLS}(\text{op}, b, \text{itmax}),
	\end{split}
\end{equation*}
where $y$ is an arbitrary vector and itmax is the maximum number of iteration.


\section{Program flow}


\subsection{Timing and memory usage}
\at{add formula and numerical table -- for different choices of $n_{af}, n_{mf}$}
Store $F$, the storage should be bounded by $O(N)$ for most cases unless much more features are needed (although currently from experiment this causes overfitting).

\section{Numerical experiments}
Currently, all experiments are run on several hand-picked compounds, where each compound has $n_\text{data} \geq 250$. 
Each fitting is limited to each compound type. The CGLS used is set to 500 maximum iteration for each fitting, and only returns the parameter vector $\theta$ at the end.
The true error is described by the mean absolute error (MAE)
\begin{equation}
	\text{MAE} := \frac{1}{N}\sum_m|E_{m} - V_K(w_m)|.
\end{equation}
The result of the fitting for each compound is shown in table \ref{tab:exp}. The pre-computation time column refers only to the computation time of the intermediate values in section \ref{sec:intermediate}. 

Extensive experiments for H7C8N1 compound were also done to observe the effect of the features and bases, which is shown in table \ref{tab:exp_2}. Given a feature vector $F \in \mathbb{R}^{n_f} $ after PCA process (\refeq{eq:pca_1})-(\refeq{eq:pca_8}) and before scaling (hence actually $F:=W_{i,:}$), the binomial features refers to
\begin{equation}
	B_{p} := F_{j}F_{k} ~ \text{ for } j<k, ~ p = 1,...,n_f(n_f-1)/2,
\end{equation}
then the feature vectors are concatenated such that
\begin{equation}
	\hat{F} \leftarrow [F, B] \in \mathbb{R}^{n_f+n_f(n_f-1)/2},
\end{equation}
the "sum" variant refers to $F := F_S$ choice, and "sum(squared)" refers to $F := F_Q$ choice as in (\refeq{eq:acsf}). Then the feature vectors are scaled afterwards following (\refeq{eq:scale_1}) and (\refeq{eq:scale_2}).


\begin{table}[h]
	\caption{Fitting for various compounds.}
	\label{tab:exp}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
	\hline
	\textbf{Compound} & \textbf{$M$} & \textbf{$N$} & \textbf{$n_f$} & \textbf{$n_b$} & \textbf{MAE (kcal/mol)} & \textbf{$f_\text{obj}$} & \textbf{\begin{tabular}[c]{@{}c@{}}Pre-comp.\\  time (s)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}CGLS \\ time (s)\end{tabular}} \\ \hline
	H6C5N4            & 100          & 150          & 40             & 15             & 8.298e+00               & 4.334e-08               & 0.395                                                                  & 1636.52                                                           \\ \hline
	H6C5O3            & 100          & 150          & 40             & 15             & 7.199e+00               & 4.145e-08               & 0.399                                                                  & 1615.242                                                          \\ \hline
	H6C5O4            & 100          & 150          & 40             & 15             & 7.008e+00               & 3.167e-08               & 0.395                                                                  & 1623.974                                                          \\ \hline
	H6C6O2            & 100          & 150          & 40             & 15             & 7.225e+00               & 5.235e-08               & 0.606                                                                  & 1641.69                                                           \\ \hline
	H6C6O3            & 100          & 150          & 40             & 15             & 8.963e+00               & 5.501e-08               & 0.507                                                                  & 1614.349                                                          \\ \hline
	H6C7N2            & 100          & 150          & 40             & 15             & 6.605e+00               & 5.583e-08               & 0.391                                                                  & 1630.287                                                          \\ \hline
	H6C7O2            & 100          & 150          & 40             & 15             & 7.67e+00                & 5.28e-08                & 0.581                                                                  & 1616.849                                                          \\ \hline
	H7C6N3            & 100          & 150          & 40             & 15             & 5.229e+00               & 3.513e-08               & 0.395                                                                  & 1628.509                                                          \\ \hline
	H7C8N1            & 100          & 150          & 40             & 15             & 7.442e+00               & 6.475e-08               & 0.4                                                                    & 1620.605                                                          \\ \hline
	H10C9             & 100          & 150          & 40             & 15             & 8.849e+00               & 2.347e-08               & 0.416                                                                  & 1632.832                                                          \\ \hline
	H12C8             & 100          & 150          & 40             & 15             & 6.719e+00               & 6.701e-09               & 0.417                                                                  & 1631.973                                                          \\ \hline
	H12C9             & 100          & 150          & 40             & 15             & 8.793e+00               & 8.396e-09               & 0.592                                                                  & 1629.829                                                          \\ \hline
	H16C9             & 100          & 150          & 40             & 15             & 1.05e+01                & 1.082e-10               & 0.428                                                                  & 1655.233                                                          \\ \hline
	\end{tabular}
\end{table}

\begin{table}[h]
	\caption{Fitting for H7C8N1 compound only, with varied features and bases.}
	\label{tab:exp_2}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
	\hline
	\textbf{$M$} & \textbf{$N$} & \textbf{$n_f$} & \textbf{$n_b$} & \textbf{\begin{tabular}[c]{@{}c@{}}MAE \\ (kcal/mol)\end{tabular}} & \textbf{$f_\text{obj}$} & \textbf{\begin{tabular}[c]{@{}c@{}}Pre-comp.\\ time (s)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}CGLS\\ time (s)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}feature \\ type\end{tabular}}          \\ \hline
	100          & 150          & 24             & 8              & 6.079e+00                                                          & 1.208e-06               & 0.123                                                                 & 591.844                                                          & default                                                                   \\ \hline
	100          & 150          & 28             & 8              & 4.529e+00                                                          & 5.199e-07               & 0.142                                                                 & 747.534                                                          & default                                                                   \\ \hline
	100          & 150          & 21             & 8              & 6.862e+00                                                          & 2.107e-06               & 0.105                                                                 & 517.05                                                           & default                                                                   \\ \hline
	100          & 150          & 21             & 8              & 1.287e+01                                                          & 5.631e-05               & 0.112                                                                 & 513.35                                                           & \begin{tabular}[c]{@{}c@{}}binomial(\\ PCA(sum))\end{tabular}          \\ \hline
	100          & 150          & 21             & 8              & 9.516e+00                                                          & 1.099e-04               & 0.099                                                                 & 523.964                                                          & \begin{tabular}[c]{@{}c@{}}binomial(\\ PCA(sum(squared)))\end{tabular} \\ \hline
	100          & 150          & 102            & 8              & 8.177e+00                                                          & 6.956e-09               & 0.643                                                                 & 2333.708                                                         & default                                                                   \\ \hline
	100          & 150          & 102            & 15             & 1.963e+01                                                          & 1.32e-09                & 1.162                                                                 & 4122.527                                                         & default                                                                   \\ \hline
	100          & 150          & 40             & 15             & 7.442e+00                                                          & 6.475e-08               & 0.401                                                                 & 1620.606                                                         & default                                                                   \\ \hline
	\end{tabular}
	\end{table}

\iffalse
========= some comments========== \\
$b$ must of course be independent of $\theta$.

In (8) there is no argument $w$; instead you need to minimize the expression
\begin{equation}
    |K| \sum_m RMSD_K(w_m)^2,
\end{equation}
where $m$ labels the molecules. To find the linear system you need to write the right hand side of (7) as
\begin{equation}
    \sum_{kl} A_j(w)_{kl} \theta_{kl} -b_j(w)
\end{equation}
and find the expressions for $A_j(w)_{kl}$ and $b_j(w)$ by comparing the coefficients of $\theta_{kl}$ and the $\theta$-independent terms. For $A_j(w)_{kl}$ you get a sum of two terms, one involving a Kronecker delta. For $b_j(w)$ you get a sum over $k$ and an additional term.
The least squares problem in (8) then has a matrix $A$ with rows indexed by $j,m$ and columns indexed by $k,l$, with
\begin{equation}
    A_{jm,kl}:=A_j(w_m)_{kl},
\end{equation}
and something similar for the entries of $b$.

================== \\
You just need to extract from (7) the terms containing $\theta_{kl}$, which are

\begin{equation}
    \frac{\phi_{kl}}{D_kS_K(D_jS_K-1)}
\end{equation}


and
\begin{equation}
    \frac{\delta_{jk}\phi_{kl}D_kS_K(D_jS_K-1)}{D_jS_K-1}.
\end{equation}


Add the two and simplify, taking out common factors.

Similarly, you extract the terms without theta's by setting all $\theta$'s to zero

================== \\

Please write a program for calculating the vector v with components $v_{jm}:=\Delta_{jK}(w_m)$ using (3), (2), (1), and (7a) of your writeup, adding in (3) only the nonzero terms. Compare times with those for computing $A\theta-b$. The numerical values should be the same, up to roundoff.

\fi

\end{document}
