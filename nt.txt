- try fitting with n_atom_select = 15 and n_mol_select = 40

data setup:
exp_all_1: naf = 20, t = 586.5640139000001s
exp_all_2: naf = 20, t = 10536.0200658s


$OMP1:
julia> testtimeactual("exp_all_1", 10_000)                                              
[Nqm9, N, nK, nf, ns, nL] = [133628, 300, 100, 28, 5, 140]                              
linop timings [t_ax, t_atv] = [0.319343393, 1.065184946], total = 1.384528339           
[intermediate values, batchpred of VK(w_m) ∀m ∈ Nqm9] = [0.223031794, 185.475802537]

$SAINT:
julia> testtimeactual("exp_all_1", 2223)
[Nqm9, N, nK, nf, ns, nL] = [133628.0, 300.0, 100.0, 28.0, 5.0, 140.0]
linop timings [t_ax, t_atv] = [0.3374591, 0.5015092], total = 0.8389683
[intermediate values, batchpred of VK(w_m) ∀m ∈ Nqm9] = [0.199886, 75.3966083]

julia> testtimeactual("exp_all_2", 5_000)
[Nqm9, N, nK, nf, ns, nL] = [133628.0, 2400.0, 100.0, 120.0, 15.0, 1800.0]
linop timings [t_ax, t_atv] = [13.2890423, 76.2501555], total = 89.53919780000001
[intermediate values, batchpred of VK(w_m) ∀m ∈ Nqm9] = [36.992655, 1190.335657]

Nqm9	nK	nU	nf	ns	MAE	||Aθ-b||^2	max(MAD(U))	use_MAD	t_inter	t_solver	t_pred  machine
133628.0	100.0	200.0	28.0	5.0	3.04453346e+04	2.53093305e+00	1.07627011e-02	false	0.292552525	603.838726578	95.438554876    OMP1
133628.0	100.0	2300.0	120.0	15.0	2.76078121e+04	6.11409172e+02	1.20520404e-01	false	37.230703	974.5096926	1190.1894936    SAINT

try: 
{4, 5, 6} n_af using corr matrix (instead of cov matrix) for the sums only, and try with the rest of the features from the best n_af (also the eigenvalues plots).

SOAP extraction time = 524.7424840927124 s
Filesize = 6GB
feature length = 1950
SOAP data setup: 1274s

implement kernel ridge regression from FCHL18

02.12.2022:
- retry the sigma comptation using only 500 x 100 data, and also store the norms, and the sigma must give a nice number (rounded, but sigma0 is fine to be floating point)
- try using the atomic features formula instead of the molecular features
- put the timing and storage to the pdf

05.12.2022:
- the MAE of QM9 doesn't match with lilienfields' paper??

05.12.2022:
- mol gaussian: 1800, atomic PCA by MAE -> mol PCA standard : 2239.269684670436
- RoSeMI: 1700
- NN (with architecture similar to ACSF paper), lowest = 1000
atomic gaussian:
- sum = 40588
- mean = 23911
- mean full feature (no PCA) = 25039
- mean scaled by MAE = 22229

06.12.2022:
Improving the feature quality:
- compute the MAE := max(|\delta f|)/N, this is the sensitivity
- compute the covariance from actual f (store for multiple usage), divide using sensitivity as the diagonals

12.12.2022:
3.42 kcal/mol schnet w/ 1k training 500 eval
14.3 kcal/mol 100 train

14.12.2022:
23 kcal/mol 100 train with just atomcount as features, with linear leastsquares solver, more details:
  L1: 23.4
  L2: 23.3
  K1: 540.08
  K2: 540.04
23.8 kcal/mol: fit atomref w/ 100 E -> use reduced E for ROSEMi fit with 100 E.

! refit the shepard model:
* using 100 points obtained from L1 (do not recompute!!)
* E[Midx] -= EL1[Midx]
* predict using the model trained from the E[all\Midx] += EL1[all\Midx] to the prediction
* compute error, the error should be reduced

! do loop exp:
* LLS using training energies from ROSEMI's features:
  - record the atomic reference energies (the thetas := [E_H, E_C, E_N, E_O, E_F]) (*)
* Refit ROSEMI (or any other models) using the reference energies:
  - compute the mean absolute value of E_red := E_l - A*theta, or E_l - n_a*theta_a, (*)
  - compute MAE of ROSEMI fitted using reduced energy: predict the E_red of all QM9 then add E_pred = E_red + n_a*theta_a (*)
(*) means record the values

15.12.2022:
atomref info, MAE in kcal/mol while the rest are in Hartree:

	MAE	E_H	E_C	E_N	E_O	E_F
	21.2	-0.6	-38.07	-54.74	-75.22	-99.86

fitting info, MAE in kcal/mol:

	model MAE nK  nU	n_af	n_mf	n_basis	
	ROSEMI	23.7  100 200	4	18	5	
  KRR	21.3  100 200	4	18	5	
  NN	21.1  100 200	4	18	5	
  LLS	19.4  100 200	4	18	5	

16.12.2022:
- try: http://www.qmlcode.org/tutorial.html for FCHL feature

29.12.2022:
FCHL:
- 100 train: 257 kcal/mol
- 100 train center: 129 kcal/mol
- 100 train center & reduced energy: 9.33706302295949 kcal/molecular
batchpred t =  3849s for SAINT
batchpred t = 11448s for OMP1

03.01.2023:
not possible to implement FCHL Kernel in Julia, the python & fortran code is not 1:1 to the formula and is way too complicated,
alternative: call py from Julia

04.01.2023:
FCHL contains edge features, so the treatment should be different (look at the fortran code)

10.01.2023:
inject fortran code: get from github, edit the .f90 files, "pip install ." in the directory, no need to uninstall if there's any change, just do pip install again

16.01.2023:
ACSF, gaussian atom kernel w/ kronecker delta (without preprocessing the features), direct solve = 37 kcal/mol, CGLS = 22.7 kcal/mol, t_pred = 907s
 ====        w/o kronecker delta, CGLS =  29.9, t_pred= 2356s
SOAP, GAK, w/ kronecker delta, CGLS = 20.5 kcal/mol
SOAP, GAL, w/ kron, preproc data, 17.5 kcal/mol

19.01.2023:
- for each experiment, "GC.gc()" garbage collector is necessary, such that it doesnt explode the ram
! fix the Julia version, different Julia version gives different eigenvectors, the fitting result of OMP1 and SAINT is different!
- to use random seed: call the seed before each random s.t. the numbers are reproducible
- do experiments with different sets of K (see paper)

24.01.2023:
- usequence experiment: uniformly distributed data especially [0,1], gives the same selected points, even on lower dimensions

01.02.2023:
- FCHL atom: removed 5 features since they give no covariance -> 140 length atomic features
- Start the hyperparameter tuning with Morteza's DFO

03.02.2023:
- for the atomic fitting, the test MAE must not be used !! (since we can only use 100 energies). Only take from the training MAE. => recompute the atomic fitting, save the training MAE
- fitting needs to be slower than the hyperopt (and needs delay)

15.02.2023:
varlist that must be emptied on each experiment run:
  alouet:
    data_setup: dataset=F=f=ϕ=dϕ=centers=redf=nothing
    fit_rose_and_atom: dataset = E_dict = f = F = ϕ = dϕ = E = D = E_atom = Midx = Uidx = Widx = nothing
    fit_atom: dataset=F_atom=E=center_ids=Midx=Widx=A=θ=stat=errors=E_pred=E_atom=E_red_mean=Ed=nothing
    ROSEMI fitter: SKs_train = SKs = γ = α = B = klidx = cidx = Axtemp = tempsA = op = b = tempsb = θ = stat = VK = outs = v = vmat = MADs = batches = VK_fin = nothing
    KRR fitter: Norms=K=θ=stat=errors=K_pred=E_pred=Er=nothing
    NN fitter: x_train=model=pars=opt=nothing
    LLS fitter: A=θ=stat=errors=E_pred=nothing
    GAK fitter: A=θ=stat=E_pred=errors=nothing

  linastic:
    PCA_atom: s = ∑ = C = D = e = nothing
    PCA_atom: C = e = F = nothing

22.02.2023:
- need to change paramcheck into parambound
- save list of (f,x) using the driver.m instead of jl

03.03.2023:
- use midpoint for initialization: change Inf bounds.

06.03.2023:
- try removing 3k dataset (the "defective" datasets)
- try regularization in CGLS (add lambda, possibly a new hyperparameter)

08.03.2023:
- revert back main in expdriver to previous one (non-translator mode)
- new projection, rounding, and translation mode, in matlab file

14.03.2023:
- try experiments with S := 1/(N-1).... instead of 1/N, it adds regularization effect
- round the atomic energies to integers before use for experiments, since the mean of the rounding error is around 1.5 while the std has larger error (this will change the things slightly).

04.04.2023:
- table of collection of hyperparameters checkpoint in "data/hyperparamopt/tables/tf_fsort.txt"
- separate mintry(x,f) and decoder(x), useful for "infeasible" initial point

14.04.2023:
- tried empirical cumulative distribution function (ECDF) for feature binning, seems like for atom (GAK), binning full data gives better MAE; not yet tested for molecules (MGK/KRR, LLS, NN, etc).

17.04.2023:
- tested FCHL Kernel with center index = 38 as training set, MAE = 7.088911846921814
- for now removed FCHL atomic feature from hyperparameteropt, since it causes NaN; changed c to int instead of real
- changed init params, following the bounds, now the vector size is 16
- all_tb_170423 contains all of the current found hyperparameters 

19.04.2023:
- Hyperparams which are possible to improve MATRs:
  + inittune.m: cdeltamin [30]
  + inittune.m: cnu [21]
  + in general larger budget gives better result, recommended = 1200*n (current, n=16 -> 19200 nfmax)
- changed penalty factor to 10, see if this gives better results
- init tuning using best hyperparameter with MAE=11.62 (see caller_pc.m)

25.04.2023:
- repker w/ CGLS w/ current "best" feature: (train, test, machine) = (13.315580220029185, 18.506755171167082, SAINT) 
? the mol PCA gives different results in SAINT and OMP1 but atom PCA is the same ???, reproduce result: data_setup("exp_reduced_energy", 20, 16, 5, 300, "data/qm9_dataset_old.jld", "data/ACSF.jld", "ACSF"; save_global_centers = false)
found the reason: cor() gives different result between SAINT and OMP1!!, turns out looks like this is accumulated from small error difference accross large data, atom gives the same eigen but some small error accumulated -> different mol features, surely the MAE is not much different?
turns out the MAE is completely different, maybe next directly compare the atom eigenvectors?
turns out the difference is all the way from the eigen atom, small numerical difference snowballs until the mol computation

26.04.2023:
- repker as feature generated from 20-16 with centers as col: (train, test, machine) = (12.503066223772425, 18.68415919418044, OMP1)
- repker atom level from 20-16: 
  ! (train, test, mode, machine) = (7.828382458724014, 10.998833688919287, CGLS, OMP1), current best! 
  ! (train, test, mode, machine) = (7.989517036864852, 11.043965907343884, CGLS, SAINT), really need to check the difference of numresults between SAINT and OMP1 (bug)!
- try parallel mode of hyperparameter optimization (need VSC access):
  1 init as usual
  2 perturb the next iterate into n different iterates, broadcast this to slaves
  3 get the best iterates from the slaves into master
  4 go to 2
- implement message passing (see paper and its writtings)

03.05.2023:
- test MP without sigma yet, (t, data, train, test, model, machine):
  - (1, ACSF, 8.531877790679385, 13.637190148167665, REAPER, SAINT)
  - (5, ACSF, 12.134951800081835, 19.285999493686106, REAPER, SAINT)
  - (1, 20--16, 6.405897394503283, 13.164806953612652, REAPER, SAINT)
  - (2, 20--16, 6.027474617840749, 13.895430026759396, REAPER, SAINT)
- test MP without PCA:
  - (1, ACSF, 7.687329449835585, 12.792789108425062, REAPER, SAINT)
  - (2, ACSF, 7.9554008405230405, 13.032915591258527, REAPER, SAINT)
  - (1, 20--16, 7.700348168196548, 13.380650010040405, REAPER, SAINT)
  - (2, 20--16, 8.115258754956065, 12.977283254952756, REAPER, SAINT)
  ...
- see paper(s) for hyperoptparallelization.
  
05.05.2023:
- the parallelization will probably be in slurm level:
  First scheme of parallelizatgion:
  - 3 big processes, 2 masters (mintry m, controller jl), and 1 slave category (n simulators jl)
  - mintry and simulators run as usual, controller controls the (x,f) flow, spawn processes, etc
  Or:
  - 3 processes: mintry.m, controller.jl/m, generator.m, simulators.jl
  - mintry: as usual, runs indefinitely, reads mintry table periodically -- get (x,f) pair, compute x = mintry(x,f), store x in mintry table 
  - controller runs indefinitely, reads sim table periodically, if sufficient numbers of data depending on id is accumulated, store best(f) in mintry table  
  - generator runs indefinitely, reads new signals from the simulators periodically, get x from mintry's table, returns xhat = perturb(xraw) for each new signal 
  - simulators: run indefenitely, asks for new xhat if idle (sends signal to generator), compute f = f(xhat), store f to sim table ({id,f,x,})
  - see paper for more details.
- the simulators can be in slurm level (node level) (recommended, since each proc \in node), the number of processes run in this case can be determined by running in jl:
    s = readchomp(`squeue -u berylaribowo`) # to read the output of command
    ...
    count the occurence of job containing "..jl" as job name identifier
- the main processors should only be: mintry, controller, simulators. Controller bridges the mintry and simulator, mintry and simulator work as usual

15.05.2023:
- (see paper) need to accomodate for when:
  - disjoint iteration: 1 -> 3. Sol? add iter_id
  - n_sim idle > thresh OR n_sim run < thresh => thresh not fulfilled => no f update for mintry => no x update from mintry => all sim idle (no iter update). Sol? put sim_state

17.05.2023:
- fobj listener is now only depends on the iter -> any simulator can compute any iterations
- iter tracker now is more simple.
- speed: simulator listener > controller listener > mintry listener, since it's possible that the simulator receive duplicate x if the simulator is too slow on receiveing signal

22.05.2023:
Quests:
? avoid controller giving multiple generated-x if simulator's listener is slower => make controller (listener) speed < simulator speed
? handle case where if there is no new f accepted AND no new x given (no running sim, no update from mintry) =>  accpet whatever f is in the contr repo, but if repo is empty??? -> stuck, iter doesnt advance.
? when not enough new iterates, 2 choices:
  - don't anything to sim -> (see quest 2)
  - (choose dis) give whatever x, sim MUST NOT compute f(x) but looks into the repo and returns f -> this needs one (x,f(x)) repository, race condition since one sim accesses the same file?? -> race condition is fine, doesnt give any error!
? write simulator (jl), algo:
  - initialize simulator with only "state = idle" f info
  - listen to controller's signal (faster than controller), read x file: 
    - set state = running
    - if uid is different, check x: 
      - if x exists in tracker: write the corresponding f. Otherwise: run f = f(x) and write f, 
    - set state = idle

08.06.2023:
- reproduced delta ML testing from QML tutorial
- reproduced kernel result of Python QML in julia, by using the direct solve and regularizer, the result is exactly the same.
? see Unit Testing of Julia for more comprehensive unit tests
? for more threading parallelization: see https://levelup.gitconnected.com/parallel-code-in-julia-with-threads-b2e7c97f071b

12.06.2023:
- tried naive pairpot as base energy, turns out to be worse than standard ML, probably need to tune the hyperparameters (alpha, delta, set their dependencies, etc)

13.06.2023:
! maybe try triplet features
! check out SLATM representation (maybe doesnt require specialized kernel)
  checked SLATM, turns out it has 17895 features! (very large), around 22gb for all qm9 data
- added overleaf shared with prof.lilienfeld

14.06.2023:
- slaves can be spawned as much as possible in the VSC
! try separating simulator output files for parallel hyperparam
! test using actual fobj
! add automatic "garbage" remover (reset files to empty) || 1
! need to add automatic simulator spawner

19.06.2023:
- get_prop(mol, n1, n2, :order) gives the bond order from MolecularGraph (see dp.jl).

20.06.2023:
- Ebase = Enull + Esob for LLS model gives ~6kcal/mol accuracy reduction, BUT for the prev best found model turns out this overfits, train = 1e-12 but test is 1e+2

21.06.23:
! for math desc of aSLATM and FCHL18 look at https://github.com/lcmd-epfl/intro-to-qml/blob/master/Introduction%20to%20QML.ipynb.
! try to get the FCHL19 first, simpler representation.
! see qmml.org for preprocessed qm9 dataset and probably usew it

26.06.23:
- nRs2 = 12, nRs3 = 10, rcut = 6˚A and nmax = lmax = 3, σ = 0.1, rcut = 6˚A (for fchl19 and aSLATM respectively from mbdf paper)

28.06.23:
(DONE) ! recompute features for dressed atoms := null, and features for dressed bonds, using new full set of data
(DONE) using dscribe ? index 184 cause error for QML's ACSF, use dscribe or the ol julia acsf instead

30.06.23:
- can use MolecularGraph's neighbors(atom) to get the triplet (probably for angle?)

04.07.23:
- check ACSF: kernels (GAK, repker) gave NaNs
- check whether i can get extra disk space for VSC

06.07.23:
- ThreadsX works well! especially on VSC, now computation is blazing fast!, other functions should be chagned to ThreadsX

07.07.23:
- std matrix threadsx gives an error, correct later

19.07.23:
! less file OP for faster speed for data_setup and fitter

20.07.23:
- difference in index and E between saint and vsc due to randomness
- fileOP is still confusing for the deltaML, fix later

26.07.23:
- C(n_neighbors, 2) = number of angles exist in a partition of n_atoms when observing a center atom.
- for torsion, probably we can observe an edge, and see neighbours(src(e)) union neighbors(dst(e)) \ dst(e) union src(e)
! finish get angle then get angle type

02.08.23:
- try PCA or manual feature selection (removing some types of angles) for dressed angle.

04.08.23:
- add manual readme for hyperparamopt parallel usage, often forgot how to use it.
- need to change some hyperparam boundaries (due to changing hyperparamopt)

08.08.23:
- trial of built in Julia PCA
- maybe try PCA bond energies too?
! think of the getting the torsion algorithm

10.08.23:
! 211CCOC == 112COCC, need to have symmetry check for torsions (perhaps angles too)
! need to check angular symmetry too!, e.g., 12CCO == 21COC, then rerun every experiment..

12CCO == 21COC, but 12CCO != 12COC, instead 12COC == 21CCO
12CCO != 21CCO

sort symmetry fragments?:
11COC => 11CCO, 11FHF => 11FFH, 12OCH => 12OCH, 21OHC => 12OCH, 12OHC => 21OCH
12OCC == 21OCC, need to account for the case when left and right atoms are the same. -> check if the symmetry atoms are equal, if yes then sort the degrees
12OCH => 12OCH -> natural indexing perserved, but is not the above case

let X = center, Y = non centers,
YY degrees (tuple): 11, 12, 22 -> degree tuple union combination
YZ degrees (combination): 11, 12, 21, 22 -> degree enumeration

algo for generating the classes:
- generate type combinations, product it with enumerated degrees
- generate type tuple of equal types, product it with degree combinations
- concat both

algo for grouping: 
- check if the left and right atoms are equal, if not then:
  - sort symmetry fragment either by the alphabetical atomic element or the bond degree, then the rest should follow the permutation index.

does this apply for torsion? probably sort by the block of atoms?

torsion:
center bond, left, right, left atom pair, right atom pair, e.g., 122COCC

examples (centers omitted, since it doesnt affect symmetry):
11COCC -> 11CCOC, 12COCC -> 21CCOC, 11FOHC -> 11CHOF, 11OOHF => 11FHOO,
21CCCC -> 12CCCC, 21CHCH -> 12CHCH, 12HCHC -> 21CHCH, 21HCHC -> 12CHCH,
12HCCH -> 12HCCH (not get sorted!!), 21HOCH -> 12HCOH

122CHOH

torsion types:
- all same atoms: indep bond sort
- completely distinct atoms: bond sort by atom sortperm
- same duplet + distinct duplet: bond sort by atom sortperm
- symmetric: XYXY (equal duplets), YXXY (equal reversed duplets): indep bond sort

algo:
if all atoms are the same OR left and right symmetric OR no atoms get swapped, then bonds are sorted independently  
if 1 > end: then swap (1,end) and (2,end-1)
else if 1 = end: if 2 > end-1, swap (2,end-1)
check for symmetric duplets: 
if symmetric then bonds independent swap
else, swap bonds by atom's permsort

21.08.23:
- as thought, angle and torsion dont improve the MAE, hypothesis: what if they improve on ntrain > 100?

24.08.23:
! result org:
  - fix the best hyperparameter from ns and sx data then compare up to dt (see the effect of baselines)
  - compare the best dx of ns and sx (see effect of data selection), and from the best of both fix the hyperparam and compare from each
  - (skip?) fix sx and compare each feature performance up to dt (see the effect of feature)
  - best of each feature
! need to also store the plot information

25.08.23:
! include elvl into hyperparameterspace (see baseline fitting)
! need to also give indication when PCA is switched off for all features (elvl and high level features)

29.08.23:
? autoencoders for feature extraction: autoencoder on all unlabeled data, and then train on only labeled data.
- next results waiting to be crawled: 
  - s2 with PCA DN5 DT5
  - ns 1k PCA DN5 DT5
  - ns 1k noPCA

30.08.23:
PCA plot:
- (5 curves) current best curve without PCA (DB), best curve found on DN without PCA (DN), best curve found on DT without PCA (DT), (DN, PCA), (DT, PCA)
- (6 curves) BODB, PCA1BODN, PCA1BODT, PCA2BODB, PCA2BODN, PCA2BODT
or just wriet the table of: best of non PCA, best of PCA1 for each level, best of PCA2 for each level

05.09.23:
- see photo of Sep 5 (in gdrive) for hyperopt new scheme that can incorporate some switches (on/off),
  and try to implement w/ new boundaries (includes the E_dressed_features)
! test whether the memory of VSC can fit all features loaded at once (and actual scenario probably need like 20x of them at once)
(done)

08.09.23:
!! need to add new data_setup that accpets the data rather than loading the data,
loading in setup costs 130s startup
! From Anatole: try the dressed atom with the electronic configurations of atoms, i.e., split C into: C_sp1, C_sp2, C_sp3, same with the others
looks like they can be grouped by the bonds (this is called hybridization)
! - can immediately find the hybridization using "mol.hybridization"
- theory on hybridization: https://www.masterorganicchemistry.com/2018/01/16/a-hybridization-shortcut/

12.09.23:
$ hybridization DA improves on the DA level MAE by ~2kcal/mol, and improves on other levels by ~0.09kcal/mol
? wwhat if we use explicit H for other levels?

13.09.23:
- main obj parameters passed looks correct, DA probably should use the standard one rather than the hybridized for now

15.09.23:
!! for papers, we can write a few papers from the current data; 
for example the systematic dressed fragments can be written as one paper by presenting the systematic improvements of the learning curves (e.g., 100, 1000, ... data_points)
, "we need to see the story from different perspectives" -> this gives one output data multiple possible papers.
! for more exploration: try optimization with the data points as the space, where we take a subset of the total set, e.g., try just taking 10k for the fobj.
! more explorations too: try the morse potential for the bonds level, for each types of bonds, see https://journals.aps.org/pr/pdf/10.1103/PhysRev.34.57

- i guess for now finish the hyperparameter optimization (and run them) first
- then try the explicit hydrogens, then morse pot, then data points optimization (this one is probably intractable/infeasible, the space is too large).


18.09.23:
- add_hydrogens!(mol) returns all hydrogens to the molecular graph, try rerun everything with this
  all_hydrogens!(mol) returns the indices of all hydrogens in the graph

19.09.23:
! check the "set_cluster" error from the main_obj later || done, due to adjoin data type, shouldve been matrix

20.09.23:
- for DeltaML dressed fragments plot (side product of QM9 challenge), consider:
  - MBDF's samples: x_i+1 = 2x_i
  - FCHL19's samples: x_i+1 = 10x_i
  but max ~30k training set so that the test set isn't biased (100k test set minimum)
  Ntrains = [100, 500, 1000, 2000, 4000, 8000, 16000, 30000] looks reasonable.
  does it need K-fold cross validation (for now just try the [Ntrains, 100k] for [train,test])
!!! turns out one of the reason why my 100 MAE is always higher is because in other papers, only the subset of 130k is used,
e.g.: in M. Rupp's paper, only 10k for validation; in FCHL paper only 20k; in MBDF 100k is used.

25.09.23:
! the performance for large training index is so slow, looks like it scales with the number of training indices!!,
need to find out how to make it faster:
  - input typing --> tested, if every function is clearly-type-defined, turns out no need for warm up!
  - warm up compiler --> tested for simple function, this makes computation faster!
  - nested ThreadsX
  - !! allocation is veyr important!, turns out this causes a lot of slowdown
  - last resort: CuArray
? gaussian kernel's norm function causes a big allocation, need to think of a way to minimize the alloc (but seems impossible) ==>> manual implementation of normsquared is much faster! Julia is awesome!!

26.09.23:
- careful with randomseed, 603 somehow has bad seed for 150 data...
! error when spawning new sim when spawning the kernel methods, need to check the data type of Vector{Adjoint{Float64, Matrix{Float64}}} --> changed everything to AbstractArray

06.10.23:
$ MBDF gets new record for (100, 100k) test!:
100  100000  "A"  "GK"  "MBDF"  20.5901  21.1273  0.000119159  6.03998

10.10.23:
MBDF gets 5.7kcal/mol (current best record!!), that means need to be included in the hyperopt
hyperparams:
  100
    "MBDF"
    "GAK"
    "direct"
    "dressed_atom"
   0.00014509030030418615
   5.780010343475244

12.10.23:
- probably need to write some automatic calleropt spawner (rem: for some reason the batch call the latest script only, so need to differentiate between one script to another).
- plans to figure out the bond-energy:
  - try to overfit the morse potential first, if this works then the fitting is correct:
    - define morse potential
    - pre-compute the distance matrices, this should comply with the atom order parsed by smiles (probably this is true by construction)
    - compute few molecules' Morse potential for each bond pair type, as a test:
      - for each mol: for each bond type:
  - if AD works for the fitting function, optimize using first order algos 

17.10.23:
!!! see custom CMBDF trarining result!!, 3.8kcal/mol base achieved!!

24.10.23:
Data selection optimization
- see Prof. Neumaier's opt recommendation, based on Tabu search, can be independent of tuning or dependent on it
- see my nonlinear integer programming with linear equality constraint, try it with NLopt perhaps there is some algorithm compatible
? will linearization work? e.g., converting MAE(x) into flinear(x), see https://en.wikipedia.org/wiki/Linearization, try using CPLEX

25.10.23:
-maybe try other solvers too if JuMP is not working for the dummy lsq
- try with diagm as row selector, but this means will need sparse since it will be memory inefficient, sparsearrays? -> ForwardDiff doesnt work
- see NOMAD -> doesnt work for my very specific case
- see TopOpt, seems to be the one for my specific linearization case (?) -> need CUDNN, and looks like for  topology only
- Alpine -> looks promising but needs GUROBI
- Juniper -> heuristics, probably the only one that will work -> doesnt work with JuMP syntax too ???
- maybe try optim

30.10.23:
- nomad froze given the IP problem || can be run but doesn even move from the initial solution, will need actual IP
- try:
  - GLPK
  - NLopt
nothing works, the only thing left are the heuristics, e.g., local search: https://discourse.julialang.org/t/using-a-jump-bin-vector-to-index-an-array-checkbounds-fails-with-type-variableref/102674/11
or:
- Prof. Neumaier's algo for data selection:
This is a high-dimensional discrete problem, which is difficult for the expensive black box optimization problem that you need to solve. 
I'd propose to start with the following, which should be reasonable, but can be modified as you gain experience:
Give penalties p(x)=(f(x)+opt)/(u(x)+1) to each molecule x, where u is the number of simulations where x participated (among all cases tried with the hyperparameters used including those of old runs), 
f is the sum of the corresponding simulation scores, and opt is the optimal simulation score obtained so far. Store the collection P of the n best training sets. 
Whenever a processor finished its simulation, it sends its training set S and its score s to the master, who updates P, opt, and the f(x), u(x), p(x) for all x in S. 
Then the master picks one of the training sets in P at random as new S, replaces m molecules of S with large penalty by m molecules not in S with small penalty, and sends the modified S to the idle processor for new simulation.
Restart after progress (measured by the decrease of the sum of the scores of training sets in P) gets too slow, but not too early (as it may take a while to get a good gain). m and n are parameters that can be kept fixed, varied, or randomized. 
I would start with n = number of processors and m=sqrt{100}=10 for a training set of size 100.
(also read Tabu search)
?? this bugged my mind: 1400 kcal/mol test on some training selection?? gotta check that out ----> see next line
---> turns out the ones that has anomalous large error is becauswe there are missing atomic elements in the training set, e.g., the 100 training data is missing "F".
what if the best selected training set is the set that has all elements (the most complete, with max number of atoms)?

06.10.23:
- the convergence of the Tabu search is affected heavily by the number of "known simulation data" and size of P.
! need to add restart criteria, if there is no gain (no fobj decrease from the current best so far) after n iterations then restart the scores,
or by the sum of scores in P (especially for parallel), instead of maximum iteration (since the optimization process is fast anyway, especially if cache is already exist, the simulation is faster)

07.10.23:
- if after multiple restarts no gain: change the hyperparameters:
n++ and m++ or randomly? (escape from local minima)? 
! add cache in file (like the hyperparameter optimization)
- looks like lesser tolerance gives faster exploration, in some cases like lsq n=30,ns=3:
ntol=10 reaches global optimum in 5.5k iterations with 582 iterates covered, while the total iterates are 15k,, 1/30 efficiency.

08.10.23:
! look at Julia's built in parallel processing manager such as https://github.com/JuliaParallel/ClusterManagers.jl
- takes only 0.2s in the cluster to do one eval for 10k

10.10.23:
- compare the MBDFs with FCHL18 on where the MAE crosses 1kcal/mol
! add idtrains_in later for main_deltaML to accomodate precomputed trainingset
! see GA in Metaheuristics.jl, looks promising, supports constraints and binary variable
==> GA works!! and it found the global minimum quite fast for dummy problem